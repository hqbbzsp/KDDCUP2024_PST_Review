<p> FLAG: Adversarial Data Augmentation for Graph Neural Networks </p><p> adversarial training for free! </p><p> ment datasets, FLAG works purely in the node feature space by adding gradientbased adversarial perturbations to the input node features with graph structures unchanged. FLAG leverages "free" methods [#b27](Shafahi et al., 2019) to conduct efficient adversarial training so that it is highly scalable to largescale datasets. We verify the effectiveness of FLAG on the. Note that our method differs from other augmentations for graphs in that it happens in the input node feature space.Augmentation for "free". We leverage the "free" adversarial training method [#b27](Shafahi et al., 2019) to craft adversarial data augmentations. PGD is a strong but inefficient way to solve the inner maximization of (6). While computing the gra </p>
<p> FLAG: Adversarial Data Augmentation for Graph Neural Networks </p><p> adversarial attacks on neural networks for graph data </p><p> "Jiang et al., 2019), and visual question answering (Gan et al., 2020). Despite the rich literature about adversarial training of GNNs for security purposes [#b44](Zgner et al., 2018Dai et al., 2018Bojchevski amp Gnnemann, 2019ref type"bibr" tar </p>
<p> FLAG: Adversarial Data Augmentation for Graph Neural Networks </p><p> latent adversarial training of graph convolution networks </p><p> rify, FLAG is intrinsically different from the previous graph adversarial training methods (Feng et al., 2019Deng et al., 2019[#b17]Jin amp Zhang, 2019). Feng et al. (2019) proposed to reinforce local smoothness to make embeddings within communities similar </p>
<p> FLAG: Adversarial Data Augmentation for Graph Neural Networks </p><p> latent adversarial training of graph convolution networks </p><p> rify, FLAG is intrinsically different from the previous graph adversarial training methods (Feng et al., 2019Deng et al., 2019[#b17]Jin amp Zhang, 2019). Feng et al. (2019) proposed to reinforce local smoothness to make embeddings within communities similar </p>
<p> FLAG: Adversarial Data Augmentation for Graph Neural Networks </p><p> graph convolutional neural networks for web-scale recommender systems </p><p> b10"(Garcia amp Bruna, 2017), social analysis (Qiu et al., 2018Li amp Goldwasser, 2019), and recommender systems [#b38](Ying et al., 2018). However, the training of GNNs on largescale datasets usually suffers from overfitting, and realistic graph datasets often involve a high volu </p>
<p> FLAG: Adversarial Data Augmentation for Graph Neural Networks </p><p> deepinf: social influence prediction with deep learning </p><p> e"bibr" target"b42"(Zhao et al., 2019Shen et al., 2018), metalearning (Garcia amp Bruna, 2017), social analysis [#b25](Qiu et al., 2018Li amp Goldwasser, 2019), and recommender systems (Ying et al., 2018) </p>
<p> FLAG: Adversarial Data Augmentation for Graph Neural Networks </p><p> adversarial attacks on node embeddings via graph poisoning </p><p> f. Despite the rich literature about adversarial training of GNNs for security purposes (Zgner et al., 2018Dai et al., 2018[#b1]Bojchevski amp Gnnemann, 2019Zhang amp Zitnik, 2020), it remains unclear how to effectively and efficiently improve GNNs' </p>
<p> FLAG: Adversarial Data Augmentation for Graph Neural Networks </p><p> adversarial examples improve image recognition </p><p> dversarial data augmentation is a datadependent regularization that could help generalize to outofdistribution samples, and its effectiveness has been verified in domains including computer vision [#b36](Xie et al., 2020), language understanding (Zhu et al., 2019Jiang et al., 2019), and vis. al., 2018Miyato et al., 2018). With an increasing amount of attention paid to leverage adversarial training for better clean performance in varied domains [#b36](Xie et al., 2020Zhu et al., 2019Gan et al., 2020), we conduct the first study on how to. ch norm. The left part of Table 5 shows that batch norm works to generalize GAT, and FLAG works to push the improvement further. In the computer vision domain, [#b36]Xie et al. (2020) proposed a new batch norm method that makes adversarial training further generalize largescale CNN models. As there is growing attention on using. ton et al., 2017Chen et al., 2018). We propose to effectively augment graph data using adversarial perturbations. On largescale image classification tasks, [#b36]Xie et al. (2020) leveraged adversarial perturbations, along with new batch norm methods, to augment data. Zhu et al. (2019) </p>
<p> FLAG: Adversarial Data Augmentation for Graph Neural Networks </p><p> generalizing to unseen domains via adversarial data augmentation </p><p> target"b0"Balaji et al., 2019), recently a growing amount of attention has been paid to using adversarial perturbations to augment datasets and ultimately alleviate overfitting. For example, [#b32]Volpi et al. (2018) showed adversarial data augmentation is a datadependent regularization that could help generalize to outofdistribution samples, and its effe. or language word embeddings do not have such straightforward semantic meanings, which makes the selection of highly heuristic. In light of the positive effect of large perturbations on generalization [#b32](Volpi et al., 2018), and also to simplify hyperparameter search, FLAG drops the projection step when performing the inner maximization. Note that, although the pe </p>
<p> FLAG: Adversarial Data Augmentation for Graph Neural Networks </p><p> imagenet classification with deep convolutional neural networks </p><p> pe"bibr" target"b15"(Hu et al., 2020), posing significant challenges for prediction problems.One promising solution to combat overfitting in deep neural networks is data augmentation [#b19](Krizhevsky et al., 2012), which is commonplace in computer vision tasks. Data augmentations apply labelpreserving transformations to images, such as translations </p>
<p> FLAG: Adversarial Data Augmentation for Graph Neural Networks </p><p> latent adversarial training of graph convolution networks </p><p> rify, FLAG is intrinsically different from the previous graph adversarial training methods (Feng et al., 2019Deng et al., 2019[#b17]Jin amp Zhang, 2019). Feng et al. (2019) proposed to reinforce local smoothness to make embeddings within communities similar </p>
<p> a transformer-based framework for multivariate time series representation learning </p><p> learning representations of multivariate time series with missing data </p><p>  </p>
<p> a transformer-based framework for multivariate time series representation learning </p><p> deep learning for time series classification: a review </p><p>  </p>
<p> a transformer-based framework for multivariate time series representation learning </p><p> a deep neural network for unsupervised anomaly detection and diagnosis in multivariate time series data </p><p> ries by directing learned representations to approximate a distance such as Dynamic Time Warping (DTW) between time series through a matrix factorization algorithm. A distinct approach is followed by [#b33]Zhang et al. (2019), who use a composite convolutional LSTM network with attention and a loss which aims at reconstructing correlation matrices between the variab </p>
<p> a transformer-based framework for multivariate time series representation learning </p><p> enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting </p><p> br" target"b0"(Bagnall et al., 2017).Transformer models for time series Recently, a full encoderdecoder transformer architecture was employed for univariate time series forecasting [#b18]Li et al. (2019) showed superior performance compared to the classical statistical method ARIMA, the recent matrix factorization method TRMF, an RNNbased autoregr. ul in the case of univariate time series, where selfattention would otherwise match (consider relevantcompatible) all time steps which share similar values for the independent variable, as noted by [#b18]Li et al. (2019).Finally, since the transformer is a feedforward architecture that is insensitive to the ordering of input, in order to make it aware of th. A.3 of the Appendix we demonstrate that our transformerbased method is economical in terms of its use of computational resources. Alternative selfattention schemes, such as sparse attention patterns [#b18](Li et al., 2019), recurrence (Dai et al., 2019) or compressed (globallocal) attention (Beltagy </p>
<p> a transformer-based framework for multivariate time series representation learning </p><p> proximity forest: an effective and scalable distancebased classifier for time series </p><p> volves training a linear classifier on top of features extracted by a flat collection of numerous and various random convolutional kernels. HIVECOTE and TSCHIEF (itself inspired by Proximity Forest [#b21](Lucas et al., 2019)), are very sophisticated methods which incorporate expert insights on time series data and consist of large, heterogeneous ensembles of classi </p>
<p> a transformer-based framework for multivariate time series representation learning </p><p> ts-chief: a scalable and accurate forest algorithm for time series classification </p><p> ke in domains such as Computer Vision or Natural Language Processing (NLP), the dominance of deep learning for time series is far from established in fact, nondeep learning methods such as TSCHIEF [#b28](Shifaz et al., 2020), HIVECOTE (Lines et al., 2018), and ROCKET (Dempster et al., 2020). ed approaches. RELATED WORKRegression and classification of time series Currently, nondeep learning methods such as TSCHIEF [#b28](Shifaz et al., 2020), HIVECOTE (Lines et al., 2018), and ROCKET (Dempster et al., 2020) </p>
<p> a transformer-based framework for multivariate time series representation learning </p><p> attention is all you need </p><p> s of time series regression and classification. Transformers are an important, recently developed class of deep learning models, which were first proposed for the task of natural language translation [#b31](Vaswani et al., 2017) but have since come to monopolize the stateoftheart performance across virtually all NLP tasks (Raffel et. HODOLOGY BASE MODELAt the core of our method lies a transformer encoder, as described in the original transformer work by [#b31]Vaswani et al. (2017) however, we do not use the decoder part of the architecture. A schematic diagram of the generic part of our model, common across all conside. a xmlid"formula_3"W pos ? R w?d to the input vectors U ? R w?d  [u   . . .  u w ] U  U  W pos .Instead of deterministic, sinusoidal encodings, which were originally proposed by [#b31]Vaswani et al. (2017), we use fully learnable positional encodings, as we observed that they perform better for all datasets presented in this work. Interestingly,. e layer normalization after computing selfattention and after the feedforward part of each encoder block, leading to significant performance gains over batch normalization, as originally proposed by [#b31]Vaswani et al. (2017). However, here we instead use batch normalization, because it can mitigate the effect of outlier values in time series, an issue that does no </p>
<p> a transformer-based framework for multivariate time series representation learning </p><p> attention is all you need </p><p> s of time series regression and classification. Transformers are an important, recently developed class of deep learning models, which were first proposed for the task of natural language translation [#b31](Vaswani et al., 2017) but have since come to monopolize the stateoftheart performance across virtually all NLP tasks (Raffel et. HODOLOGY BASE MODELAt the core of our method lies a transformer encoder, as described in the original transformer work by [#b31]Vaswani et al. (2017) however, we do not use the decoder part of the architecture. A schematic diagram of the generic part of our model, common across all conside. a xmlid"formula_3"W pos ? R w?d to the input vectors U ? R w?d  [u   . . .  u w ] U  U  W pos .Instead of deterministic, sinusoidal encodings, which were originally proposed by [#b31]Vaswani et al. (2017), we use fully learnable positional encodings, as we observed that they perform better for all datasets presented in this work. Interestingly,. e layer normalization after computing selfattention and after the feedforward part of each encoder block, leading to significant performance gains over batch normalization, as originally proposed by [#b31]Vaswani et al. (2017). However, here we instead use batch normalization, because it can mitigate the effect of outlier values in time series, an issue that does no </p>
<p> a transformer-based framework for multivariate time series representation learning </p><p> on the difficulty of training recurrent neural networks </p><p> t) networks practically only retain information from a limited number of time steps stored inside their hidden state (vanishing gradient problem (Hochreiter, 1998[#b25]Pascanu et al., 2013)), and thus the context used for representing each sequence element is inevitably local. ? Multiple attention heads can consider different rep </p>
<p> a transformer-based framework for multivariate time series representation learning </p><p> ts-chief: a scalable and accurate forest algorithm for time series classification </p><p> ke in domains such as Computer Vision or Natural Language Processing (NLP), the dominance of deep learning for time series is far from established in fact, nondeep learning methods such as TSCHIEF [#b28](Shifaz et al., 2020), HIVECOTE (Lines et al., 2018), and ROCKET (Dempster et al., 2020). ed approaches. RELATED WORKRegression and classification of time series Currently, nondeep learning methods such as TSCHIEF [#b28](Shifaz et al., 2020), HIVECOTE (Lines et al., 2018), and ROCKET (Dempster et al., 2020) </p>
<p> a transformer-based framework for multivariate time series representation learning </p><p> unsupervised learning of semantic audio representations </p><p> hich aims at reconstructing correlation matrices between the variables of the multivariate time series input. They use and evaluate their method only for the task of anomaly detection.Finally, [#b15]Jansen et al. (2018) rely on a triplet loss and the idea of temporal proximity (the loss rewards similarity of representations between proximal segments and penali </p>
<p> Bootstrap Your Own Latent A New Approach to Self-Supervised Learning </p><p> bootstrap latent-predictive representations for multitask reinforcement learning </p><p> [pos is Related work] "bibr" target"b36"[].Our approach has some similarities with Predictions of Bootstrapped Latents (PBL, [#b48][49]), a selfsupervised representation learning technique for reinforcement learning (RL). PBL jointly trains the agent's history representation and an encoding of </p>
<p> Bootstrap Your Own Latent A New Approach to Self-Supervised Learning </p><p> sgdr: stochastic gradient descent with warm restarts </p><p>  </p>
<p> Bootstrap Your Own Latent A New Approach to Self-Supervised Learning </p><p> pseudo-label: the simple and efficient semi-supervised learning method for deep neural networks </p><p> han contrastive methods we suspect that not relying on negative pairs is one of the leading reasons for its improved robustness. While previous methods based on bootstrapping have used pseudolabels [#b15][16], cluster indices [17] or a handful of labels [18,19, </p>
<p> Bootstrap Your Own Latent A New Approach to Self-Supervised Learning </p><p> data-efficient image recognition with contrastive predictive coding </p><p> [pos is Related work] mage generation may not be necessary for representation learning.Among discriminative methods, contrastive methods [9,10,[#b31]32,33,34,11,35,. [pos is Related work] et, this time using label information. We follow the semisupervised protocol of [[#b31]] detailed in Appendix D.1, and use the same fixed splits of respectively 1 and 10 of ImageNet labeled training data as in [8]r </p>
<p> Bootstrap Your Own Latent A New Approach to Self-Supervised Learning </p><p> discriminative unsupervised feature learning with convolutional neural networks </p><p> [pos is Related work] ef type"bibr" target"b42"[43], image jigsaw puzzle [44], image superresolution [45], and geometric transformations [#b45][] have been shown to be useful. Yet, even with suitable architectures [48], these m </p>
<p> Bootstrap Your Own Latent A New Approach to Self-Supervised Learning </p><p> revisiting self-supervised visual representation learning </p><p> [pos is Related work] "b44"[45], and geometric transformations [] have been shown to be useful. Yet, even with suitable architectures [#b47][48], these methods are being outperformed by contrastive methods [37,8,ref type"bibr" t. [pos is Related work] ns1.0"Linear evaluation on ImageNetWe first evaluate BYOL's representation by training a linear classifier on top of the frozen representation, following the procedure described in [#b47][], </p>
<p> Bootstrap Your Own Latent A New Approach to Self-Supervised Learning </p><p> on the difficulty of training recurrent neural networks </p><p>  </p>
<p> Bootstrap Your Own Latent A New Approach to Self-Supervised Learning </p><p> unsupervised visual representation learning by context prediction </p><p> [pos is Related work] s. Related workMost unsupervised methods for representation learning can be categorized as either generative or discriminative [#b22][]. Generative approaches to representation learning build a distribution over data and latent embedding and use the learne. [pos is Related work] to trivial solutions. Some selfsupervised methods are not contrastive but rely on using auxiliary handcrafted prediction tasks to learn their representation. In particular, relative patch prediction [#b22][], colorizing grayscale images [], image </p>
<p> Bootstrap Your Own Latent A New Approach to Self-Supervised Learning </p><p> batch normalization: accelerating deep network training by reducing internal covariate shift </p><p>  </p>
<p> Bootstrap Your Own Latent A New Approach to Self-Supervised Learning </p><p> revisiting self-supervised visual representation learning </p><p> [pos is Related work] "b44"[45], and geometric transformations [] have been shown to be useful. Yet, even with suitable architectures [#b47][48], these methods are being outperformed by contrastive methods [37,8,ref type"bibr" t. [pos is Related work] ns1.0"Linear evaluation on ImageNetWe first evaluate BYOL's representation by training a linear classifier on top of the frozen representation, following the procedure described in [#b47][], </p>
<p> Bootstrap Your Own Latent A New Approach to Self-Supervised Learning </p><p> s4l: self-supervised semisupervised learning </p><p>  </p>
<p> leveraging text data using hybrid transformer-lstm based end-to-end asr in transfer learning </p><p> independent language model architecture for end-toend asr </p><p> www.teic.orgns1.0"In this work, we study leveraging extra text data to improve lowresource endtoend ASR under crosslingual transfer learning setting. To this end, we extend our prior work [#b0][1], and propose a hybrid TransformerLSTM based architecture. This architecture not only takes advantage of the highly effective encoding capacity of the Transform. rget"b3"4] and rescoring stages [5]. Such techniques not only require external language models but also lead to a slow inference. To tackle this problem, [#b0][1] has proposed long short term memory (LSTM)based encoderdecoder architecture which allows improving the LM capacity of the decoder using the extra text data. Ho. ttention. Therefore, it is not straightforward to employ extra text data to improve the decoder.In this work, we propose a hybrid TransformerLSTM architecture which combines the advantages of [#b0][1] and [6]. It not only has a high encoding capacity of the Transformer but also benefits from the extra text data due to the L. he target language. Lastly, the extra text data is used to boost the decoder of the transferred model.The paper is organized as follows. Section 2 describes baseline architectures mentioned in [#b0][1] and [6]. Then, the proposed techniques are presented in Section 3. Experimental setup and results are presented in Section 4. cludes our work. Baseline architectures 2.1. LSTMbased encoderdecoder architectureA LSTMbased encoderdecoder architecture [#b0][1], denoted as A1 in the rest of this paper, consists of a Bidirectional LSTM encoder and a LSTMbased decoder which are shown in Fig. 1.. states at time step i  1 and i respectively, embedding() and proj() are embedding and projection layers respectively. Figure 1 LSTMbased encoderdecoder architecture (A1) [#b0][1], where the decoder acts as an independent language model.From Equation (1), the LSTM is only conditioned on the previous decoding hidden state and previo. M is only conditioned on the previous decoding hidden state and previous decoding output. In other words, the LSTM acts as an independent language model that can be easily updated with textonly data [#b0][1]. Transformer encoderdecoder architectureTransformer has been proposed in ref type. th two main steps. In the first step, we merge the extra text and the labeled data together to finetune the transferred model. This avoids a socalled catastrophic forgetting problem as mentioned in [#b0][1]. Specifically, at each training iteration, we mix a batch of labeled data consisting of B labeled utterances with a batch of text data consisting of Btext utter. denote the ASR loss and LM loss generated by the labeled data and text data respectively. In the second step, the model is further finetuned with the labeled data of the target language. Similar to [#b0][1], we empirically found that the second step is necessary to improve overall performance.Step 1 Finetune E2E modelStep   div xmlns"htt </p>
<p> leveraging text data using hybrid transformer-lstm based end-to-end asr in transfer learning </p><p> why self-attention? a targeted evaluation of neural machine translation architectures </p><p> the same sequence regardless of their distance. In contrast, although in theory LMST can model longrange dependence, in practice it faces difficulty to capture dependencies of fardistance elements [#b16][17] which limits its modeling capacity for long sequences such as acoustic signal. Second, by relying entirely on feedforward components, the Transformer model a </p>
<p> leveraging text data using hybrid transformer-lstm based end-to-end asr in transfer learning </p><p> joint ctc-attention based end-to-end speech recognition using multi-task learning </p><p> ilterbank coefficients with pitch as input features, and 500 BytePair Encoding (BPE) units are used as output units. For all E2E architectures, the acoustic features are processed by the VGG network [#b21][22]. Detailed setting of architectures can be seen in Table 2. Each BLSTM layer has 320 cells, while each LSTM layer of th </p>
<p> leveraging text data using hybrid transformer-lstm based end-to-end asr in transfer learning </p><p> accelerating neural transformer via an average attention network </p><p> which is conditioned on the encoder output. In contrast, the LSTMbased decoder (in Section 2.1) can be easily boosted using the text data. Another issue of the Transformer decoder is slow inference [#b17][18]. Specifically, to generate an output yi, the decoder needs to process all previous decoding units y1i1. On the other hand, the LSTMbased decoder has faster </p>
<p> leveraging text data using hybrid transformer-lstm based end-to-end asr in transfer learning </p><p> speech-transformer: a norecurrence sequence-to-sequence model for speech recognition </p><p> architectureTransformer has been proposed in [6] for sequencetosequence modeling in natural language processing tasks, then adopted to the ASR task in [#b11][]. The model architecture, denoted as A2, is shown in Fig. 2. The encoder is sh </p>
<p> leveraging text data using hybrid transformer-lstm based end-to-end asr in transfer learning </p><p> end-to-end speech recognition with word-based rnn language models </p><p> e language. This is a common scenario in realworld applications.The extra text is usually employed to train language models (LM) applied during decoding [[#b2]] and rescoring stages [5]. Such techniques not only require external language models bu </p>
<p> leveraging text data using hybrid transformer-lstm based end-to-end asr in transfer learning </p><p> cross-language knowledge transfer using multilingual deep neural network with shared hidden layers </p><p> ge model decoder. To further benefit from the labeled data from another language, we employ crosslingual transfer learning, which is a popular approach to address the limited resource problem in ASR [#b6][], on t </p>
<p> leveraging text data using hybrid transformer-lstm based end-to-end asr in transfer learning </p><p> layer normalization </p><p> on (DotP rodAtt) as follows,DotP rodAtt(Q, K, V )  sof tmax( QK T d k )V(4)where d k is the hidden dimension. Besides, layer normalization [#b13][14] and residual connection [15] are introduced to each encoderblock for effective training.The decoder is shown in t </p>
<p> leveraging text data using hybrid transformer-lstm based end-to-end asr in transfer learning </p><p> joint ctc-attention based end-to-end speech recognition using multi-task learning </p><p> ilterbank coefficients with pitch as input features, and 500 BytePair Encoding (BPE) units are used as output units. For all E2E architectures, the acoustic features are processed by the VGG network [#b21][22]. Detailed setting of architectures can be seen in Table 2. Each BLSTM layer has 320 cells, while each LSTM layer of th </p>
<p> leveraging text data using hybrid transformer-lstm based end-to-end asr in transfer learning </p><p> layer normalization </p><p> on (DotP rodAtt) as follows,DotP rodAtt(Q, K, V )  sof tmax( QK T d k )V(4)where d k is the hidden dimension. Besides, layer normalization [#b13][14] and residual connection [15] are introduced to each encoderblock for effective training.The decoder is shown in t </p>
<p> leveraging text data using hybrid transformer-lstm based end-to-end asr in transfer learning </p><p> end-to-end speech recognition with word-based rnn language models </p><p> e language. This is a common scenario in realworld applications.The extra text is usually employed to train language models (LM) applied during decoding [[#b2]] and rescoring stages [5]. Such techniques not only require external language models bu </p>
<p> USMPep: Universal Sequence Models for Major Histocompatibility Complex Binding Afﬁnity Prediction </p><p> udsmprot: universal deep sequence models for protein classification </p><p> headII. METHODS A. USMPep Universal Sequence Models for Peptide Binding PredictionThe approach builds on the UDSMProtframework [#b11][12] and related work in natural language processing [13]. We distinguish two variants of our approach, either train the regre. he model is finetuned on the regression task of MHC binding prediction by replacing the output layer with a concat pooling layer and two fully connected layers. The setup closely follows that used in [#b11][12], where protein properties were predicted. The smaller dataset sizes and shorter sequence lengths in the peptide setting (in comparison to protein classificati. lding up large contexts and were accounted for by the reduction of the number of layers from 3 to 1, of the number of hidden units from 1150 to 64 and of the embedding size from 400 to 50. Similar to [#b11][12], the training procedure included 1cycle learning rate scheduling [15] and discriminative learning rates ref type"bibr". o one MHC molecule. These results stress that further efforts might be required to truly leverage the potential of unlabeled peptide data in order to observe similar improvements as seen for proteins [#b11][12] in particular for small datasets.  Turning to MHC Class II binding prediction, we aim to demonstrate the universality of our approach beyond its applicability. age sites. Second, even we evaluated on protein data, the protein language model only reaches an accuarcy of 0.137, which is is considerably lower than the accuracy of 0.41 reported in the literature [#b11][12]. This effect is a direct consequence of the considerably smaller model size (1 instead of 3 layers 64 instead of 1150 hidden units embedding size of 50 inst </p>
<p> USMPep: Universal Sequence Models for Major Histocompatibility Complex Binding Afﬁnity Prediction </p><p> introduction to information retrieval </p><p> bibr" target"b20"[21]. The difference between both evaluation approaches is related to the discussion about micro vs. macro averages for the evaluation of multiclass classification problems [#b21][22]. In particular, there are two fundamental differences between both evaluation approaches First, the datasets enter the overall score with different weights d </p>
<p> USMPep: Universal Sequence Models for Major Histocompatibility Complex Binding Afﬁnity Prediction </p><p> introduction to information retrieval </p><p> bibr" target"b20"[21]. The difference between both evaluation approaches is related to the discussion about micro vs. macro averages for the evaluation of multiclass classification problems [#b21][22]. In particular, there are two fundamental differences between both evaluation approaches First, the datasets enter the overall score with different weights d </p>
<p> USMPep: Universal Sequence Models for Major Histocompatibility Complex Binding Afﬁnity Prediction </p><p> mhcseqnet: a deep neural network model for universal mhc binding prediction </p><p> ning protocol to achieve its performance.Only limited benchmarking results are available, which makes it difficult to realistically assess its prediction performance. The very recent MHCSeqNet [#b10][11] also uses a recurrent architecture, again with pretrained rather than learned embeddings, incorporating both peptide and allele sequence to train a single pre </p>
<p> USMPep: Universal Sequence Models for Major Histocompatibility Complex Binding Afﬁnity Prediction </p><p> the role of the proteasome in generating cytotoxic t-cell epitopes: insights obtained from improved predictions of proteasomal cleavage </p><p> d a dataset of simulated proteasomecleaved peptides to pretrain USMpep on a large corpus of unlabeled sequences. We filtered the SwissProt release 2018 10 for the human proteome and employed NetChop [#b22][23] to obtain proteasome cleavage sites for these proteins. The stochastic process of protein slicing was modeled by cutting with the cleavage probability provide </p>
<p> USMPep: Universal Sequence Models for Major Histocompatibility Complex Binding Afﬁnity Prediction </p><p> the immune epitope database (iedb): 2018 update </p><p> a) Kim14 is a commonly used binding affinity dataset compiled by [16], available on the Immune Epitope Database (IEDB)1  [#b16][17], and is split into a nonoverlapping training (BD2009) and test set (Blind). Similar peptides (of same length with at least 80 sequence identity) shared by t </p>
<p> USMPep: Universal Sequence Models for Major Histocompatibility Complex Binding Afﬁnity Prediction </p><p> a comprehensive review and performance evaluation of bioinformatics tools for hla class i peptide-binding prediction </p><p>  </p>
<p> USMPep: Universal Sequence Models for Major Histocompatibility Complex Binding Afﬁnity Prediction </p><p> a comprehensive review and performance evaluation of bioinformatics tools for hla class i peptide-binding prediction </p><p>  </p>
<p> USMPep: Universal Sequence Models for Major Histocompatibility Complex Binding Afﬁnity Prediction </p><p> the role of the proteasome in generating cytotoxic t-cell epitopes: insights obtained from improved predictions of proteasomal cleavage </p><p> d a dataset of simulated proteasomecleaved peptides to pretrain USMpep on a large corpus of unlabeled sequences. We filtered the SwissProt release 2018 10 for the human proteome and employed NetChop [#b22][23] to obtain proteasome cleavage sites for these proteins. The stochastic process of protein slicing was modeled by cutting with the cleavage probability provide </p>
<p> USMPep: Universal Sequence Models for Major Histocompatibility Complex Binding Afﬁnity Prediction </p><p> engineering patient-specific cancer immunotherapies </p><p> efore, the prediction if a certain peptide binds is a very challenging task that is, however, a crucial sub task for neoantigen identification for practical realizations of personalized immunotherapy [#b0][1].The MHC binding prediction is a wellestablished problem in bioinformatics with a large number of existing algorithmic solutions. Although many of them s </p>
<p> USMPep: Universal Sequence Models for Major Histocompatibility Complex Binding Afﬁnity Prediction </p><p> universal language model fine-tuning for text classification </p><p> iversal Sequence Models for Peptide Binding PredictionThe approach builds on the UDSMProtframework [12] and related work in natural language processing [#b12][13]. We distinguish two variants of our approach, either train the regression from scratch or employ language model pretraining. A language model tries to predict. 50. Similar to [12], the training procedure included 1cycle learning rate scheduling [15] and discriminative learning rates [#b12][13] during finetuning. Target variables for the regression model were logtransformed halfmaximal inhibitory concentration (IC 50 )values and a modified MSE los </p>
<p> speech driven talking face generation from a single image and an emotion condition </p><p> generating talking face landmarks from speech </p><p> can automatically generate talking faces from speech in order to provide the visual cues when they are not available [5], [6], [#b6][7], [8], [9], [10], [11]. for a single speaker. Another twostage system is proposed by Chen et al. [5]. The system first predicts 68 face landmarks from speech using an LSTMbased network [#b6][7], and then predicts a few talking face images from the condition image and the face landmarks. They employ a discriminator network to improve image quality. In a </p>
<p> speech driven talking face generation from a single image and an emotion condition </p><p> multimodal databases of everyday emotion: facing up to complexity </p><p> "B. Multimodal Human Emotion PerceptionEmotion perception from auditory and visual stimuli has been examined in recent years. Existing work [23], [#b23][24], [25], [26], [27] concludes that different moda </p>
<p> speech driven talking face generation from a single image and an emotion condition </p><p> albumentations: fast and flexible image augmentations </p><p> to different parts of the scene. This allows us to model the natural head movements in addition to facial expressions. During training, we randomly augmented the data using the Albumentations library [#b34][35] to improve the generalization capability of our network. The data augmentation includes randomly changing brightness, contrast, gamma, hue, saturation, and va </p>
<p> speech driven talking face generation from a single image and an emotion condition </p><p> auditory and auditory-visual intelligibility of speech in fluctuating maskers for normal-hearing and hearingimpaired listeners </p><p> on does not solely depend on the acoustic signal. Visual cues, when present, also play a vital role. The presence of visual cues improves speech comprehension [1], [#b1][2], [3], [4] in noisy environments and for the hardofhearing population. Consequently, res </p>
<p> speech driven talking face generation from a single image and an emotion condition </p><p> analysis of emotion recognition using facial expressions, speech and multimodal information </p><p> recent years. Existing work [23], [24], [25], [26], [#b26][27] concludes that different modalities complement each other, and there are also intermodal effects. Cowie [26] showed that </p>
<p> speech driven talking face generation from a single image and an emotion condition </p><p> audiovisual integration of emotional signals from others' social interactions </p><p> b27"[28] investigated human responses to emotions expressed by the body and voice of humanoid robots, showing that crossmodal incongruency decreased emotion recognition accuracy. Piwek et al. [#b28][29] found that subjects weighted visual cues higher in emotion judgments when presented emotionally incongruent audiovisual clips with happy or angry emotion. How </p>
<p> speech driven talking face generation from a single image and an emotion condition </p><p> few-shot adversarial learning of realistic neural talking head models </p><p> b6"[7], and then predicts a few talking face images from the condition image and the face landmarks. They employ a discriminator network to improve image quality. In another work, Egor et al. [#b18][19] proposed a stylebased landmarktoimage conversion method using generative adversarial networks (GANs) with a few shots of the target face. This method, howe </p>
<p> speech driven talking face generation from a single image and an emotion condition </p><p> image quality assessment: from error visibility to structural similarity </p><p> p C. Objective EvaluationWe evaluated the image quality of the generated videos using Peak SNR (PSNR) and Structural Similarity (SSIM) [#b35][36] between the generated video frames and the groundtruth video frames. To measure the audiovisual synchronization, we used the normalized landmarks distance (N </p>
<p> speech driven talking face generation from a single image and an emotion condition </p><p> end-to-end generation of talking faces from noisy speech </p><p> ype"bibr" target"b9"[10] that focus on improving the realness of video frames, the continuity between generated frames, and the synchronization between audio and visual data. Eskimez et al. [#b20][21] proposed an endtoend talking face generation system that is robust to noisy speech input. The system contains a frame discriminator to improve image quality. . Figure 1 shows the system overview, which employs the generative adversarial network (GAN) framework. Our generator network architecture is built based on our previous work [#b20][21], with a modification to accept the emotion condition input. For discriminator networks, we use one discriminator to distinguish the emotions expressed in vide. e, and emotion encoders, and a video decoder.1) Speech Encoder The speech encoder processes the input speech waveform and outputs a speech embedding. It follows the original implementation of [#b20][21]   WGANGPGenerator Fig. 1 Overview of the proposed neural network. ding sequence.2) Image Encoder The image encoder computes an image embedding from the input condition face image. The architecture follows the original implementation without any modification [#b20][21]. It contains six layers of 2D convolutional layers with the following number of filters, kernel sizes, and downsampling factors (64, 3, 2), (128, 3, 2), (2. e embedding. This noise embedding models the head movements that are not correlated with the speech, image, and the emotion condition.5) Video Decoder We modify the video decoder described in [#b20][21] to accept the additional emotion embedding. We concatenate the speech, image, noise and emotion embeddings, and feed them into the decoder. For each time step. c.orgns1.0"D. Objective FunctionsOur system employs multiple objective functions that focus on different aspects of the generated videos a mouth region mask (MRM) loss proposed in [#b20][21] to improve mouthaudio synchronization, a perceptual loss to improve image quality, a frame GAN loss for image quality, and an emotion GAN loss for emotion ex. type"bibr" target"b35"[36] between the generated video frames and the groundtruth video frames. To measure the audiovisual synchronization, we used the normalized landmarks distance (NLMD) [#b20][21] between landmarks extracted from the generated and groundtruth video frames.The baseline method generates 96x128 images, while our method yields 128x1 </p>
<p> speech driven talking face generation from a single image and an emotion condition </p><p> the perceptual and cognitive role of visual and auditory channels in conveying emotional information </p><p> n that predicting emotions purely from speech audio is quite difficult for untrained people [14] and that we heavily rely on visual cues in emotion interpretation [#b14][15]. Therefore, to make the visual rendering more realistic and to improve speech communication, it is important for automatic talking face generation systems to </p>
<p> speech driven talking face generation from a single image and an emotion condition </p><p> albumentations: fast and flexible image augmentations </p><p> to different parts of the scene. This allows us to model the natural head movements in addition to facial expressions. During training, we randomly augmented the data using the Albumentations library [#b34][35] to improve the generalization capability of our network. The data augmentation includes randomly changing brightness, contrast, gamma, hue, saturation, and va </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> unpaired image-to-image translation using cycleconsistent adversarial networks </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] sing interest in learning superresolution models without paired data in the unsupervised setting []. Based on CycleGAN [#b58][56], Yuan et al. [43] propose a CinCGAN model to generate HR images without paired data. Recently, some blind SR methods ref. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] opposite mappings simultaneously to enhance the performance of language translation. Recently, this scheme has also been used to perform image translation without paired training data, e.g., CycleGAN [#b58][56] and DualGAN [42]. Specifically, a cycle consistency loss is proposed to avoid the mode collapse issue of GAN methods ref. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] GAN [#b58][56] and DualGAN [42]. Specifically, a cycle consistency loss is proposed to avoid the mode collapse issue of GAN methods [#b58][] and help minimize the distribution divergence. However, these methods cannot be di. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] m CycleGAN based SR MethodsThere are several differences and advantages of DRN compared to CycleGAN based SR methods. First, CycleGAN based methods [[#b58]] use a cycle consistency loss to avoid the possible mode collapse issue when solving the underconstrained image translation problem ref type"bibr" target"b. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] r" target"b45"[[#b58]] use a cycle consistency loss to avoid the possible mode collapse issue when solving the underconstrained image translation problem [#b58][56]. Unlike these methods, we seek to improve the performance of our SR model by adding an extra constraint, which reduces the possible function space by mapping. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] train a DRNAdapt model for each kind of unpaired data, i.e., Nearest data, BD data, and video frames collected from YouTube. Thus, there are 3 DRNadapt models in total. And We also train a CinCGAN [#b58][56] model for each kind of unpaired data for comparison. Based on pretrained DRNS, We train our DRNAdapt models with a learning rate of 10 4 and the data ratio </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> image super-resolution using very deep residual channel attention networks </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] ant task that aims at learning a nonlinear mapping to reconstruct highresolution (HR) images from lowresolution (LR) images. Based on DNNs, many methods have been proposed to improve SR performance [#b53][]ref. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] ve the SR performance, one can design effective models by increasing the model capacity, e.g., EDSR [26], DBPN [16], and RCAN [#b53][51]. However, these methods still suffer from the large space issue of possible mapping functions, resulting in the limited performance without producing sharp te. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] including the interpolationbased approaches [19] and reconstructionbased methods [[#b53]]. Haris et al. [16] propose a backprojection network (DBPN) that consists of several upand downsampling layers to iterati. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] ris et al. [16] propose a backprojection network (DBPN) that consists of several upand downsampling layers to iteratively produce LR and HR images. Zhang et al. [#b53][51] propose the channel attention mechanism to build a deep model called RCAN to further improve the performance of SR. However, these methods still have a very l. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images]  upscaling (See Figure 3) and 3 blocks for 8 upscaling. Unlike the baseline UNet, we build each basic block using B residual channel attention block (RCAB) [#b53][51] to improve the model capacity. Following [], we add additional outputs to produ. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] P, D)  H dual closedloop, according to the architecture design of the primal model, there are 2 dual models for 4 SR and 3 dual models for 8 SR, respectively. Let B be the number of RCABs [#b53][51] and F be the number of base feature channels. For 4 SR, we set B  30 and F  16 for DRNS and B  40 and F  20 for DRNL. For 8 SR, we set B  30 and F . [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] put patches of size 48  48 from LR images and the corresponding HR patches as the paired training data, and augment the training data following the method in [[#b53]]. As shown in TableTest data. For quantitative comparison on paired data, we evaluate differe </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] represent the convolution layer with the stride of 2. Following the settings of EDSR [26], we build the Upsampler with one convolution layer and one pixelshuffle [#b35][33] layer to upscale the feature maps. Moreover, we use h and w to represent the height and width of the input LR images. Thus, the shape of output images should </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> contour detection and hierarchical image segmentation </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] plementation DetailsWe compare different methods on five benchmark datasets, including SET5 [3], SET14 [47], BSDS100 [#b3][1], URBAN100 [21] and MANGA109 [29]. Two commonly used image quality metrics are adopted. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] n the supplementary). Effect of  on DRNWe conduct an experiment to investigate the impact of the hyperparameter  in Eqn. [#b3](1). From Table 4, when we increase  from 0.001 to 0.1, the dual regression loss gradually becomes more important and provi. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] parison on paired data, we evaluate different SR models using five benchmark datasets, including SET5 [3], SET14 [47], BSDS100 [#b3][1], URBAN100 [21] and MANGA109 [29]. Implementation details. For training, we apply Adam </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> deep networks for image super-resolution with sparse prior </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] r 8 upscaling. Unlike the baseline UNet, we build each basic block using B residual channel attention block (RCAB) [51] to improve the model capacity. Following [#b41][], we add additional outputs to produce images at the corresponding scale (i.e., 1, 2, and 4 images) and apply the pr </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> dual supervised learning </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] me seeks to adapt SR models to new LR data by exploiting both the realworld LR data and the paired synthetic data.Dual learning. Dual learning methods [[#b42]] contain a primal model and a dual model to learn two opposite mappings simultan. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] ons. More discussions can be referred to Remark 1. We highlight that the derived generalization bound of the dual regression scheme, where the loss function is bounded by [ C], is more general than [#b42][40]. Moreover, this generalization bound is tight when training data is sufficient, and the primal and dual models are powerful enough.   Remark 1 Based on the de. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] neralization. Moreover, the generalization bound of dual learning is more general for the case that the loss function L P (P (x), y)  L D (D(P (x)), x) is bounded by [ C], which is different from [#b42][40].  Remark 2 Based on the definition of Rademacher complexity, the capacity of the function spacehead </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> double forward propagation for memorized batch normalization </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] ns1.0"IntroductionDeep neural networks (DNNs) have been the workhorse of many realworld applications, including image classification [[#b16]],. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] space B. Model Details of Dual Regression NetworkDeep neural networks (DNNs) have achieved great success in image classification [#b16][], image generation ref type"bibr" targ </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> deep mutual learning </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] data and the paired synthetic data.Dual learning. Dual learning methods [[#b55]] contain a primal model and a dual model to learn two opposite mappings simultaneously to enhance the performance of language translation. Recently, this scheme </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> adversarial learning with local coordinate coding </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] ref and DualGAN [42]. Specifically, a cycle consistency loss is proposed to avoid the mode collapse issue of GAN methods [[#b6]] and help minimize the distribution divergence. However, these methods cannot be directly applied to the standard SR problem. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] get"b16"[], image generation [[#b6]], and image restoration []. In this paper, we propose a novel Dual Regression Netwo </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> unsupervised image superresolution using cycle-in-cycle generative adversarial networks </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] possible space of the mapping functions to improve the training of SR models becomes an important problem.Second, it is hard to obtain a promising SR model when the paired data are unavailable [#b45][]. Note that most SR methods rely on the paired training data, i.e., HR images with their Bicubicdegraded LR counterpar. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] or realworld applications can be very challenging. More critically, if we directly apply existing SR models to realworld data, they often incur a severe adaptation problem and yield poor performance [#b45][]. Therefore, how to effectively exploit the unpaired data to adapt SR models to realworld applications becomes an urge. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] v xmlns"httpwww.teic.orgns1.0"Unsupervised superresolution.There is an increasing interest in learning superresolution models without paired data in the unsupervised setting [#b45][]. Based on CycleGAN [56], Yuan et al. [#b45][43] p. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] ut paired data in the unsupervised setting [#b45][]. Based on CycleGAN [56], Yuan et al. [#b45][43] propose a CinCGAN model to generate HR images without paired data. Recently, some blind SR methods [2,ref type"bibr" tar. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] orld LR data. More critically, the degradation methods of LR images are often unknown, making this problem very challenging. In this case, existing SR models often incur the severe adaptation problem [#b45][]. To alleviate this issue, we propose an efficient algorithm to adapt SR models to the new LR data. The training algori. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] rgns1.0"Differences from CycleGAN based SR MethodsThere are several differences and advantages of DRN compared to CycleGAN based SR methods. First, CycleGAN based methods [#b45][] use a cycle consistency loss to avoid the possible mode collapse issue when solving the underconstrained image transl </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> on single image scale-up using sparse-representations </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] w.teic.orgns1.0"Datasets and Implementation DetailsWe compare different methods on five benchmark datasets, including SET5 [3], SET14 [#b49][47], BSDS100 [1], URBAN100 [21] and MANGA109 [29].. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] n in TableTest data. For quantitative comparison on paired data, we evaluate different SR models using five benchmark datasets, including SET5 [3], SET14 [#b49][47], BSDS100 [1], URBAN100 [21] and MANGA109 [29]. I </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] represent the convolution layer with the stride of 2. Following the settings of EDSR [26], we build the Upsampler with one convolution layer and one pixelshuffle [#b35][33] layer to upscale the feature maps. Moreover, we use h and w to represent the height and width of the input LR images. Thus, the shape of output images should </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> contour detection and hierarchical image segmentation </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] plementation DetailsWe compare different methods on five benchmark datasets, including SET5 [3], SET14 [47], BSDS100 [#b3][1], URBAN100 [21] and MANGA109 [29]. Two commonly used image quality metrics are adopted. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] n the supplementary). Effect of  on DRNWe conduct an experiment to investigate the impact of the hyperparameter  in Eqn. [#b3](1). From Table 4, when we increase  from 0.001 to 0.1, the dual regression loss gradually becomes more important and provi. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] parison on paired data, we evaluate different SR models using five benchmark datasets, including SET5 [3], SET14 [47], BSDS100 [#b3][1], URBAN100 [21] and MANGA109 [29]. Implementation details. For training, we apply Adam </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> blind super-resolution kernel estimation using an internal-gan </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] GAN [56], Yuan et al. [43] propose a CinCGAN model to generate HR images without paired data. Recently, some blind SR methods [#b4][] were proposed to learn the unknown degradation methods. However, these methods often totally discard the paired syntheti. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] 1, for each iteration, we first sample m unpaired realworld data from S U and n paired synthetic data from S P , respectively. Then, we train our model endtoend by minimizing the objective in Eqn. [#b4](2). For convenience, we define the data ratio of unpaired data as  m(m  n).(3)Since paired syntheti </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> deep image prior </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] wo limitations.First, learning the mapping from LR to HR images is typically an illposed problem since there exist infinitely many HR images that can be downscaled to obtain the same LR image [#b38][36]. Thus, the space of the possible functions that map LR to HR images becomes extremely large. As a result, the learning performance can be limited since learni </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> rectifier nonlinearities improve neural network acoustic models </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] downsampling operation, which is much simpler than the primal task for learning the upscaling mapping. Thus, we design the dual model with only two convolution layers and a LeakyReLU activation layer [#b30][28], which has much lower computation cost than the primal model but works well in practice (See results in Section 5). div xmlns"httpwww.teic.org </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> deep residual learning for image recognition </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images]  IntroductionDeep neural networks (DNNs) have been the workhorse of many realworld applications, including image classification [#b20][18,14,9,15,27, </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> cubic splines for image interpolation and digital filtering </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] httpwww.teic.orgns1.0"Related WorkSupervised superresolution. Many efforts have been made to improve the performance of SR, including the interpolationbased approaches [#b21][19] and reconstructionbased methods []. Har </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> feedback network for image superresolution </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] made to improve the performance of SR, including the interpolationbased approaches [19] and reconstructionbased methods [[#b27]]. Haris et al. [16] propose a backprojection network (DBPN) that consists of severa </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> enhanced deep residual networks for single image super-resolution </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] r mapping to reconstruct highresolution (HR) images from lowresolution (LR) images. Based on DNNs, many methods have been proposed to improve SR performance [[#b28]]. However, these methods may suffer from. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] rformance can be limited since learning a good solution in such a large space is very hard. To improve the SR performance, one can design effective models by increasing the model capacity, e.g., EDSR [#b28][26], DBPN [16], and RCAN [51]. However, these methods still suffer from the large space. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] .0"Training MethodTraining method on paired data. Given paired training data, we follow the learning scheme of supervised SR methods [[#b28]] and train model by minimizing Eqn. ( 1). More details are shown in Section 5 and the supplementary.Trainin. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] ch as PSNR and SSIM [38]. Following [37], we train our models on DIV2K [34] and Flickr2K [#b28][26] datasets. Comparison with Stateoftheart MethodsWe compare our method with stat. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images]  to represent the convolution layer with the kernel size of 1  1 and 3  3, respectively. We use Conv s2 to represent the convolution layer with the stride of 2. Following the settings of EDSR [#b28][26], we build the Upsampler with one convolution layer and one pixelshuffle [33] layer to upscale the feature maps. Moreover. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] utput images should be 8h  8w for the 8 model. Training data. Following [37], we train our model on DIV2K [34] and Flickr2K [#b28][26] datasets, which contain 800 and 2650 training images separately. We use the RGB input patches of size 48  48 from LR images and the corresponding HR patches. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] ning images separately. We use the RGB input patches of size 48  48 from LR images and the corresponding HR patches as the paired training data, and augment the training data following the method in [#b28][]. As shown in TableTest data. For quantitative compa </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> dense regression network for video grounding </p><p>  </p>
<p> closed-loop matters: dual regression networks for single image super-resolution </p><p> auto-embedding generative adversarial networks for high resolution image synthesis </p><p> [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] b46"44,6] and many other applications [[#b13]]. Recently, image superresolution (SR) has become an important task that aims at learning a nonlinear mapping to recons. [pos is Deep neural networks have exhibited promising performance in image super-resolution (SR) by learning a nonlinear mapping function from low-resolution (LR) images] image classification [], image generation [#b13][], and image restoration []. In this paper, </p>
<p> Limago: an FPGA-based Open-source 100 GbE TCP IP Stack </p><p> scalable 10gbps tcp/ip stack architecture for reconfigurable hardware </p><p> llenge, in this paper we introduce Limago, an opensource 100 Gbits TCPIP network stack on an FPGA. Limago explores the changes needed to upgrade an existing opensource TCPIP stack from 10 Gbits [#b10][11] to 100 Gbits, but maintaining the same highproductivity design methodology, based on VivadoHLS, that was utilized in the previous design. In doing so, Lima. 48 connections working as a server. It also can send up to 40 Gbits but only receive up to 4 Gbits. The starting point for Limago is a 10 Gbits TOE written by Sidler et al. in C using VivadoHLS [#b10][11], [29]. IV. LIMAGO ARCHITECTUREFigure ref type"figure. hitecture of the TOE (Figure 2). It is divided into three parts, the incoming data path (Rx Engine), the outgoing data path (Tx Engine), and the statekeeping data structures [#b10][11], [29]. The dash boxes are optional modules that can be enabled at synthesis. div xmlns"httpwww.teic.orgn </p>
<p> Limago: an FPGA-based Open-source 100 GbE TCP IP Stack </p><p> azure accelerated networking: smartnics in the public cloud </p><p> nts is provided by Microsoft Catapult [3], a deployment of FPGAs in the cloud that has evolved through several generations [4], [#b4][5]. The current version inserts an FPGA on the data path between the top of rack (ToR) switch and the server machine. Hence, all network traffic in and out of the. A is then used to augment the network functionality with system and applicationlevel features. For instance, it can be used as a customizable smartNIC to offload network virtualization functionality [#b4][5], applicationlevel functionality such as RDMA packet processing to support keyvalue stores [6], or for distributed machine </p>
<p> Limago: an FPGA-based Open-source 100 GbE TCP IP Stack </p><p> serving dnns in real time at datacenter scale with project brainwave </p><p> get"b4"[5], applicationlevel functionality such as RDMA packet processing to support keyvalue stores [6], or for distributed machine learning algorithms [#b6][7]. Catapult is, by far, not the only possible design. In IBM's cloudFPGA [8], the FPGA is deployed as a networkattached accel </p>
<p> Limago: an FPGA-based Open-source 100 GbE TCP IP Stack </p><p> rfc7323: tcp extensions for high performance </p><p> adD. TCP Window Scale OptionLinks with a large bandwidth ? delay product suffer from the Long Fat Pipe issue those links where the bandwidth ? delay product is larger than the buffer size [#b14][15]. The Window Scale option is used to allocate any fixsize buffer in the range of 2 16 to 2 30 bytes, thereby leading to a better usage of links.Current </p>
<p> Limago: an FPGA-based Open-source 100 GbE TCP IP Stack </p><p> ibex -an intelligent storage engine with support for advanced sql offloading </p><p> res taking advantage of the increasing availability of High Bandwidth Memory in FPGAs. This feature will improve the throughput when packet loss occurs as well as support application level processing [#b33][34], [35].Fig. 1 1 </p>
<p> Limago: an FPGA-based Open-source 100 GbE TCP IP Stack </p><p> low-latency tcp/ip stack for data center applications </p><p> can send up to 40 Gbits but only receive up to 4 Gbits. The starting point for Limago is a 10 Gbits TOE written by Sidler et al. in C using VivadoHLS [11], [#b28][29]. IV. LIMAGO ARCHITECTUREFigure 1 shows Limago's ma. ure"2). It is divided into three parts, the incoming data path (Rx Engine), the outgoing data path (Tx Engine), and the statekeeping data structures [11], [#b28][29]. The dash boxes are optional modules that can be enabled at synthesis. A. Rx EngineIncoming </p>
<p> Limago: an FPGA-based Open-source 100 GbE TCP IP Stack </p><p> serving dnns in real time at datacenter scale with project brainwave </p><p> get"b4"[5], applicationlevel functionality such as RDMA packet processing to support keyvalue stores [6], or for distributed machine learning algorithms [#b6][7]. Catapult is, by far, not the only possible design. In IBM's cloudFPGA [8], the FPGA is deployed as a networkattached accel </p>
<p> Limago: an FPGA-based Open-source 100 GbE TCP IP Stack </p><p> low-latency tcp/ip stack for data center applications </p><p> can send up to 40 Gbits but only receive up to 4 Gbits. The starting point for Limago is a 10 Gbits TOE written by Sidler et al. in C using VivadoHLS [11], [#b28][29]. IV. LIMAGO ARCHITECTUREFigure 1 shows Limago's ma. ure"2). It is divided into three parts, the incoming data path (Rx Engine), the outgoing data path (Tx Engine), and the statekeeping data structures [11], [#b28][29]. The dash boxes are optional modules that can be enabled at synthesis. A. Rx EngineIncoming </p>
<p> Limago: an FPGA-based Open-source 100 GbE TCP IP Stack </p><p> ibex -an intelligent storage engine with support for advanced sql offloading </p><p> res taking advantage of the increasing availability of High Bandwidth Memory in FPGAs. This feature will improve the throughput when packet loss occurs as well as support application level processing [#b33][34], [35].Fig. 1 1 </p>
<p> Limago: an FPGA-based Open-source 100 GbE TCP IP Stack </p><p> complex event detection at wire speed with fpgas </p><p> lability of High Bandwidth Memory in FPGAs. This feature will improve the throughput when packet loss occurs as well as support application level processing [34], [#b34][35].Fig. 1 1Fig. 1 General Architecture Overview.fig </p>
<p> Limago: an FPGA-based Open-source 100 GbE TCP IP Stack </p><p> ibex -an intelligent storage engine with support for advanced sql offloading </p><p> res taking advantage of the increasing availability of High Bandwidth Memory in FPGAs. This feature will improve the throughput when packet loss occurs as well as support application level processing [#b33][34], [35].Fig. 1 1 </p>
<p> flat: chinese ner using flat-lattice transformer </p><p> attention is all you need </p><p> es at each position, as in Figure 1(b). LRCNN uses CNN to encode potential words at different window sizes. However, RNN and CNN are hard to model longdistance dependencies [#b19](Vaswani et al., 2017), which may be useful in NER, such as coreference (Stanislawek et al., 2019). Due to the dynamic lattice. o use LSTM as the bottom encoder to carry the sequential inductive bias, which makes the model complicated.In this paper, we propose FLAT Flat LAttice Transformer for Chinese NER. Transformer [#b19](Vaswani et al., 2017) adopts fullyconnected selfattention to model the longdistance dependencies in a sequence. To keep the position information, Transformer in. rmula_10"Rij  ReLU(Wr(p d (hh) ij ? p d (th) ij ? p d (ht) ij ? p d (tt) ij )), (8)where W r is a learnable parameter, ? denotes the concatenation operator, and p d is calculated as in [#b19]Vaswani et al. (2017),p (2k) d  sin d10000 2kd model ,(9)p (2k1) d  cos </p>
<p> flat: chinese ner using flat-lattice transformer </p><p> event extraction via dynamic multipooling convolutional neural networks </p><p>  IntroductionNamed entity recognition (NER) plays an indispensable role in many downstream natural language processing (NLP) tasks [#b1](Chen et al., 2015Diefenbach et al., 2018). Compared with English NER (Lample et al., 2016 </p>
<p> flat: chinese ner using flat-lattice transformer </p><p> a lexicon-based graph neural network for chinese ner </p><p>  </p>
<p> flat: chinese ner using flat-lattice transformer </p><p> conditional random fields: probabilistic models for segmenting and labeling sequence data </p><p> 1). The following calculation is the same with vanilla Transformer.After FLAT, we only take the character representation into output layer, followed by a Condiftional Random Field (CRF) [#b8](Lafferty et al., 2001)   Sun, 2016). We show statistics of these datasets in Table 1. W </p>
<p> flat: chinese ner using flat-lattice transformer </p><p> event extraction via dynamic multipooling convolutional neural networks </p><p>  IntroductionNamed entity recognition (NER) plays an indispensable role in many downstream natural language processing (NLP) tasks [#b1](Chen et al., 2015Diefenbach et al., 2018). Compared with English NER (Lample et al., 2016 </p>
<p> flat: chinese ner using flat-lattice transformer </p><p> named entity recognition -is there a glass ceiling? </p><p> different window sizes. However, RNN and CNN are hard to model longdistance dependencies (Vaswani et al., 2017), which may be useful in NER, such as coreference [#b16](Stanislawek et al., 2019). Due to the dynamic lattice structure, these methods cannot fully utilize the parallel computation of GPU. (2) Another line is to conver </p>
<p> flat: chinese ner using flat-lattice transformer </p><p> cnn-based chinese ner with lexicon rethinking </p><p>  </p>
<p> flat: chinese ner using flat-lattice transformer </p><p> learning sparse sharing architectures for multiple tasks </p><p> . Compared with English NER (Lample et al., 2016Yang et al., 2017Liu et al., 2017[#b18]Sun et al., 2020), Chinese NER is more difficult since it usually involves word segmentation.Recently, the lattice structure has been proved to have a great </p>
<p> flat: chinese ner using flat-lattice transformer </p><p> conditional random fields: probabilistic models for segmenting and labeling sequence data </p><p> 1). The following calculation is the same with vanilla Transformer.After FLAT, we only take the character representation into output layer, followed by a Condiftional Random Field (CRF) [#b8](Lafferty et al., 2001)   Sun, 2016). We show statistics of these datasets in Table 1. W </p>
<p> flat: chinese ner using flat-lattice transformer </p><p> event extraction via dynamic multipooling convolutional neural networks </p><p>  IntroductionNamed entity recognition (NER) plays an indispensable role in many downstream natural language processing (NLP) tasks [#b1](Chen et al., 2015Diefenbach et al., 2018). Compared with English NER (Lample et al., 2016 </p>
<p> flat: chinese ner using flat-lattice transformer </p><p> event extraction via dynamic multipooling convolutional neural networks </p><p>  IntroductionNamed entity recognition (NER) plays an indispensable role in many downstream natural language processing (NLP) tasks [#b1](Chen et al., 2015Diefenbach et al., 2018). Compared with English NER (Lample et al., 2016 </p>
<p> XGNN: Towards Model-Level Explanations of Graph Neural Networks </p><p> gnnexplainer: generating explanations for graph neural networks </p><p> [pos is CCS CONCEPTS] ifying the inner working mechanisms, GNNs cannot be fully trusted, which prevents their use in critical applications pertaining to fairness, privacy, and safety [[#b39]]. For example, we can train a GNN model to predict the effects of drugs where we treat each drug as a molecular graph. Without exploring the working mechanisms,. [pos is CCS CONCEPTS] ingly important but is still less explored. To the best of our knowledge, there is no existing study on interpreting GNNs at the modellevel. The existing study [[#b39]] only provides examplelevel explanations for graph models. As a radical departure from existing work, we propose a novel interpretation technique, known as XGN. [pos is CCS CONCEPTS] "Graph Model InterpretationsTo the best of our knowledge, there are only a few existing studies focusing on the interpretability of deep graph models [[#b39]]. The recent GNN interpretation tool GNN Explainer [#b39][40] proposes to explain deep graph models at the examplelevel by learn. [pos is CCS CONCEPTS] xisting studies focusing on the interpretability of deep graph models [[#b39]]. The recent GNN interpretation tool GNN Explainer [#b39][40] proposes to explain deep graph models at the examplelevel by learning soft masks. For a given example, it applies soft masks to graph edges and node features. [pos is CCS CONCEPTS] b_4"1. Since there is no existing work investigating modellevel interpretations of GNNs, we have no baseline to compare with. Note that existing studies [[#b39]] only focus on interpreting GNNs at examplelevel while ignoring the modellevel explanations. Comparing with them is not expected since these examplelevel and </p>
<p> XGNN: Towards Model-Level Explanations of Graph Neural Networks </p><p> junction tree variational autoencoder for molecular graph generation </p><p> [pos is CCS CONCEPTS] ces in graph generation lead to many successful graph generation models, such as GraphGAN [38], ORGAN [14], Junction Tree VAE [#b16][17], DGMG [22], and Graph Convolutional Policy Network (GCPN) [41]. formula xmlid </p>
<p> XGNN: Towards Model-Level Explanations of Graph Neural Networks </p><p> junction tree variational autoencoder for molecular graph generation </p><p> [pos is CCS CONCEPTS] ces in graph generation lead to many successful graph generation models, such as GraphGAN [38], ORGAN [14], Junction Tree VAE [#b16][17], DGMG [22], and Graph Convolutional Policy Network (GCPN) [41]. formula xmlid </p>
<p> XGNN: Towards Model-Level Explanations of Graph Neural Networks </p><p> link prediction based on graph neural networks </p><p> [pos is CCS CONCEPTS] pe"bibr" target"b10"[], graph classification [], and link prediction [#b45][46]. In addition, extensive efforts have been made towards different graph operations, such as graph convolution [13,ref typ </p>
<p> XGNN: Towards Model-Level Explanations of Graph Neural Networks </p><p> automatic differentiation in pytorch </p><p> [pos is CCS CONCEPTS] "bibr" target"b0"(1). In addition, we employ Sigmoid as the nonlinear function in GCNs for dataset Is_Acyclic while we use Relu for dataset MUTAG. These models are implemented using Pytorch [#b26][27] and trained using Adam optimizer [18]. The training accuracies of these models are reported in Table ref type"table" ta. [pos is CCS CONCEPTS] he total reward R t  1 if the generated graph violates any graph rule. In addition, we perform rollout m  10 times each step to obtain final graph rewards. The models are implemented using Pytorch [#b26][27] and trained using Adam optimizer [18] with  1  0.9 and  2  0.999.  In each row, from left to right, we report the gen </p>
<p> XGNN: Towards Model-Level Explanations of Graph Neural Networks </p><p> an endto-end deep learning architecture for graph classification </p><p> [pos is CCS CONCEPTS] on different graph tasks, such as node classification [], graph classification [[#b46]], and link prediction [46]. In addition, extensive efforts have been made towards different graph operations, such as graph </p>
<p> XGNN: Towards Model-Level Explanations of Graph Neural Networks </p><p> plug & play generative networks: conditional iterative generation of images in latent space </p><p> [pos is CCS CONCEPTS] " target"b31"32,43,45,48] or modellevel [[#b23]] methods. Examplelevel interpretations explain the prediction for a given input example, by determining important featu. [pos is CCS CONCEPTS] l behavior of the model by investigating what input patterns can lead to a certain prediction, without respect to any specific input example. Input optimization [[#b23][][25][26] is the most popular modellevel interpretation method. These two categories. [pos is CCS CONCEPTS] ionsNext, we briefly discuss popular modellevel interpretation techniques for deep learning models on image data, known as input optimization methods [[#b23][][25][26]. These methods generally generate optimized input that can maximize a certai. [pos is CCS CONCEPTS] ere G  is the optimized input graph we need. A popular way to obtain such optimized input for interpreting image and text models is known as input optimization [[#b23][][25][26]43]. However, as discussed in Section 2. </p>
<p> XGNN: Towards Model-Level Explanations of Graph Neural Networks </p><p> network motifs in the transcriptional regulation network of escherichia coli </p><p> [pos is CCS CONCEPTS] h widely exist in graphs from biochemistry, neurobiology, ecology, and engineering [[#b29]]. Different motif sets can be found in graphs with different functions [], which me </p>
<p> XGNN: Towards Model-Level Explanations of Graph Neural Networks </p><p> graph attention networks </p><p> [pos is CCS CONCEPTS] raph Neural Networks (GNNs) have shown their effectiveness and obtained the stateoftheart performance on different graph tasks, such as node classification [[#b36]], graph classification [], and link prediction [46. [pos is CCS CONCEPTS] , graph pooling [], and graph attention [[#b36]]. Since graph data widely exist in different realworld applications, such as social networks, chemistry, and biology, GNNs are becoming increasingly important. [pos is CCS CONCEPTS] features based on these matrices. Even though there are several variants of GNNs, such as graph convolution networks (GCNs) [19], graph attention networks (GATs) [#b36][37], and graph isomorphism networks (GINs) [39], they share a similar feature learning strategy. For each node, GNNs update i </p>
<p> XGNN: Towards Model-Level Explanations of Graph Neural Networks </p><p> adam: a method for stochastic optimization </p><p> [pos is CCS CONCEPTS] near function in GCNs for dataset Is_Acyclic while we use Relu for dataset MUTAG. These models are implemented using Pytorch [27] and trained using Adam optimizer [#b17][18]. The training accuracies of these models are reported in Table 1, which show that the models we try to interpret are m. [pos is CCS CONCEPTS] addition, we perform rollout m  10 times each step to obtain final graph rewards. The models are implemented using Pytorch [27] and trained using Adam optimizer [#b17][18] with  1  0.9 and  2  0.999.  In each row, from left to right, we report the generated graphs with increasing maximum node number limits. In addition, we f </p>
<p> XGNN: Towards Model-Level Explanations of Graph Neural Networks </p><p> self-attention graph pooling </p><p> [pos is CCS CONCEPTS] towards different graph operations, such as graph convolution [], graph pooling [#b19][], and graph attention [10,36,ref type"bibr" </p>
<p> a collective approach to scholar name disambiguation </p><p> on graph-based name disambiguation </p><p> to attack.Most existing methods tackle name disambiguation separately [5], [6], [9], [#b9][11], [13], [15], [17], ref type"bibr" target"b16. wever, by tackling each name separately and independently, these methods neglect the connection between these subproblems. For example, coauthors, which are used as a strong evidence in many methods [#b9][11], [32], [39], may also be ambiguous. Fig. 1. hat they refer to the same person. In fact, there are two different "Wei Xu" and two different "Ying Zhang" in this example. More troubles may appear when multihop coauthorships are used as features [#b9][11], [32]. For instance, "Jianxin Li" in the University of Western Australia is a coauthor and 2hop coauthor of "Wei Wang" in. ets (AMiner, ACM, and DBLP) to evaluate NDCC. We find that our method NDCC is both effective and efficient, compared with the stateoftheart methods CE [7], GHOST [#b9][11], CSLR [19], MIX [18], and AM [44]. Specifically,. where W AA 2 i,j is the number of valid 2hop coauthorship paths connecting authors i and j. To avoid the redundant information, we only consider valid 2hop coauthorship paths connecting two authors [#b9][11]. Specifically, a valid 2hop coauthorship path in G is an AP AP A path a i p i a j p j a k , where a i  a j , a i  a k , a j  a k and p i  p j .. ing three real datasets, we conduct four sets of experiments to evaluate (1) the effectiveness and efficiency of NDCC versus stateoftheart methods CE [7], GHOST [#b9][11], CSLR [19], MIX [18], and AM [44], (2) the effec. relational data. Its similarity function considers both attributes and relational information, and a greedy agglomerative clustering method is used to merge the most similar clusters.(2) GHOST [#b9][11] is a graphbased method employing coauthorship only. Its similarity function considers both quantity and quality (length) of paths, and an affinity propagation. citation information is available.Exp1.2 Efficiency performance comparison. Among the chosen baselines, only CE and GHOST analyze the time complexity [7], [#b9][11]. The time complexity of CE is O(A (0) k log A (0) ), where A (0)  is the number of atomic authors, and k is largest number of buckets that a buckets conn. 36"[38], [40], [44] and unsupervised [7], [9], [#b9][11], [19], [28], [29], ref type"bibr" target"b30. hods use clustering, e.g., agglomerative clustering [9], [19], [39], affinity propagation [#b9][11] and Markov clustering [41], or topic modeling [28], [2 </p>
<p> a collective approach to scholar name disambiguation </p><p> object distinction: distinguishing objects with identical names </p><p> " target"b27"[29], [32], [35], [36], [37], [#b36][38], [39], [40], [44]. For each name to be disambig. ion information, like many other digital libraries. Thus, we dismiss baselines relying on these external features [32], [37], [#b36][38]. Besides, some methods require the number of authors for each name [#b36][38], [39], which. ures [32], [37], [#b36][38]. Besides, some methods require the number of authors for each name [#b36][38], [39], which is unavailable in practice. Thus, we compare 10414347 (c) 2020 IEEE. Personal use is permitted, but republi. br" target"b2"[4], [15], [17], [18], [35], [#b36][38], [40], [44] and unsupervised [7], ref type"bib. , [39], [41], [42]. Supervised methods use labeled data to train a classifier, e.g., SVM [#b36][38] and random forests [17], [18], [35], which is t. There are three kinds of evidences that are commonly explored by disambiguation methods [12] citation information [7], [#b36][38], web information [19], affiliation [5], and implicit evidence ref type"bibr" target </p>
<p> a collective approach to scholar name disambiguation </p><p> online person name disambiguation with constraints </p><p> ibr" target"b7"[9], [11], [13], [15], [17], [#b16][18], [19], [29], [32], ref type"bibr" target"b3. nd efficient, compared with the stateoftheart methods CE [7], GHOST [11], CSLR [19], MIX [#b16][18], and AM [44]. Specifically, (a) NDCC on average improves the MacroF1 over (CE, GHOST, CSLR, MIX, AM) by (17.87, 23.25,. X, AM) by (17.87, 23.25, 16.65, 45.39, 21.24) on AMiner, (25.36, 24.26, 14.16, 37.46, 14.96) on ACM, and (13.11, 23.31, 8.47, 50.37, 9.86) on DBLP, respectively. (b) NDCC is on average [#b16](18,195,19) times faster than (CE, CSLR and MIX) on AMiner, (15,r. one title contains the word "hardware" and the other contains the word "circuit". Both are related to computer hardware. In this case, the traditional unigram model returns a low similarity score. In [#b16][18], [19], the string level or character level tolerance is used when comparing two titles. However, these methods cannot cap. and efficiency of NDCC versus stateoftheart methods CE [7], GHOST [11], CSLR [19], MIX [#b16][18], and AM [44], (2) the effectiveness of author number estimation, (3) effects of important components in NDCC, and (4) the. get"b17"[19] first groups the author references based on coauthorships to generate initial clusters. Then these clusters are merged by venuebased and titlebased similarities.(4) MIX [#b16][18] is a supervised method. Random forests are used to calculate pairwise distances, and DBSCAN is used to group the author references. For effectiveness evaluati. 8.47, 50.37, 9.86) on DBLP, on average, respectively. MIX adopts random forests to learn pairwise similarities, which works well when many features are available, such as abstract, and affiliation [#b16][18]. While, in this study, we address the scholarly name disambiguation problem in a more challenging setting, where only basic citation features are available. W. iguate all names in the smallest dataset AMiner within 12 hours. Thus, its running time is not reported here.The results show that NDCC is more efficient than the baseline methods. (a) NDCC is [#b16](18,195,19) times faster than (CE, CSLR and MIX) on AMiner, (15, 8) times faster than    Exp2 Effecti. scholar name disambiguation can be divided into two classes supervised [4], [15], [17], [#b16][18], [35], [38], [40], ref type"bibr" target"b4. target"b40"[42]. Supervised methods use labeled data to train a classifier, e.g., SVM [38] and random forests [17], [#b16][18], [35], which is then used to assign publications to different author entities. However, labeling data is timeconsuming a </p>
<p> a collective approach to scholar name disambiguation </p><p> object distinction: distinguishing objects with identical names </p><p> " target"b27"[29], [32], [35], [36], [37], [#b36][38], [39], [40], [44]. For each name to be disambig. ion information, like many other digital libraries. Thus, we dismiss baselines relying on these external features [32], [37], [#b36][38]. Besides, some methods require the number of authors for each name [#b36][38], [39], which. ures [32], [37], [#b36][38]. Besides, some methods require the number of authors for each name [#b36][38], [39], which is unavailable in practice. Thus, we compare 10414347 (c) 2020 IEEE. Personal use is permitted, but republi. br" target"b2"[4], [15], [17], [18], [35], [#b36][38], [40], [44] and unsupervised [7], ref type"bib. , [39], [41], [42]. Supervised methods use labeled data to train a classifier, e.g., SVM [#b36][38] and random forests [17], [18], [35], which is t. There are three kinds of evidences that are commonly explored by disambiguation methods [12] citation information [7], [#b36][38], web information [19], affiliation [5], and implicit evidence ref type"bibr" target </p>
<p> a collective approach to scholar name disambiguation </p><p> collective entity resolution with multi-focal attention </p><p> ), denoted by K (line 12). It then calculates pairwise author similarity scores in A n , and finds Kth largest score t by using a Ksize minimum heap (lines [13][#b12][14]. Then it merges author pairs whose similarity scores are no less than t, and updates the network G (equally, the matrix W AP ) accordingly (lines ref type"b. me disambiguation in a collective way. There are also some collective entity resolution methods that can be used to solve multiple name disambiguation problem [7], [#b12][14], [27]. However, they are not designed for scholar name disambiguation, as they mainly aim to deal with duplication proble. olar name disambiguation, as they mainly aim to deal with duplication problems in relational databases caused by different forms of the same names. Most of them need another clean knowledge base (KB) [#b12][14], [27], which is unavailable in most cases. [7] is a collective entity resolution meth </p>
<p> a collective approach to scholar name disambiguation </p><p> online person name disambiguation with constraints </p><p> ibr" target"b7"[9], [11], [13], [15], [17], [#b16][18], [19], [29], [32], ref type"bibr" target"b3. nd efficient, compared with the stateoftheart methods CE [7], GHOST [11], CSLR [19], MIX [#b16][18], and AM [44]. Specifically, (a) NDCC on average improves the MacroF1 over (CE, GHOST, CSLR, MIX, AM) by (17.87, 23.25,. X, AM) by (17.87, 23.25, 16.65, 45.39, 21.24) on AMiner, (25.36, 24.26, 14.16, 37.46, 14.96) on ACM, and (13.11, 23.31, 8.47, 50.37, 9.86) on DBLP, respectively. (b) NDCC is on average [#b16](18,195,19) times faster than (CE, CSLR and MIX) on AMiner, (15,r. one title contains the word "hardware" and the other contains the word "circuit". Both are related to computer hardware. In this case, the traditional unigram model returns a low similarity score. In [#b16][18], [19], the string level or character level tolerance is used when comparing two titles. However, these methods cannot cap. and efficiency of NDCC versus stateoftheart methods CE [7], GHOST [11], CSLR [19], MIX [#b16][18], and AM [44], (2) the effectiveness of author number estimation, (3) effects of important components in NDCC, and (4) the. get"b17"[19] first groups the author references based on coauthorships to generate initial clusters. Then these clusters are merged by venuebased and titlebased similarities.(4) MIX [#b16][18] is a supervised method. Random forests are used to calculate pairwise distances, and DBSCAN is used to group the author references. For effectiveness evaluati. 8.47, 50.37, 9.86) on DBLP, on average, respectively. MIX adopts random forests to learn pairwise similarities, which works well when many features are available, such as abstract, and affiliation [#b16][18]. While, in this study, we address the scholarly name disambiguation problem in a more challenging setting, where only basic citation features are available. W. iguate all names in the smallest dataset AMiner within 12 hours. Thus, its running time is not reported here.The results show that NDCC is more efficient than the baseline methods. (a) NDCC is [#b16](18,195,19) times faster than (CE, CSLR and MIX) on AMiner, (15, 8) times faster than    Exp2 Effecti. scholar name disambiguation can be divided into two classes supervised [4], [15], [17], [#b16][18], [35], [38], [40], ref type"bibr" target"b4. target"b40"[42]. Supervised methods use labeled data to train a classifier, e.g., SVM [38] and random forests [17], [#b16][18], [35], which is then used to assign publications to different author entities. However, labeling data is timeconsuming a </p>
<p> a collective approach to scholar name disambiguation </p><p> collective entity resolution with multi-focal attention </p><p> ), denoted by K (line 12). It then calculates pairwise author similarity scores in A n , and finds Kth largest score t by using a Ksize minimum heap (lines [13][#b12][14]. Then it merges author pairs whose similarity scores are no less than t, and updates the network G (equally, the matrix W AP ) accordingly (lines ref type"b. me disambiguation in a collective way. There are also some collective entity resolution methods that can be used to solve multiple name disambiguation problem [7], [#b12][14], [27]. However, they are not designed for scholar name disambiguation, as they mainly aim to deal with duplication proble. olar name disambiguation, as they mainly aim to deal with duplication problems in relational databases caused by different forms of the same names. Most of them need another clean knowledge base (KB) [#b12][14], [27], which is unavailable in most cases. [7] is a collective entity resolution meth </p>
<p> a collective approach to scholar name disambiguation </p><p> use of researchgate and google cse for author name disambiguation </p><p> parameters. RELATED WORKIn general, existing work for scholar name disambiguation can be divided into two classes supervised [#b2][4], [15], [17], [18], ref type"bibr" target"b33" </p>
<p> a collective approach to scholar name disambiguation </p><p> collective entity resolution with multi-focal attention </p><p> ), denoted by K (line 12). It then calculates pairwise author similarity scores in A n , and finds Kth largest score t by using a Ksize minimum heap (lines [13][#b12][14]. Then it merges author pairs whose similarity scores are no less than t, and updates the network G (equally, the matrix W AP ) accordingly (lines ref type"b. me disambiguation in a collective way. There are also some collective entity resolution methods that can be used to solve multiple name disambiguation problem [7], [#b12][14], [27]. However, they are not designed for scholar name disambiguation, as they mainly aim to deal with duplication proble. olar name disambiguation, as they mainly aim to deal with duplication problems in relational databases caused by different forms of the same names. Most of them need another clean knowledge base (KB) [#b12][14], [27], which is unavailable in most cases. [7] is a collective entity resolution meth </p>
<p> a collective approach to scholar name disambiguation </p><p> effective unsupervised author disambiguation with relative frequencies </p><p> ear, but no author affiliations, homepages and publication abstracts. This makes name disambiguation even more challenging to attack.Most existing methods tackle name disambiguation separately [#b3][5], [6], [9], [11], [13]. that less similar venue pairs are considered. Exp4.3 Impacts of ?. To evaluate the impacts of ?, we vary ? from 1 to 100 (1,2,[#b3]5,10,20,50,100) and fix other parameters to t. rget"b10"[12] citation information [7], [38], web information [19], affiliation [#b3][5], and implicit evidence [28], [29]. Citation information is extracted directly from cit </p>
<p> a collective approach to scholar name disambiguation </p><p> name disambiguation in aminer: clustering, maintenance, and human in the loop </p><p> aised various troubles in scholar search, document retrieval and so on [21], [24], [32], [#b42][44]. For example, we read an interesting paper written by "Wei Wang" in DBLP, and we want to find more hisher publications. However, over 200 authors share the s. " target"b34"[36], [37], [38], [39], [40], [#b42][44]. For each name to be disambiguated, these methods only deal with the papers having that author name. However, by tackling each name separately and independent. thods CE [7], GHOST [11], CSLR [19], MIX [18], and AM [#b42][44]. Specifically, (a) NDCC on average improves the MacroF1 over (CE, GHOST, CSLR, MIX, AM) by (17.87, 23.25, 16.65, 45.39, 21.24) on AMiner, (25.36, 24.26. thods CE [7], GHOST [11], CSLR [19], MIX [18], and AM [#b42][44], (2) the effectiveness of author number estimation, (3) effects of important components in NDCC, and (4) the impacts of parameters on accuracy and efficiency.. beled names as the training set for each author name to be disambiguated. For efficiency evaluation, we randomly choose 5 labeled names to train the model and use the others for testing.(5) AM [#b42][44] is the method deployed in AMiner to tackle the name disambiguation. A representation learning method is used to include global and local information. An endt. generated subsets are dense. Although AM is deployed with thousands of millions of papers, it is not efficient to compute the clustering from scratch due to the local linkage learning and IO overhead [#b42][44]. Indeed, this method even cannot disambiguate all names in the smallest dataset AMiner within 12 hours. Thus, its running time is not reported here.The. " target"b15"[17], [18], [35], [38], [40], [#b42][44] and unsupervised [7], [9], [11], ref type"bibr" </p>
<p> SIGN: Scalable Inception Graph Neural Networks </p><p> going deeper with convolutions </p><p> y complexity. In this paper, we propose a simple scalable graph neural network architecture generalizing GCN, SGCN, ChebNet and related methods. Our architecture is analogous to the inception module [#b43]Szegedy et al. (2015) Kazi et al. (2019) and combines graph convolutional filters of different size that are amenable to effi. s, e.g. via softmax or sigmoid function, depending on the task at hand. Note that the model in equation ( 4) is analogous to the popular Inception module [#b43]Szegedy et al. (2015) for classic CNN architectures (Figure 1) it consists of convolutional filters of different sizes determined by the </p>
<p> SIGN: Scalable Inception Graph Neural Networks </p><p> weighted graph cuts without eigenvectors a multilevel approach </p><p> Convolutionlike operators (discussed in details in Section 2.3) performing local aggregation of features by means of message passing with the neighbor nodes, and possibly Pooling amounting to fixed [#b12]Dhillon et al. (2007) or learnable Ying et al. (2018b)Bianchi et al. (2019a) graph coarse </p>
<p> SIGN: Scalable Inception Graph Neural Networks </p><p> backpropagation applied to handwritten zip code recognition </p><p> erators on graphsSpectral graph CNNs. Bruna et al. Bruna et al. (2014) used the graph Fourier transform to generalize convolutional neural networks (CNN) [#b27]LeCun et al. (1989) to graphs. This approach has multiple drawbacks. First, the computation of the Fourier transform has O(n 2 ) complexity, in addition to O(n 3 ) </p>
<p> SIGN: Scalable Inception Graph Neural Networks </p><p> variational graph auto-encoders </p><p> sed learning is to compute a representation of the nodes (or the graph, respectively) capturing the underlying structure. Typical representatives of this class of architectures are graph autoencoders [#b24]Kipf and Welling (2016) and random walkbased embeddings Grover and Leskovec (2016) Perozzi et </p>
<p> SIGN: Scalable Inception Graph Neural Networks </p><p> inceptiongcn: receptive field aware graph convolutional network for disease prediction </p><p> aph neural network architecture generalizing GCN, SGCN, ChebNet and related methods. Our architecture is analogous to the inception module Szegedy et al. (2015) [#b22]Kazi et al. (2019) and combines graph convolutional filters of different size that are amenable to efficient precomputation, allowing extremely fast training and i. ion across nodes). Owing to this analogy, we refer to our model as the Scalable Inception Graph Network (SIGN). We notice that one work extending the idea of an Inception module to GNNs is the one in [#b22]Kazi et al. (2019) in this work, however, authors do not discuss the inclusion of a linear, nondiffusive term (r  0) which effectively accounts for a skip conne </p>
<p> SIGN: Scalable Inception Graph Neural Networks </p><p> ncrna classification with graph convolutional networks </p><p> 18), discovery of anticancer foods Veselkov et al. (2019), modeling of proteins Gainza et al. (2019) and nucleic acids [#b37]Rossi et al. (2019), and fake news detection on social media Monti et al. (2019) to mention a few. Somewhat surprisingly, ofte </p>
<p> SIGN: Scalable Inception Graph Neural Networks </p><p> backpropagation applied to handwritten zip code recognition </p><p> erators on graphsSpectral graph CNNs. Bruna et al. Bruna et al. (2014) used the graph Fourier transform to generalize convolutional neural networks (CNN) [#b27]LeCun et al. (1989) to graphs. This approach has multiple drawbacks. First, the computation of the Fourier transform has O(n 2 ) complexity, in addition to O(n 3 ) </p>
<p> SIGN: Scalable Inception Graph Neural Networks </p><p> graph neural networks for icecube signal classification </p><p> ang and Chen (2018), humanobject interaction Qi et al. (2018), computer graphics Monti et al. (2016), particle physics [#b10]Choma et al. (2018), chemistry Duvenaud et al. (2015) Gilmer et al. (2017), medicine ref type"bibr" </p>
<p> SIGN: Scalable Inception Graph Neural Networks </p><p> neural message passing for quantum chemistry </p><p> graphics Monti et al. (2016), particle physics Choma et al. (2018), chemistry Duvenaud et al. (2015) [#b15]Gilmer et al. (2017), medicine Parisot et al. (2018), drug repositioning Zitnik et al. (2018) </p>
<p> SIGN: Scalable Inception Graph Neural Networks </p><p> ncrna classification with graph convolutional networks </p><p> 18), discovery of anticancer foods Veselkov et al. (2019), modeling of proteins Gainza et al. (2019) and nucleic acids [#b37]Rossi et al. (2019), and fake news detection on social media Monti et al. (2019) to mention a few. Somewhat surprisingly, ofte </p>
<p> SIGN: Scalable Inception Graph Neural Networks </p><p> pitfalls of graph neural network evaluation </p><p> fake news detection on social media Monti et al. (2019) to mention a few. Somewhat surprisingly, often very simple architectures perform well in many applications [#b40]Shchur et al. (2018a). In particular, graph convolutional networks (GCN) Kipf and Welling (2017) and their more recent variant. essentially shown to learn lowpass filters NT and Maehara (2019) while still performing on par with models with more complex aggregation functions in the task of semisupervised node classification [#b40]Shchur et al. (2018a).Accordingly, we propose the following architecture for nodewise classificationZ   [X (0)  BX (1. formative, but actually misleading for the model.   Conclusion and Future WorkOur results are consistent with previous reports [#b40]Shchur et al. (2018a) advocating in favor of simple architectures (with just a single graph convolutional layer) in graph learning tasks. Our architecture achieves </p>
<p> a neural influence and interest diffusion network for social recommendation </p><p> a neural influence diffusion model for social recommendation </p><p> social influence diffusion process from the global social network structure. Recently, we propose a preliminary work of a neural influence Diff usion Network (i.e., DiffNet) for social recommendation [#b42][43]. DiffNet models the recursive social diffusion process for each user, such that the influence diffusion hidden in the higherorder social network is captured. rs to alleviate data sparsity and enhancing recommendation performance [19], [20], [14], [#b42][43].In fact, as users play a central role in social platforms with useruser social behavior and useritem interest behavior, the key to social recommendat. , some works have applied GCNs separately on these two kinds of graphs [51], [41], [48], [#b42][43]. On one hand, given the useritem interest graph, NGCF is proposed to directly encode the collaborative information of users by exploring the higherorder conn. neural Network (DiffNet) to model the recursive social diffusion process in the social network, such that the higherorder social structure is directly modeled in the recursive user embedding process [#b42][43]. These graph based models showed superior performance compared to the previous nongraph based recommendation models by modeling either graph structure. Never. y aggregate user embeddings from different nodes in a graph, and then from different graphs. In summary, our main contributions are listed as follows? Compared to our previous work of DiffNet [#b42][43], we revisit the social recommendation problem as predicting the missing edges in the useritem interest graph by taking both useritem interest graph and user. ers could be naturally formulated as a useruser graph, recently we propose a preliminary graph based social recommendation model, DiffNet, for modeling the social diffusion process in recommendation [#b42][43]. DiffNet advances classical embedding based models with carefully designed influence diffusion layers, such that how users are influenced by the recursive inf. fNet adopts the recursive influence diffusion process for iterative user embedding learning, such that the up to Kth order social network structure is injected into the social recommendation process [#b42][43]. In this part, we propose DiffNet, an enhanced model of DiffNet that fuses both influence diffusion in the social network G S and interest diffusion in the. aset is also publicly available 5 .Among the four datasets, Yelp and Flickr are two datasets with user and item attributes, and are adopted as datasets of our previously proposed DiffNet model [#b42][43]. The remaining two datasets of Epinions and Dianping do not contain user and item attributes. We use the same preprocessing steps of the four datasets. Specif. . Please note that, in PinSage, we take the useritem graph with both user and item features as input, in order to transform this model for the recommendation task. For our proposed models of DiffNet [#b42][43] and DiffNet, since both models are flexible and could be reduced to simpler versions without user and item features, we use DiffNetnf and DiffNetnf to r. N ranking evaluation, we use two widely used metrics, Hit Ratio (HR) [9] and Normalized Discounted Cummulative Gain (NDCG) [9], [#b42][43]. Specifically, HR measures the percentage of hit items in the topN list, and NDCG puts more emphasis on the top ranked items. As we focus on the topN rankin. opN list, and NDCG puts more emphasis on the top ranked items. As we focus on the topN ranking performance with large itemset, similar as many other works [17], [#b42][43], to evaluate the performance, for each user, we randomly select 1000 unrated items that a user has not interacted with as negative samples. Then, we mix these </p>
<p> a neural influence and interest diffusion network for social recommendation </p><p> deep social collaborative filtering </p><p> ation task. Researchers proposed to generate social sequences based on random walks on useruser and useritem graph, and further leveraged the sequence embedding techniques for social recommendation [#b10][11]. This model could better capture the higherorder social network structure. However, the performance heavily relies on the choice of random walk strategy, inc. en features for social influence strength learning. Without confusion, we omit the normalization step of all attention modeling in the following, as all of them share the similar form as shown in Eq. [#b10](11). Similarly, we calculate the interest influence score ? k1 ai by taking related user embedding and item embedding as input? </p>
<p> a neural influence and interest diffusion network for social recommendation </p><p> a matrix factorization technique with trust propagation for recommendation in social networks </p><p> ibr" target"b1"[2]. Therefore, social recommendation has emerged, which focuses on exploiting social relations among users to alleviate data sparsity and enhancing recommendation performance [#b18][19], [20], [14], [43].In fact, as users play. e these CF models by leveraging the useruser matrix to enhance each user's embedding learning with social neighbors' records, or regularizing the user embedding learning process with social neighbors [#b18][19], [30], [21], [15]. For example, SocialMF ref t. hbors [#b18][19], [30], [21], [15]. For example, SocialMF [#b18][19] and SR [30] added social regularization terms based on social neighbors in the optimization function, and TrustSVD incorp. social recommendation models are also built on these embedding models. These social embedding models could be summarized into the following two categories the social regularization based approaches [#b18][19], [30], [21], [26], ref type"bibr" target"b4.  with competitive baselines, including classical CF models (BPR [37], FM [36]), social based recommendation model (SocialMF [#b18][19], TrustSVD [14], ContextMF [21], CNSR [44]), as </p>
<p> a neural influence and interest diffusion network for social recommendation </p><p> a model of saliency-based visual attention for rapid scene analysis </p><p> n adopted when multiple elements in a sequence or set would have an impact of the following output, such that attentive weights are learned with deep neural networks to distinguish important elements [#b17][18], [3], [46]. Given a user's rated item history, NAIS is proposed to learn the neural a. of the node attention layer. We use ? (k1)  [? k al ] ? R M ?2 to denote the attentive weight matrix of the multilevel networks in Eq.( 17) and Eq. [#b17](18). All these three attention matrices can be calculated similarly as shown above.After learning the attention matrices, we could update user and item emb </p>
<p> a neural influence and interest diffusion network for social recommendation </p><p> neural graph collaborative filtering </p><p> eractions as a bipartite interest graph and useruser social network as a social graph, some works have applied GCNs separately on these two kinds of graphs [51], [#b40][41], [48], [43]. On one hand, given the useritem interest graph, NGCF is proposed to dir. one hand, given the useritem interest graph, NGCF is proposed to directly encode the collaborative information of users by exploring the higherorder connectivity patterns with embedding propagation [#b40][41]. On the other hand, in our previous work, we propose a Diff usion neural Network (DiffNet) to model the recursive social diffusion process in the social netwo. re, many recent works focus on the spatial based GCNs for recommendation [48], [4], [49], [#b40][41]. PinSage is a GCN based content recommendation model by propagating item features in the itemitem correlation graph [48]. rget"b3"[4]. NGCF extended GCMC with multiple layers, such that the higherorder collaborative signals between users and items can be modeled in the user and item embedding learning process [#b40][41].As the social structure among users could be naturally formulated as a useruser graph, recently we propose a preliminary graph based social recommenda. f type"bibr" target"b43"[44]), as well as the graph based recommendation models of GraphRec [10], PinSage [48], NGCF [#b40][41]. Please note that, in PinSage, we take the useritem graph with both user and item features as input, in order to transform this model for the recommendation </p>
<p> a neural influence and interest diffusion network for social recommendation </p><p> bpr: bayesian personalized ranking from implicit feedback </p><p> pCollaborative Filtering (CF) based recommender systems learn user and item embeddings by utilizing useritem interest behavior data, and have attracted attention from both the academia and industry [#b36][37], [32]. However, as most users have limited behavior data, CF suffers from the data sparsity issue ref type"bibr" target. of behaviors. For a long time, by treating the useritem interest network as a useritem matrix, CF based models resort to matrix factorization to project both users and items into a low latent space [#b36][37], [32], [36]. Most social based recommender systems advance these CF models by levera. some implicit feedbacks (e.g., watching movies, purchasing items, listening to songs ) are more common in realworld applications, we also consider the recommendation scenario with implicit feedback [#b36][37]. Let R denote users' implicit feedback based rating matrix, with r ai  1 if user a is interested in item i, otherwise it equals 0. We use R a represents the. ed both users and items in a low dimension latent space, such that each user's predicted preference to an unknown item turns to the inner product between the corresponding user and item embeddings as [#b36][37], [32], [36]rai  v T i ua,(1)formu. cial regularization based approaches assumed that connected users would show similar embeddings under the social influence diffusion. As such, besides the classical CF pairwised loss function in BPR [#b36][37], an additional social regularization term is incorporated in the overall optimization function asM i1 M j1 sijui uj 2. . Model TrainingWe use a pairwise ranking based loss function for optimization, which is widely used for implicit feedback [#b36][37]L  min ? (a,i)?R  ?(a,j)?R  ln?(r ai raj )  ?? 2 . (20)form. s the user and item free embeddings ?1[P Q], and the parameter set in the fusion layer and the attention modeling, i.e., ?2[W W [M LPi]i1,2,3,4]. Since most embedding based models (e.g., BPR [#b36][37], FM [36]) need to store the embeddings of each user and each item, the space complexity of ? 1 is the same as classical e. del Input User Embedding Ability   Baselines and Evaluation Metrics. To illustrate the effectiveness of our method, we compare DiffNet with competitive baselines, including classical CF models (BPR [#b36][37], FM [36]), social based recommendation model (SocialMF [19], TrustSVD ref type"bib </p>
<p> a neural influence and interest diffusion network for social recommendation </p><p> exploiting local and global social context for recommendation </p><p> ype"bibr" target"b33"[34], temporal context [38], rich contextual information [42], user role in the social network [#b38][39], and efficient training models without negative sampling [6]. All these previous works focused on how to explore the socia </p>
<p> a neural influence and interest diffusion network for social recommendation </p><p> exploiting local and global social context for recommendation </p><p> ype"bibr" target"b33"[34], temporal context [38], rich contextual information [42], user role in the social network [#b38][39], and efficient training models without negative sampling [6]. All these previous works focused on how to explore the socia </p>
<p> a neural influence and interest diffusion network for social recommendation </p><p> exploiting local and global social context for recommendation </p><p> ype"bibr" target"b33"[34], temporal context [38], rich contextual information [42], user role in the social network [#b38][39], and efficient training models without negative sampling [6]. All these previous works focused on how to explore the socia </p>
<p> a neural influence and interest diffusion network for social recommendation </p><p> neural machine translation by jointly learning to align and translate </p><p> nce or set would have an impact of the following output, such that attentive weights are learned with deep neural networks to distinguish important elements [18], [#b2][3], [46]. Given a user's rated item history, NAIS is proposed to learn the neural attentive weights for item similarity in ite </p>
<p> a neural influence and interest diffusion network for social recommendation </p><p> factorization meets the neighborhood: a multifaceted collaborative filtering model </p><p> ows implicit feedback, and y i is an implicit factor vector. Therefore, these first two terms compose SVD model that explicitly builds each user's liked items in the user embedding learning process [#b22][23]. In the third term, u b denotes the latent embedding of user b, who is trusted by a. As such, a's latent embedding is enhanced by considering the influence of </p>
<p> pre-training is a hot topic: contextualized document embeddings improve topic coherence </p><p> sentencebert: sentence embeddings using siamese bertnetworks </p><p> length limit in BERT. Concretely, we extend ProdLDA (Srivastava and Sutton, 2017), a stateoftheart topic model that implements blackbox variational inference [#b20](Ranganath et al., 2014), to include BERT representations. Our approach leads to consistent significant improvements in topic coherence, and produces competitive r </p>
<p> pre-training is a hot topic: contextualized document embeddings improve topic coherence </p><p> word features for latent dirichlet allocation </p><p> orate several types of information (Xun et al., 2017Das et al., 2015Nguyen et al., 2015[#b18]Petterson et al., 2010), use word relationships derived from external knowledge bases (Chen et al., 2013ref type"bibr" targe </p>
<p> pre-training is a hot topic: contextualized document embeddings improve topic coherence </p><p> discovering coherent topics using general knowledge </p><p> et al., 2015Nguyen et al., 2015Petterson et al., 2010), use word relationships derived from external knowledge bases [#b3](Chen et al., 2013Yang et al., 2015), or pretrained word embeddings (Das et al., 2015re </p>
<p> pre-training is a hot topic: contextualized document embeddings improve topic coherence </p><p> a similarity measure for indefinite rankings </p><p> Then, we compute the overall average of those values for all the topics ().Eventually, to evaluate how diverse the topics generated by a single model are, we use the rankbiased overlap (RBO) [#b26](Webber et al., 2010). RBO compares two topics of the same model. The key qualities of this measure are twofold it allows disjointedness between the lists of topi </p>
<p> pre-training is a hot topic: contextualized document embeddings improve topic coherence </p><p> word features for latent dirichlet allocation </p><p> orate several types of information (Xun et al., 2017Das et al., 2015Nguyen et al., 2015[#b18]Petterson et al., 2010), use word relationships derived from external knowledge bases (Chen et al., 2013ref type"bibr" targe </p>
<p> pre-training is a hot topic: contextualized document embeddings improve topic coherence </p><p> discovering coherent topics using general knowledge </p><p> et al., 2015Nguyen et al., 2015Petterson et al., 2010), use word relationships derived from external knowledge bases [#b3](Chen et al., 2013Yang et al., 2015), or pretrained word embeddings (Das et al., 2015re </p>
<p> pre-training is a hot topic: contextualized document embeddings improve topic coherence </p><p> distributed representations of words and phrases and their compositionality </p><p> ilar to that described in Ding et al. (2018). First, we compute the average pairwise cosine similarity of the word embeddings of the top10 words in a topic using [#b14](Mikolov et al., 2013) embeddings. Then, we compute the overall average of those values for all the topics ().Eventually, to evaluate how diverse the topic </p>
<p> pre-training is a hot topic: contextualized document embeddings improve topic coherence </p><p> gaussian lda for topic models with word embeddings </p><p> e"bibr" target"b21"Rder et al., 2015).Topic models have inspired many extensions that incorporate several types of information (Xun et al., 2017[#b4]Das et al., 2015Nguyen et al., 2015Petterson et al., 2010), use word relationships deriv. word relationships derived from external knowledge bases (Chen et al., 2013Yang et al., 2015), or pretrained word embeddings [#b4](Das et al., 2015Dieng et al., 2019Nguyen et al., 2015). Even for neural topic models, th </p>
<p> pre-training is a hot topic: contextualized document embeddings improve topic coherence </p><p> discovering discrete latent topics with neural variational inference </p><p> riational inference (Miao et al., 2016Mnih and Gregor, 2014Srivastava and Sutton, 2017[#b12]Miao et al., 2017Ding et al., 2018). Miao et al. (2016)   framework that explicitly appro. b13"Miao et al. (2016)   framework that explicitly approximates the Dirichlet prior, using a Gaussian distribution to obtain more interpretable and coherent topics. Similar to their approach, [#b12]Miao et al. (2017) parameterize the multinomial distributions of each document, proposing three variants of neural topic models that are able to exhibit sparse top </p>
<p> pre-training is a hot topic: contextualized document embeddings improve topic coherence </p><p> a neural autoregressive topic model </p><p> oherent, but are affected by some added noisemixture of topics. Related WorkSeveral topic models are based on neural networks [#b10](Larochelle and Lauly, 2012Salakhutdinov and Hinton, 2009) or neural variational inference (Mi </p>
<p> pre-training is a hot topic: contextualized document embeddings improve topic coherence </p><p> discovering discrete latent topics with neural variational inference </p><p> riational inference (Miao et al., 2016Mnih and Gregor, 2014Srivastava and Sutton, 2017[#b12]Miao et al., 2017Ding et al., 2018). Miao et al. (2016)   framework that explicitly appro. b13"Miao et al. (2016)   framework that explicitly approximates the Dirichlet prior, using a Gaussian distribution to obtain more interpretable and coherent topics. Similar to their approach, [#b12]Miao et al. (2017) parameterize the multinomial distributions of each document, proposing three variants of neural topic models that are able to exhibit sparse top </p>
<p> Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification </p><p> recurrent models of visual attention </p><p> ven larger for higher accuracy. However, large images usually come at a high computational cost and high memory footprint, both of which grow quadratically with respect to the image height (or width) [#b37][38]. In realworld applications like contentbased image search [54] or autonomous vehicles [3]. ss of recognizing an image. In fact, CNNs are shown to be able to produce correct classification results with only a few classdiscriminative patches, such as the head of a dog or the wings of a bird [#b37][]. These regions are typically smaller than the whole image, and thus require much. "bibr" target"b35"36] and EfficientNet [50], can be deployed for higher efficiency. This differentiates our method from early recurrent attention methods [#b37][38] which adopt pure recurrent models. In addition, we focus on improving the computational efficiency under the adaptive inference setting, while most existing w. e"bibr" target"b25"26,12,8].One similar work to our GFNet is the recurrent visual attention model proposed in [#b37][38]. However, our method differs from it in two important aspects 1) we adopt a flexible and general CNNbased framework that is compatible with a wide variety o </p>
<p> Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification </p><p> deep visual-semantic alignments for generating image descriptions </p><p> centrate on the related regions of the image when generating the word sequence [[#b26]]. In the context of image recognition, the attention mechanism is typically exploited to extract information from some t </p>
<p> Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification </p><p> ran el-yaniv, and yoshua bengio. binarized neural networks </p><p> "[] or quantizing the weights [[#b23]]. Another technique is knowledge distillation [15], which trains a small network to </p>
<p> Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification </p><p> speedboost: anytime prediction with uniform near-optimality </p><p> 50], etc.) in the budgeted batch classification setting [20], where the test set comes with a given computational budget, and the anytime prediction setting [#b12][], where the network can be forced to output a prediction at any given point in time. We also benchmark the average late. hod (1) budgeted batch classification [20], where the network needs to classify a set of test samples within a given computational budget (2) anytime prediction [#b12][], where the network can be forced to output a prediction at any given point in time. As discussed in ref type"bibr" t </p>
<p> Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification </p><p> stacked attention networks for image question answering </p><p> mage when generating the word sequence [[#b64]]. In the context of image recognition, the attention mechanism is typically exploited to extract information from some taskrelevant regions ref type"bibr" ta </p>
<p> Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification </p><p> deep residual learning for image recognition </p><p> CNNs have achieved superhumanlevel performance on the competitive ILSVRC [9] competition with 224224 or 320320 images [[#b13]]. Recent works ref type"bibr" target". b8"[9], with 1.2 million images for training and 50,000 images for validation. We adopt the same data augmentation and preprocessing configurations as [[#b13]]. In our implementation, we estimate the confidence thresholds of GFNet on the training set, since we find that it achie. eoftheart CNNs, including MobileNetV3 [16], RegNetY [40], EfficientNet [50], ResNet [#b13][14] and DenseNet [22]. These networks serve as the two deep encoders in our methods. In Budgeted batch classification, for ea. EfficientNets [50], we use a gated recurrent unit (GRU) with 256 hidden units [7] in the patch proposal network . For ResNets [#b13][14] and DenseNets [22], we adopt 1024 hidden units and remove the convolutional layer in . This does not hurt the efficiency. olutional layer in . This does not hurt the efficiency since here the computational cost of  is negligible compared with the two encoders. With regards to the recurrent classifier f C , for ResNets [#b13][14], DenseNets [22] and RegNets [40], we use a GRU with 1024 hidden units. For MobileNet. t"b15"16] to match the reported performance, and use the obtained networks as the pretrained models. For H  W finetuning, we use the same training hyperparameters as the training process [#b13][]ref. bal encoder f G after initialization and do not train it any more, which we find is beneficial for the final performance of the Glance Step.Stage I. We train all networks using a SGD optimizer [#b13][] with a cosine learning rate annealing technique and a Nesterov momentum of 0.9 </p>
<p> Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification </p><p> condensenet: an efficient densenet using learned group convolutions </p><p> r" target"b2"[3], computation usually translates into latency and power consumption, which should be minimized for both safety and economical reasons [[#b20]].In this paper, we seek to reduce the computational cost introduced by highresolution inputs in image classifica. t of the stateoftheart lightweighted CNNs, such as MobileNets [], CondenseNet [#b20][21], ShuffleNets [] and EfficientNet [50], can. s to develop efficient network architectures, such as MobileNets [], CondenseNet [#b20][21], ShuffleNets [] and EfficientNet [50]. Sin. ve baselines, i.e., MnasNets [49], ShuffleNetsV2 [36], MobileNetsV2 [43], CondenseNets [#b20][21], FBNets [59], ProxylessNAS [5], SkipNet [55], SA </p>
<p> Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification </p><p> deep visual-semantic alignments for generating image descriptions </p><p> centrate on the related regions of the image when generating the word sequence [[#b26]]. In the context of image recognition, the attention mechanism is typically exploited to extract information from some t </p>
<p> Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification </p><p> learning efficient convolutional networks through network slimming </p><p> iderable number of redundant weights [11], some other approaches focus on pruning [[#b33]] or quantizing the weights [41,24,ref type"b </p>
<p> Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification </p><p> joint active feature acquisition and classification with variable-size set encoding </p><p> put. With these design innovations, the proposed GFNet has achieved new SOTA performance on ImageNet in terms of both the theoretical computational efficiency and actual inference speed. In addition, [#b45][46] shares a similar spirit to us in selecting important features with reinforcement learning, but it is not based on CNNs nor image data. div xmlns"h </p>
<p> Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification </p><p> optimal brain damage </p><p> f type"bibr" target"b49"[50]. Since deep networks typically have a considerable number of redundant weights [11], some other approaches focus on pruning [#b29][] or quantizing the weights ref type"b </p>
<p> acceltcp: accelerating network applications with stateful tcp offloading </p><p> server network scalability and tcp offload </p><p> [pos is Open access to the Proceedings of the 17th USENIX Symposium on Networked] mprovement. NIC Offload of TCP FeaturesThere have been a large number of works and debates on NIC offloading of TCP features [#b33][]. While AccelTCP pursues the same benef </p>
<p> acceltcp: accelerating network applications with stateful tcp offloading </p><p> shenango: achieving high cpu efficiency for latencysensitive datacenter workloads </p><p> [pos is Open access to the Proceedings of the 17th USENIX Symposium on Networked] nt of compute cycles. One can optimize them by exploiting flowlevel parallelism [[#b57]] or by steering the tasks into fast and slow paths [48] on kernelbypass stacks. However, the inherent complexity makes it. [pos is Open access to the Proceedings of the 17th USENIX Symposium on Networked] parallelism on multicore systems by flow steering on NIC. More recently, systems like ZygOS [63], Shinjuku [42], and Shenango [#b57][59] further improve kernelbypass stack by reducing the tail latency, employing techniques like task stealing, centralized packet distribution, and dynamic core r </p>
<p> acceltcp: accelerating network applications with stateful tcp offloading </p><p> connection handoff policies for tcp offload network interfaces </p><p> [pos is Open access to the Proceedings of the 17th USENIX Symposium on Networked] Offload of TCP FeaturesThere have been a large number of works and debates on NIC offloading of TCP features [[#b48]]. While AccelTCP pursues the same benefit of saving CPU cycles 2 All 120 flowprocessing cores in Agilio LX are enabled.. [pos is Open access to the Proceedings of the 17th USENIX Symposium on Networked] hance these stacks by offloading connection management tasks to NIC.NIC offload Existing TCP offloads mostly focus on improving large message transfer either by offloading the whole TCP stack [#b48][50] or by selectively offloading common sendreceive operations [46]. In contrast, our work focuses on connection management </p>
<p> acceltcp: accelerating network applications with stateful tcp offloading </p><p> tcp offload is a dumb idea whose time has come </p><p> [pos is Open access to the Proceedings of the 17th USENIX Symposium on Networked] ave been a large number of works and debates on NIC offloading of TCP features [[#b55]]. While AccelTCP pursues the same benefit of saving CPU cycles 2 All 120 flowprocessing cores in Agilio LX are enabled. and memory bandwidth, it targets a diff </p>
<p> acceltcp: accelerating network applications with stateful tcp offloading </p><p> zygos: achieving low tail latency for microsecond-scale networked tasks </p><p> [pos is Open access to the Proceedings of the 17th USENIX Symposium on Networked] Most of them employ a fast userlevel packet IO [10], and exploit high parallelism on multicore systems by flow steering on NIC. More recently, systems like ZygOS [#b61][63], Shinjuku [42], and Shenango [59] further improve kernelbypass stack by reducing th </p>
<p> acceltcp: accelerating network applications with stateful tcp offloading </p><p> understanding pcie performance for end host networking </p><p> [pos is Open access to the Proceedings of the 17th USENIX Symposium on Networked] efit to realworld applications.Keyvalue store (Redis) We evaluate the effectiveness of AccelTCP with Redis (v.4.0.8) [17], a popular 6 Theoretical maximum throughput is 63 Gbps according to [#b56][58].  inmemory keyvalue store. We use Redis on mTCP as a baseline server while we port it to use AccelTCP for comparison. We test with the USR workload from Fac </p>
<p> acceltcp: accelerating network applications with stateful tcp offloading </p><p> the design and implementation of open vswitch </p><p> [pos is Open access to the Proceedings of the 17th USENIX Symposium on Networked] as they support flexible packet processing at high speed with programming languages like C or P4 [33]. Re cent smart NICs are flexible enough to run Open vSwitch [#b60][62], Berkeley packet filter [49], or even keyvalue lookup [53], often achieving 2x to 3x </p>
<p> acceltcp: accelerating network applications with stateful tcp offloading </p><p> megapipe: a new programming interface for scalable network i/o </p><p> [pos is Open access to the Proceedings of the 17th USENIX Symposium on Networked] ns. To better understand the cost, we analyze the overhead of the TCP stack operations in these workloads.To avoid the inefficiency of the kernel stack [[#b37]], we use mTCP [41], a scalable userlevel TCP stack on DPDK ref type"bibr" target </p>
<p> acceltcp: accelerating network applications with stateful tcp offloading </p><p> megapipe: a new programming interface for scalable network i/o </p><p> [pos is Open access to the Proceedings of the 17th USENIX Symposium on Networked] ns. To better understand the cost, we analyze the overhead of the TCP stack operations in these workloads.To avoid the inefficiency of the kernel stack [[#b37]], we use mTCP [41], a scalable userlevel TCP stack on DPDK ref type"bibr" target </p>
<p> acceltcp: accelerating network applications with stateful tcp offloading </p><p> understanding pcie performance for end host networking </p><p> [pos is Open access to the Proceedings of the 17th USENIX Symposium on Networked] efit to realworld applications.Keyvalue store (Redis) We evaluate the effectiveness of AccelTCP with Redis (v.4.0.8) [17], a popular 6 Theoretical maximum throughput is 63 Gbps according to [#b56][58].  inmemory keyvalue store. We use Redis on mTCP as a baseline server while we port it to use AccelTCP for comparison. We test with the USR workload from Fac </p>
<p> acceltcp: accelerating network applications with stateful tcp offloading </p><p> uno: uniflying host and smart nic offload for flexible packet processing </p><p> [pos is Open access to the Proceedings of the 17th USENIX Symposium on Networked] focuses on connection management and proxying whose performance is often critical to modern network workloads, while we intentionally avoid the complexity of application data transfer offloading. UNO [#b50][52] and Metron [45] strive to achieve optimal network function (NF) performance with NIC offload based on runtime traffic sta </p>
<p> semantics of the black-box: can knowledge graphs help make deep learning systems more interpretable and explainable? </p><p> shades of knowledgeinfused learning for enhancing deep learning </p><p> h scenarios, it is exceptionally challenging to probe the model's mechanism without the support of background knowledge [1]. Although Neural Attention Models (NAM) [#b1][2] are endowed with a certain degree of interpretability in visualizing parts of the sentence which are focused on answering the question, they cannot provide furt. results (e.g., identifying emerging subevents in natural disasters [8]), (c) discover inherent bias in the model's predictive strategy (e.g., contextual modeling [#b1][2][9]), (d) prevent prediction errors in unintuitive scenarios (e.g. adversarial examples [10], CHECKLIST ref type"bibr" targ. nd unlabeled corpus for transfer learning) [11]. Also, knowledgeinfusion during the model learning using informationtheoretic loss function (e.g., KL divergence [#b1][2]) can check conceptual drifting at the representational level through weaksupervision from KG. Alternatively, the loss of information during learning can be sup. escriptive performance of generating explanations and interpretations.Infusing domain knowledge in DL models can be categorized into the shallow infusion, semideep infusion, and deep infusion [#b1][2]. In shallow infusion, both the external information and method of knowledge infusion is shallow, i.e., Word2Vec or GloVE embeddings are reconstructed using doma. orms an inherent structure, that raises challenges in natural language understanding (1) Anaphorawhere sentences are purposefully paraphrased to elicit meaningful responses from an agent (or user) [#b1](2) The clinical conversation contains implicit references to healthcare conditions, developing sparsity in the clinicallyrelevant concepts. Such a problem scenari </p>
<p> semantics of the black-box: can knowledge graphs help make deep learning systems more interpretable and explainable? </p><p> towards explainable artificial intelligence </p><p> [pos is II. NEED FOR EXPLAINABILITY AND INTERPRETABILITY] d to establish a generic definition of explainability, which is the ability to generate humancomprehensible explanations around the decisionmaking process [5][6] [#b6][7]. In contrast, interpretability is the ability to discern the internal mechanisms of any module. Key reasons we need explainability and interpretability are to </p>
<p> semantics of the black-box: can knowledge graphs help make deep learning systems more interpretable and explainable? </p><p> unsupervised detection of sub-events in large scale disasters </p><p> [pos is II. NEED FOR EXPLAINABILITY AND INTERPRETABILITY] rlier findings), and metainformation such as time and spacelocation may be critical in understanding, interpreting, and explaining results (e.g., identifying emerging subevents in natural disasters [#b7][8]), (c) discover inherent bias in the model's predictive strategy (e.g., contextual modeling [2][9]), (d) prevent prediction e </p>
<p> semantics of the black-box: can knowledge graphs help make deep learning systems more interpretable and explainable? </p><p> towards explainable artificial intelligence </p><p> [pos is II. NEED FOR EXPLAINABILITY AND INTERPRETABILITY] d to establish a generic definition of explainability, which is the ability to generate humancomprehensible explanations around the decisionmaking process [5][6] [#b6][7]. In contrast, interpretability is the ability to discern the internal mechanisms of any module. Key reasons we need explainability and interpretability are to </p>
<p> semantics of the black-box: can knowledge graphs help make deep learning systems more interpretable and explainable? </p><p> knowledge-infused deep learning </p><p> [pos is II. NEED FOR EXPLAINABILITY AND INTERPRETABILITY] ons would be in natural language explaining the decision, while interpretations can be statistical or conceptual (using either generic or domainspecific KG [12], [#b8][9]) in nature pertaining to its inner functioning."Explanations can be thought of as answers to cascading "why questions" in the context of model prediction </p>
<p> semantics of the black-box: can knowledge graphs help make deep learning systems more interpretable and explainable? </p><p> knowledge graphs to empower humanity-inspired ai systems </p><p> [pos is II. NEED FOR EXPLAINABILITY AND INTERPRETABILITY] of external contextual information required to understand online conversations, mainly when the problem is a low resource (insufficient benchmark datasets and unlabeled corpus for transfer learning) [#b10][11]. Also, knowledgeinfusion during the model learning using informationtheoretic loss function (e.g., KL divergence [2]) ca </p>
<p> semantics of the black-box: can knowledge graphs help make deep learning systems more interpretable and explainable? </p><p> multimodal explanations: justifying decisions and pointing to the evidence </p><p> [pos is II. NEED FOR EXPLAINABILITY AND INTERPRETABILITY] tracing over KG. Future advances in this area would seek AI systems that can help stakeholders (e.g., instructors, public health experts) to conceptually understand the working of involved AI systems [#b19][20]. There is also a pressing requirement for benchmark datasets which assess the quality of explanations derived from model outcomes and interpretability of the </p>
<p> semantics of the black-box: can knowledge graphs help make deep learning systems more interpretable and explainable? </p><p> multimodal explanations: justifying decisions and pointing to the evidence </p><p> [pos is II. NEED FOR EXPLAINABILITY AND INTERPRETABILITY] tracing over KG. Future advances in this area would seek AI systems that can help stakeholders (e.g., instructors, public health experts) to conceptually understand the working of involved AI systems [#b19][20]. There is also a pressing requirement for benchmark datasets which assess the quality of explanations derived from model outcomes and interpretability of the </p>
<p> semantics of the black-box: can knowledge graphs help make deep learning systems more interpretable and explainable? </p><p> knowledge-infused deep learning </p><p> [pos is II. NEED FOR EXPLAINABILITY AND INTERPRETABILITY] ons would be in natural language explaining the decision, while interpretations can be statistical or conceptual (using either generic or domainspecific KG [12], [#b8][9]) in nature pertaining to its inner functioning."Explanations can be thought of as answers to cascading "why questions" in the context of model prediction </p>
<p> semantics of the black-box: can knowledge graphs help make deep learning systems more interpretable and explainable? </p><p> knowledge-infused deep learning </p><p> [pos is II. NEED FOR EXPLAINABILITY AND INTERPRETABILITY] ons would be in natural language explaining the decision, while interpretations can be statistical or conceptual (using either generic or domainspecific KG [12], [#b8][9]) in nature pertaining to its inner functioning."Explanations can be thought of as answers to cascading "why questions" in the context of model prediction </p>
<p> semantics of the black-box: can knowledge graphs help make deep learning systems more interpretable and explainable? </p><p> nature inspired algorithms in remote sensing image classification </p><p> [pos is II. NEED FOR EXPLAINABILITY AND INTERPRETABILITY] l the explanation correlates with the model's decision making. It is considered plausible when it has a humanunderstandable justification for the prediction [13] [#b13][14]. Such systems are considered to be potentially useful for realworld decision making in various domains, especially healthcare and education technology.  </p>
<p> idlg: improved deep leakage from gradients </p><p> deep leakage from gradients </p><p> gated gradients. In these frameworks, it is a common practice to share only the gradients in order protect the proprietary data. However, recent work by Zhu et al., "Deep Leakage from Gradient" (DLG) [#b0][1] showed the possibility to steal the private training data from the shared gradients of other participants.The main idea of DLG is to generate dummy data. initely leaked by sharing gradients of a Neural Network (NN) trained with crossentropy loss. This enables us to always extract the groundtruth labels and significantly simplify the objective of DLG [#b0][1] in order to extract goodquality data. Hence, we name our approach, Improved DLG (iDLG). The main contributions of our work includes By revealing the r. ct the groundtruth labels from the shared gradients with 100 accuracy, which facilitates the data extraction with better fidelity. We empirically demonstrate the advantages of iDLG over DLG [#b0][1] via comparing the accuracy of extracted labels and the fidelity of extracted data on three datasets.The rest of the paper is organised as follows Sectio. G through the experimental evaluation, and Section 4 concludes the paper with discussion. MethodologyRecent work by Zhu et al. [#b0][1] presents an approach (DLG) to steal the proprietary data protected by the participants in distributed learning from the shared gradients. In their method, they. ype"bibr" target"b2"3  ExperimentsIn this section, we empirically demonstrate the advantages of our (iDLG) method over DLG [#b0][1]. We perform experiments on the classification task over three datasets MNIST [8], CIFAR100 [. " target"b7"[8], CIFAR100 [9], and LFW [10] with 10, 100, and 5749 categories respectively. Following the settings in [#b0][1], we use the randomly initialized LeNet for all experiments. LBFGS [11] with learning rate 1 is used as the optimizer. For. zed LeNet for all experiments. LBFGS [11] with learning rate 1 is used as the optimizer. For fast training, we resize all images in LFW to 32  32.For DLG [#b0][1], as described by the authors, we start the procedure with the randomly initialized dummy data and outputs (x , y ), then iteratively update them to minimize the. labels c , and (ii) the fidelity of the extracted Dataset DLG iDLG MNIST 89.9 100.0 CIFAR100 83.3 100.0 LFW 79.1 100.0 Table 1 Accuracy of the extracted labels for DLG [#b0][1] and iDLG. Note that iDLG always extracts the correct label as opposed to DLG which extracts wrong labels frequently. threshold of good fidelity. From left to ri </p>
<p> idlg: improved deep leakage from gradients </p><p> on the limited memory bfgs method for large scale optimization </p><p> t"b9"[10] with 10, 100, and 5749 categories respectively. Following the settings in [1], we use the randomly initialized LeNet for all experiments. LBFGS [#b10][11] with learning rate 1 is used as the optimizer. For fast training, we resize all images in LFW to 32  32.For DLG [1]ref </p>
<p> idlg: improved deep leakage from gradients </p><p> on the limited memory bfgs method for large scale optimization </p><p> t"b9"[10] with 10, 100, and 5749 categories respectively. Following the settings in [1], we use the randomly initialized LeNet for all experiments. LBFGS [#b10][11] with learning rate 1 is used as the optimizer. For fast training, we resize all images in LFW to 32  32.For DLG [1]ref </p>
<p> idlg: improved deep leakage from gradients </p><p> collaborative learning for deep neural networks </p><p> "  IntroductionIn multinode distributed learning systems such as Collaborative Learning [[#b2]] and Federated Learning [5,6,ref type"bibr" target. te. LG  W  W 2 FCalculate the loss (difference between gradients). 6x   x   x LG Update the dummy datum. 7 end for [#b2]3  ExperimentsIn this section, we empirically demonstrate the advantages of our (iDLG) method ove </p>
<p> idlg: improved deep leakage from gradients </p><p> privacy-preserving deep learning </p><p> Desc    IntroductionIn multinode distributed learning systems such as Collaborative Learning [#b1][] and Federated Learning [5,ref type"bibr" targe </p>
<p> idlg: improved deep leakage from gradients </p><p> on the limited memory bfgs method for large scale optimization </p><p> t"b9"[10] with 10, 100, and 5749 categories respectively. Following the settings in [1], we use the randomly initialized LeNet for all experiments. LBFGS [#b10][11] with learning rate 1 is used as the optimizer. For fast training, we resize all images in LFW to 32  32.For DLG [1]ref </p>
<p> idlg: improved deep leakage from gradients </p><p> privacy-preserving deep learning </p><p> Desc    IntroductionIn multinode distributed learning systems such as Collaborative Learning [#b1][] and Federated Learning [5,ref type"bibr" targe </p>
<p> idlg: improved deep leakage from gradients </p><p> federated learning: collaborative machine learning without centralized training data </p><p> ollaborative Learning [] and Federated Learning [[#b5]], it is widely believed that sharing gradients between nodes will not leak the private training data. In the popular setup, </p>
<p> idlg: improved deep leakage from gradients </p><p> on the limited memory bfgs method for large scale optimization </p><p> t"b9"[10] with 10, 100, and 5749 categories respectively. Following the settings in [1], we use the randomly initialized LeNet for all experiments. LBFGS [#b10][11] with learning rate 1 is used as the optimizer. For fast training, we resize all images in LFW to 32  32.For DLG [1]ref </p>
<p> idlg: improved deep leakage from gradients </p><p> collaborative learning for deep neural networks </p><p> "  IntroductionIn multinode distributed learning systems such as Collaborative Learning [[#b2]] and Federated Learning [5,6,ref type"bibr" target. te. LG  W  W 2 FCalculate the loss (difference between gradients). 6x   x   x LG Update the dummy datum. 7 end for [#b2]3  ExperimentsIn this section, we empirically demonstrate the advantages of our (iDLG) method ove </p>
<p> idlg: improved deep leakage from gradients </p><p> gradient-based learning applied to document recognition </p><p> section, we empirically demonstrate the advantages of our (iDLG) method over DLG [1]. We perform experiments on the classification task over three datasets MNIST [#b7][8], CIFAR100 [9], and LFW [10] with 10, 100, and 5749 categories respectively. Following t </p>
<p> Big Self-Supervised Models are Strong Semi-Supervised Learners </p><p> mean teachers are better role models: weight-averaged consistency targets improve semi-supervised deep learning results </p><p>  </p>
<p> Big Self-Supervised Models are Strong Semi-Supervised Learners </p><p> colorful image colorization </p><p> [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] re associated with labels. 3 As in previous work, we also report performance when training a linear classifier on top of a fixed representation with all labels [#b30][] to directly evaluate SimCLRv2 representations. We use. [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] t"b38"39,47]. There are other approaches to selfsupervised learning that are based on handcrafted pretext tasks [[#b30]]. We also note a concurrent work on adva </p>
<p> Big Self-Supervised Models are Strong Semi-Supervised Learners </p><p> self-supervised gans via auxiliary rotation loss </p><p> [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] ashion instead of a generative one as in [[#b46]]. There are other approaches to selfsupervised learning that are based on handcrafted pretext tasks [48,ref type"bibr" t </p>
<p> Big Self-Supervised Models are Strong Semi-Supervised Learners </p><p> regularization with stochastic transformations and perturbations for deep semi-supervised learning </p><p>  </p>
<p> Big Self-Supervised Models are Strong Semi-Supervised Learners </p><p> model compression </p><p> [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] end, we make use of unlabeled data for a second time to encourage the student network to mimic the teacher network's label predictions. Thus, the distillation [[#b22]] phase of our method using unlabeled data is reminiscent of the use of pseudo labels [11] in selftraining ref type"bibr". [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] lftraining  knowledge distillation via unlabeled examples.To further improve the network for the target task, here we leverage the unlabeled data directly for the target task. Inspired by [#b22][]ref. [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] type"bibr" target"b50"[51], which we compare against in Appendix G. Our work also extends the "unsupervised pretrain, supervised finetune" paradigm by combining it with (self)distillation [#b22][] using unlabeled data.Taskspecific use of unlabeled data. Aside from th </p>
<p> Big Self-Supervised Models are Strong Semi-Supervised Learners </p><p> dandelion mane, vijay vasudevan, and quoc v le. autoaugment: learning augmentation strategies from data </p><p> [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] ) models of varied sizes on different label fractions. ResNets with depths of 50, 101, 152, width multiplier of 1, 2 (wo SK) are presented here. For supervised models on 110 labels, AutoAugment [#b33][34] and label smoothing [35] are used. Increasing the size of SimCLRv2 models by 10, from ResNet50 to ResNet152 (2), impr. [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] It is also worth noting that these results may reflect a "ceiling effect" as the performance gets closer to the ceiling, the improvement becomes smaller. , Supervised learning with autoaugmentation [#b33][34] and label smoothing [67] are applied for 110 label fractions, (c) semisupervised learning by finetuning SimCLRv2.p </p>
<p> Big Self-Supervised Models are Strong Semi-Supervised Learners </p><p> colorful image colorization </p><p> [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] re associated with labels. 3 As in previous work, we also report performance when training a linear classifier on top of a fixed representation with all labels [#b30][] to directly evaluate SimCLRv2 representations. We use. [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] t"b38"39,47]. There are other approaches to selfsupervised learning that are based on handcrafted pretext tasks [[#b30]]. We also note a concurrent work on adva </p>
<p> Big Self-Supervised Models are Strong Semi-Supervised Learners </p><p> rethinking the inception architecture for computer vision </p><p> [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] with depths of 50, 101, 152, width multiplier of 1, 2 (wo SK) are presented here. For supervised models on 110 labels, AutoAugment [34] and label smoothing [#b34][35] are used. Increasing the size of SimCLRv2 models by 10, from ResNet50 to ResNet152 (2), improves label efficiency by 10.   Figure 4r </p>
<p> Big Self-Supervised Models are Strong Semi-Supervised Learners </p><p> selective kernel networks </p><p> [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] et50 (4), we train models that are deeper but less wide. The largest model we train is a 152layer ResNet [25] with 3 wider channels and selective kernels (SK) [#b27][28], a channelwise attention mechanism that improves the parameter efficiency of the network. By scaling up the model from ResNet50 to ResNet152 (3SK), we ob. [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] 005  sqrt(BatchSize)) for standard ResNets [25], and 0.064 ( 0.002  sqrt(BatchSize)) for larger ResNets variants (with width multiplier larger than 1 andor SK [#b27][28]). A batch size of 1024 is used. Similar to [1], we finetune for 60 epochs with 1 of labels, and 30 epochs with 10 of labels, as well. [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] r Models Are More LabelEfficientIn order to study the effectiveness of big models, we train ResNet models by varying width and depth as well as whether or not to use selective kernels (SK) [#b27][28]. 4 Whenever SK is used, we also use the ResNetD [36] variant of ResNet. The smallest model is the standard ResNet50, an. [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] "other"Figure B.1 shows the top1 accuracy of finetuned SimCLRv2 models of different sizes. It shows that (1) bigger models are better, but (2) with SK [#b27][28], better performance can be achieved with the same parameter count. It is worth to note that, in this work, we do not leverage group convolution for SK ref ty. [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] ith SK [#b27][28], better performance can be achieved with the same parameter count. It is worth to note that, in this work, we do not leverage group convolution for SK [#b27][28] and we use only 3  3 kernels. We expect further improvement in terms of parameter efficiency if group convolution is utilized.  div xmlns"http </p>
<p> Big Self-Supervised Models are Strong Semi-Supervised Learners </p><p> s4l: self-supervised semi-supervised learning </p><p> [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] compact model.3 Empirical Study Settings and Implementation DetailsFollowing the semisupervised learning setting in [#b29][], we evaluate the proposed method on ImageNet ILSVRC2012 [21]. [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] inetuning. For our smaller models, we use selfdistilled ResNet152 (3SK) as the teacher. Methods using unlabeled data in a taskspecific way Pseudolabel [[#b29]] ResNet50 51.6 82.4 VATEntropy Min. [[#b29]]ref. [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] ef type"bibr" target"b10"[[#b29]] ResNet50 51.6 82.4 VATEntropy Min. [[#b29]] ResNet50 47.0 83.4 UDA (w. RandAug) [14] ResNet50 68.8 88.5 FixMatch (w. RandAug) [15. [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] 83.4 UDA (w. RandAug) [14] ResNet50 68.8 88.5 FixMatch (w. RandAug) [15] ResNet50 71.5 89.1 S4L (RotVATEntropy Min.) [#b29][30]  , a subarea within selfsupervised learning. These contrastive learning based approaches learn representations in a discriminative fashion instead of </p>
<p> Big Self-Supervised Models are Strong Semi-Supervised Learners </p><p> semi-supervised sequence learning </p><p> [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] roach has become predominant in natural language processing, where one first trains a large language model on unlabeled text (e.g., Wikipedia), and then finetunes the model on a few labeled examples [#b4][5][6][7][8][9]ref. [pos is arXiv:2006.10029v1 [cs.LG] 17 Jun 2020] nsupervised or selfsupervised pretraining followed by supervised finetuning on a few labeled examples has been extensively used in natural language processing [[#b4][][8][9], but has only shown promising results in com </p>
<p> ansor : generating high-performance tensor programs for deep learning </p><p> {tvm}: an automated end-to-end optimizing compiler for deep learning </p><p> "bibr" target"b30"[32].Given the importance of DNNs' performance, researchers and industry practitioners have turned to searchbased compilation [[#b9]] for automated generation of tensor progr. ulaMatrix Multiplication  ",     ", )  ),  ) Figure 1 The computation definition of matrix multiplication. ple compiler techniques have been introduced (e.g., TVM [#b9][10], Halide [38], Tensor Comprehensions [45]). Users define the computation in a form sim. efficient because it has to deal with the unnecessary exponential explosion of the search space. Typically, the compiler partitions the large computational graph of a DNN into several small subgraphs [#b9][10]. This partition has a negligible effect on the performance thanks to the layerbylayer construction nature of DNNs. This brings the final challenge of Ansor. atrix multiplications with large input sizes. It is hard for compilationbased approaches to beat manuallywritten assembly code on large matrix multiplications [[#b9]], as the code has been handoptimized for decades.  Ablation study. We run variants of Ansor on two test cases in Figure. "bibr" target"b26"28,33]. The latest one with beam search and learned cost model performs the best among them, which is also used in our evaluation. TVM [#b9][10] utilizes a similar scheduling language and includes a templateguided search framework AutoTVM [11]. Similar to the motiva. duling language similar to Halide language, and it needs manual scheduling. TensorComprehensions can search for GPU code automatically, but it is not yet meant to be used for computebounded problems [#b9][10]. It cannot outperform TVM on operators like conv2d and matmul [#b9][]. This is because. arch for GPU code automatically, but it is not yet meant to be used for computebounded problems [#b9][10]. It cannot outperform TVM on operators like conv2d and matmul [#b9][]. This is because of the lack of certain optimizations and the inaccurate implicit cost model in the polyhedral formulat. graph level without changing the internal implementations of operators. The common optimizations at graph level include layout optimizations [29], operator fusion [#b9][], constant folding [39], autobatching [30], a </p>
<p> ansor : generating high-performance tensor programs for deep learning </p><p> parallel associative reductions in halide </p><p> he fixed order of sequential decisions limits the design of the search space. For example, some optimization needs to add new nodes to the computational graph (e.g., adding cache nodes, using rfactor [#b40][42]). The number of decisions for different programs becomes different. It is thus hard to align the incomplete programs for a fair comparison. (3) Sequential con. d, now the final output node writes its results into a cache block, and the cache block will be written to the main memory at once when all data in the block is computed.Rule 6 can use rfactor [#b40][42] to factorize a reduction loop into a space loop to bring more parallelism.Figure 5 shows three examples of the </p>
<p> ansor : generating high-performance tensor programs for deep learning </p><p> automatically scheduling halide image processing pipelines </p><p> both manual optimization and automatic search. Halide has three versions of autoscheduler based on different techniques [[#b31]]. The latest one with beam search and learned cost model performs the best among them, which is also used in our evaluation. TVM [ </p>
<p> ansor : generating high-performance tensor programs for deep learning </p><p> tiramisu: a polyhedral compiler for expressing fast and portable code </p><p> This is because BERT consists of many matrix multiplications with large input sizes. It is hard for compilationbased approaches to beat manuallywritten assembly code on large matrix multiplications [#b4][], as the code has been handoptimized for decades.  Ablation study. We run variant. zation of programs as an integer linear programming (ILP) problem. It optimizes a program with affine loop transformation that minimizes the data reuse distance between dependent statements. Tiramisu [#b4][5] and TensorComprehensions [45] are two polyhedral compilers that also target deep learning domain. Tiramisu provides a sched </p>
<p> ansor : generating high-performance tensor programs for deep learning </p><p> pytorch: an imperative style, high-performance deep learning library </p><p> lution and matrix multiplication) and directed edges represent the dependencies between operators. Existing deep learning frameworks (e.g., Tensorflow [1], PyTorch [#b34][36], MXNet [9]) map the operators in DNNs to vendorprovided kernel libraries (e.g., CuDNN [12]. uate them with two batch sizes (1 and 16). In total, there are 10 operators 4 shape configurations 2 batch size ( 80) test cases. We run these test cases on the Intel CPU.We include PyTorch [#b34][36], Halide autoscheduler [2], FlexTensor [53] and AutoTVM ref type"bibr" target"b1 </p>
<p> ansor : generating high-performance tensor programs for deep learning </p><p> the cityscapes dataset for semantic urban scene understanding </p><p> text xmllang"en"  IntroductionLow latency execution of deep neural networks (DNN) plays a critical role in autonomous driving [#b12][13], augmented reality [3], language translation [14], and other applications of AI. DNNs </p>
<p> ansor : generating high-performance tensor programs for deep learning </p><p> fast algorithms for convolutional neural networks </p><p> 3While the presented rules are practical enough to cover the structures for most operators, there are always exceptions. For example, some special algorithms (e.g., Winograd convolution [#b25][27]) and accelerator intrinsics (e.g., TensorCore [34]) require special tile structures to be effective. Although the templat. l a few inner loops. We also randomly change the computation location of some nodes in the program to make a slight tweak to the tile structure. If some special algorithms (e.g., Winograd convolution [#b25][27]) require special annotation policy to be effective, we allow users to give simple hints in the computation definition to adjust the annotation policy. Finally </p>
<p> ansor : generating high-performance tensor programs for deep learning </p><p> tiramisu: a polyhedral compiler for expressing fast and portable code </p><p> This is because BERT consists of many matrix multiplications with large input sizes. It is hard for compilationbased approaches to beat manuallywritten assembly code on large matrix multiplications [#b4][], as the code has been handoptimized for decades.  Ablation study. We run variant. zation of programs as an integer linear programming (ILP) problem. It optimizes a program with affine loop transformation that minimizes the data reuse distance between dependent statements. Tiramisu [#b4][5] and TensorComprehensions [45] are two polyhedral compilers that also target deep learning domain. Tiramisu provides a sched </p>
<p> ansor : generating high-performance tensor programs for deep learning </p><p> polyhedral parallel code generation for cuda </p><p> some important optimizations (e.g., operator fusion).Polyhedral compilation models. Polyhedral compilation model [[#b46]] formulates the optimization of programs as an integer linear programming (ILP) problem. It optimizes a program with affine loop transformation that minimizes t </p>
<p> ansor : generating high-performance tensor programs for deep learning </p><p> machine learning systems are stuck in a rut </p><p> hardware platform and operator. The significant manual efforts required to produce efficient operator implementations for each target accelerator limit the development and innovation of new operators [#b5][6] and specialized accelerators [32].Given the importance of DNNs' performance, researchers and industry practitioners </p>
<p> ansor : generating high-performance tensor programs for deep learning </p><p> fftw: an adaptive software architecture for the fft </p><p> r relies on userspecified search space, while Ansor constructs the search space automatically. Traditional highperformance libraries such as ATLAS [51] and FFTW [#b15][16] also utilizes autotuning. More recent works NeuroVectorizer [17] and AutoPhase [18, </p>
<p> score-based generative modeling through stochastic differential equations </p><p> generative modeling by estimating gradients of the data distribution </p><p>  </p>
<p> score-based generative modeling through stochastic differential equations </p><p> denoising diffusion probabilistic models </p><p> in dynamics to sample from a sequence of decreasing noise scales during generation. Denoising diffusion probabilistic modeling (DDPM) (SohlDickstein et al., 2015[#b17]Ho et al., 2020) trains a sequence of probabilistic models to reverse each step of the noise corruption, using knowledge of the functional form of the reverse dist. s et al., 2017Goyal et al., 2017), have proven effective at generation of images (Song amp Ermon, 20192020[#b17]Ho et al., 2020), audio (Chen et al., 2020Kong et al., 2020), graphs ref type"bibr" tar. sing a single unconditional scorebased model without retraining.Unified picture The methods of SMLD and DDPM can be unified into our framework as discretizations of different SDEs. Although [#b17]Ho et al. (2020) has reported higher sample quality than Song amp Ermon (20192020), we show that with better archi. to performing ancestral sampling from the graphical model  N i"1 p  px i1  x i q. The objective Eq. ( 3) described here is equivalent to L simple in [#b17]Ho et al. (2020), but we rewrite it in a slightly different form to expose more similarity to Eq. (1). Like Eq. (1), Eq. ( ref type"formula" target"formula_3". ype"bibr"(Chen et al., 2019) 3.28 46.37 FFJORD (Grathwohl et al., 2018) 3.40 Flow (Ho et al., 2019) 3.29 DDPM (L) [#b17](Ho et al., 2020)  3.70 13.51 DDPM (Lsimple) [#b17](Ho et al., 2020)  3.75 3.17   (Song amp Ermon, 2019). 4"(Grathwohl et al., 2018) 3.40 Flow (Ho et al., 2019) 3.29 DDPM (L) [#b17](Ho et al., 2020)  3.70 13.51 DDPM (Lsimple) [#b17](Ho et al., 2020)  3.75 3.17   (Song amp Ermon, 2019) 25.32 8.87 .12 NCSNv2 (Song amp Ermon, 2020) 10.87 8.40 . "bibr" target"b17"(Ho et al., 2020)  3.75 3.17   (Song amp Ermon, 2019) 25.32 8.87 .12 NCSNv2 (Song amp Ermon, 2020) 10.87 8.40 .07 DDPM [#b17](Ho et al., 2020) 3.17 9.46 .11 Exact likelihood computation Leveraging the connection to neural ODEs, we can compute the density defined by Eq. ( ref type"form. ated in the same way (excluding models evaluated with variational dequantization (Ho et al., 2019) or discrete data). Main results (i) For the same DDPM model in [#b17]Ho et al. (2020), we obtain better bitsdim compared to the upper bound given by ELBO, since our likelihoods are exact (ii) Using the same architecture, we traine. framework, there is a performance gap reported in previous papers. The best FID values of SMLD models on CIFAR10 is 10.23 (Song amp Ermon, 2020), whereas for DDPM it is 3.17 [#b17](Ho et al., 2020). With PC samplers and the same model architecture in [#b17]Ho et al. (2020), the gaps can be reduced, but scoreba. AR10 is 10.23 (Song amp Ermon, 2020), whereas for DDPM it is 3.17 [#b17](Ho et al., 2020). With PC samplers and the same model architecture in [#b17]Ho et al. (2020), the gaps can be reduced, but scorebased models trained with the VE SDE still perform slightly worse (see Tab. 1). This raises the question of wh. min tp max  min qqxdt b min tp max  min qdw,(28)where xp0q " p data pxq. In our experiments, we let min " 0.1 and max " 20, which correspond to the settings in [#b17]Ho et al. (2020). The perturbation kernel is given byp 0t pxptq  xp0qq " N xptq e 1 4 t 2 p maxminq1 2 t min xp0q, I Ie. FLOW SAMPLING WITH BLACKBOX ODE SOLVERSFor producing figures in Fig. 4, we use a DDPM model trained on 256 256 CelebAHQ with the same settings in [#b17]Ho et al. (2020). We use the RK45 ODE solver (Dormand amp Prince, 1980) provided by scipy.integrate.solve_ivp in all cases.. latent space manipulation in Fig. 7, including interpolation and temperature scaling. The model tested here is a DDPM model trained with the same settings in [#b17]Ho et al. (2020). D.5 UNIQUELY IDENTIFIABLE ENCODINGAs a sanity check, we train two models (deno. sampling methods (that are based on the discretization strategy in Eq. ( 41)) reverse diffusion samplers.Note that the ancestral sampling of DDPM [#b17](Ho et al., 2020) (Eq. ( 4)) matches its reverse diffusion counterpart when  i  0 for all i (which happens when t. e, the original ancestral sampler of Eq. ( 4) is essentially a different discretization to the same reversetime SDE. This unifies the sampling method in [#b17]Ho et al. (2020) as a numerical solver to the reversetime VP SDE in our continuous framework. F ANCESTRAL. 0  x 1   x N , whereppx i  x i1 q " N px i  x i1 , p 2 i 2 i1 qIq, i " 1, 2, , N.Here we assume  0 " 0 to simplify notations. Following [#b17]Ho et al. (2020), we can computeqpx i1  x i , x 0 q " N xi1   2 i1  2 i x i 1 2 i1  2 i x0 ,  2 i1 p 2 i 2 i1. i z. We can therefore parameterize   px i , iq via  px i , iq " x i p 2 i 2 i1 qs  px i , iq,where s  px i , iq is to estimate z i . As in [#b17]Ho et al. (2020), we let  i "c  2 i1 p 2 i 2 i1 q  2 i . Th. estral sampling method for SMLD models. G ADDITIONAL DETAILS ON PREDICTORCORRECTOR SAMPLERSTraining We use the same architecture in [#b17]Ho et al. (2020) for our scorebased models. For the VE SDE, we train a model with the original SMLD objective in Eq. ( 1. only sampler using 2000 steps) which requires 2000 noise scales, we need to interpolate between 1000 noise scales at test time. The specific architecture of the noiseconditional scorebased model in [#b17]Ho et al. (2020) uses sinusoidal positional embeddings for conditioning on integer time steps. This allows us to interpolate between noise scales at test time in a. on their FID scores averaged over checkpoints after 0.5M iterations. The FIDs are computed on 50k samples with TFGAN. For sampling, we use the PC sampler discretized at 1000 noise scales. We follow [#b17]Ho et al. (2020) for optimization, including the learning rate, gradient clipping, and learning rate warmup schedules. Unless otherwise noted, all models are trai. Unless otherwise noted, all models are trained with the original SMLD objective Eq. ( 1) and use a batch size of 128. Our architecture is mostly based on [#b17]Ho et al. (2020). We additionally search over the following components to explore the potential of scorebased models trained with VE perturbations. Upsamp. al blocks achieves an FID of 2.45 on CIFAR10. Here in order to match the convention used in Karras et al. (2018) Song amp Ermon (2019) [#b17]Ho et al. (2020), the FID value here is the lowest over the course of training, rather than the average over checkpoints after 0.5M iterations (as done in our arch. re, and should be jointly tuned as part of the design choices.To further improve the NCSN model upon conditioning on continuous time variables, we change positional embeddings, the layers in [#b17]Ho et al. (2020) for conditioning on discrete time steps, to random Fourier feature embeddings, as advocated in Tancik et al. (2020) </p>
<p> score-based generative modeling through stochastic differential equations </p><p> correlation functions and computer simulations </p><p> addition, we propose two special methods not viable for general SDEs (i) PredictorCorrector (PC) samplers that combine numerical SDE solvers with scorebased MCMC approaches, such as Langevin MCMC [#b34](Parisi, 1981Grenander amp Miller, 1994) and (ii) deterministic samplers based on the probability flow ordinary differenti. we have additional information that can be used to improve solutions. Since we have a scorebased model s  px, tq   x log p t pxq, we can employ scorebased MCMC approaches, such as Langevin MCMC [#b34](Parisi, 1981Grenander amp Miller, 1994) or HMC (Neal et al., 2011) to sample from p t </p>
<p> score-based generative modeling through stochastic differential equations </p><p> generating diverse high-fidelity images with vq-vae-2 </p><p> CSN on CIFAR10, we proceed to test it on 1024 1024 CelebAHQ (Karras et al., 2018), a task that was previously only achievable by some GAN models and VQVAE2 [#b35](Razavi et al., 2019). We used a batch size of 8, increased the EMA rate to 0.9999, and trained the NCSN model with the continuous objective (Eq. ( ref type"fo </p>
<p> score-based generative modeling through stochastic differential equations </p><p> a stochastic estimator of the trace of the influence matrix for laplacian smoothing splines </p><p> s expensive, so we follow Grathwohl et al. (2018) to estimate it with the SkillingHutchinson trace estimator (Skilling, 1989[#b18]Hutchinson, 1990).In particular, we have f  px, tq " E pp q r T  f px, tq s,(35)where  f </p>
<p> score-based generative modeling through stochastic differential equations </p><p> correlation functions and computer simulations </p><p> addition, we propose two special methods not viable for general SDEs (i) PredictorCorrector (PC) samplers that combine numerical SDE solvers with scorebased MCMC approaches, such as Langevin MCMC [#b34](Parisi, 1981Grenander amp Miller, 1994) and (ii) deterministic samplers based on the probability flow ordinary differenti. we have additional information that can be used to improve solutions. Since we have a scorebased model s  px, tq   x log p t pxq, we can employ scorebased MCMC approaches, such as Langevin MCMC [#b34](Parisi, 1981Grenander amp Miller, 1994) or HMC (Neal et al., 2011) to sample from p t </p>
<p> score-based generative modeling through stochastic differential equations </p><p> a style-based generator architecture for generative adversarial networks </p><p> ref.  Rescaling all skip connections by 1  ? 2. This has been demonstrated effective in several works, including ProgressiveGAN (Karras et al., 2018), StyleGAN [#b21](Karras et al., 2019) and StyleGAN2 (Karras et al., 2020b). Replacing the original residual blocks in DDPM with residual blocks fro. rding to StyleGAN2.We also tested equalized learning rates, a trick used in very successful models like ProgressiveGAN (Karras et al., 2018) and StyleGAN [#b21](Karras et al., 2019). However, we found it harmful at an early stage of our experiments, and therefore decided not to include it in the architecture search.p </p>
<p> score-based generative modeling through stochastic differential equations </p><p> variational walkback: learning a transition operator as a stochastic recurrent net </p><p> refore refer to these two model classes together as scorebased generative models.Scorebased generative models, and related techniques (Bordes et al., 2017[#b13]Goyal et al., 2017), have proven effective at generation of images (Song amp Ermon, 20192020ref type"bibr" targ </p>
<p> score-based generative modeling through stochastic differential equations </p><p> deep unsupervised learning using nonequilibrium thermodynamics </p><p> log probability density) at each noise scale, and then uses Langevin dynamics to sample from a sequence of decreasing noise scales during generation. Denoising diffusion probabilistic modeling (DDPM) [#b39](SohlDickstein et al., 2015Ho et al., 2020) trains a sequence of probabilistic models to reverse each step of the noise corr </p>
<p> score-based generative modeling through stochastic differential equations </p><p> improved techniques for training score-based generative models </p><p>  </p>
<p> score-based generative modeling through stochastic differential equations </p><p> reverse-time diffusion equation models </p><p> on the data and has no trainable parameters. By reversing this process, we can smoothly mold random noise into data for sample generation. Crucially, this reverse process satisfies a reversetime SDE [#b1](Anderson, 1982), which can be derived from the forward SDE given the score of the marginal probability densities as a function of time. We can therefore approximat. gns1.0"GENERATING SAMPLES BY REVERSING THE SDEBy starting from samples of xpT q " p T and reversing the process, we can obtain samples xp0q " p 0 . A remarkable result from [#b1]Anderson (1982) states that the reverse of a diffusion process is also a diffusion process, running backwards in time and given by the reversetime SDEformula. 6"dx " f px, tqdt Gpx, tqdw,(14)where f p, tq  R d  R d and Gp, tq  R d  R dd . We follow the It interpretation of SDEs throughout this paper.According to [#b1](Anderson, 1982), the reversetime SDE is given by (cf ., Eq. ( 6))dx " tf px, tq  rGpx, tqGpx, tq T s Gpx,. th the following general form dx " f px, tqdt Gpx, tqdw, and suppose the initial state distribution is p 0 pxp0q  yq. The density at time t is p t pxptq  yq when conditioned on y. Therefore, using [#b1]Anderson (1982), the reversetime SDE is given by dx " tf px, tq  rGpx, tqGpx, tq T s Gpx, tqGpx, tq To test this idea, we trained a Wide ResNet ref type"bibr </p>
<p> score-based generative modeling through stochastic differential equations </p><p> generating diverse high-fidelity images with vq-vae-2 </p><p> CSN on CIFAR10, we proceed to test it on 1024 1024 CelebAHQ (Karras et al., 2018), a task that was previously only achievable by some GAN models and VQVAE2 [#b35](Razavi et al., 2019). We used a batch size of 8, increased the EMA rate to 0.9999, and trained the NCSN model with the continuous objective (Eq. ( ref type"fo </p>
<p> score-based generative modeling through stochastic differential equations </p><p> improved techniques for training score-based generative models </p><p>  </p>
<p> score-based generative modeling through stochastic differential equations </p><p> generative adversarial nets </p><p> ion abilities to the family of scorebased generative models.While our proposed sampling approaches improve results and enable more efficient sampling, they remain slower at sampling than GANs [#b12](Goodfellow et al., 2014) on the same datasets. Identifying ways of combining the stable learning of scorebased generative models with the fast sampling of implic </p>
<p> score-based generative modeling through stochastic differential equations </p><p> adversarial score matching and improved sampling for image generation </p><p>  </p>
<p> score-based generative modeling through stochastic differential equations </p><p> the eigenvalues of mega-dimensional matrices </p><p> 9"33). In many cases computing  f  px, tq is expensive, so we follow Grathwohl et al. (2018) to estimate it with the SkillingHutchinson trace estimator [#b38](Skilling, 1989Hutchinson, 1990).In particular, we have f  px, tq " E pp q r T  f </p>
<p> score-based generative modeling through stochastic differential equations </p><p> a stochastic estimator of the trace of the influence matrix for laplacian smoothing splines </p><p> s expensive, so we follow Grathwohl et al. (2018) to estimate it with the SkillingHutchinson trace estimator (Skilling, 1989[#b18]Hutchinson, 1990).In particular, we have f  px, tq " E pp q r T  f px, tq s,(35)where  f </p>
<p> score-based generative modeling through stochastic differential equations </p><p> tweedie's formula and selection bias </p><p> oes not use a denoising step at the end of sampling, while the latter does. In all experiments of this paper we ensure there is a single denoising step at the end of sampling, using Tweedie's formula [#b11](Efron, 2011).Adhoc interpolation methods for noise scales Models in this experiment are all trained with 1000 noise scales. To get results for P2000 (pred </p>
<p> score-based generative modeling through stochastic differential equations </p><p> a style-based generator architecture for generative adversarial networks </p><p> ref.  Rescaling all skip connections by 1  ? 2. This has been demonstrated effective in several works, including ProgressiveGAN (Karras et al., 2018), StyleGAN [#b21](Karras et al., 2019) and StyleGAN2 (Karras et al., 2020b). Replacing the original residual blocks in DDPM with residual blocks fro. rding to StyleGAN2.We also tested equalized learning rates, a trick used in very successful models like ProgressiveGAN (Karras et al., 2018) and StyleGAN [#b21](Karras et al., 2019). However, we found it harmful at an early stage of our experiments, and therefore decided not to include it in the architecture search.p </p>
<p> score-based generative modeling through stochastic differential equations </p><p> neural ordinary differential equations </p><p>  </p>
<p> score-based generative modeling through stochastic differential equations </p><p> learning gradient fields for shape generation </p><p> ref, audio (Chen et al., 2020Kong et al., 2020), graphs (Niu et al., 2020), and shapes [#b5](Cai et al., 2020). However, the Work done during an internship at Google Brain. practical performance of these two model classes is often quite different for reas </p>
<p> score-based generative modeling through stochastic differential equations </p><p> adversarial score matching and improved sampling for image generation </p><p>  </p>
<p> toward improving ecg biometric identification using cascaded convolutional neural networks </p><p> human identification using finger vein and ecg signals </p><p> g"en"  IntroductionHuman identification by the uniqueness of every individual is an indispensable part of human life nowadays [#b0][]. However, because of the lack of liveness check, traditional biometric recognition techniques via fingerprint, palm print, </p>
<p> toward improving ecg biometric identification using cascaded convolutional neural networks </p><p> ecg based personal identification using extended kalman filter </p><p> hown in  and much better than those of [[#b62]] by a large margin. However, the dedicated methods presented in Table 9 do not </p>
<p> toward improving ecg biometric identification using cascaded convolutional neural networks </p><p> one-lead ecg-based personal identification using ziv-merhav cross parsing </p><p> "[], amplitude features of various peaks [24] and morphological features [[#b39]]are favored in the literatures. Based on the fiducial points, Safie et al. [13] adopted pulse active ratio as the features </p>
<p> toward improving ecg biometric identification using cascaded convolutional neural networks </p><p> on evaluating ecg biometric systems: session-dependence and body posture </p><p> d length M ? 1 .The proposed method is based on template comparison. Templates of users are needed be generated and stored. To obtain a more representative template for one user as analysed in [#b8][9], several processing operations are performed. Denote x n as a heartbeat segmented from the registered recording and x as the calculated template by N heartbeats </p>
<p> toward improving ecg biometric identification using cascaded convolutional neural networks </p><p> dropout: a simple way to prevent neural networks from overfitting </p><p> 1.(7)For FCNN, overlapped Pooling (w l gt s l ) without zero padding is adopted for keeping more information.(3) Dropout layer for regularization. Dropout layer [#b54][55] is proposed to regularize FCNN for avoiding overfitting. Denote the input of Dropout layer asX l ? R H l ?1?D l ,p </p>
<p> toward improving ecg biometric identification using cascaded convolutional neural networks </p><p> a neural network to identify human subjects with electrocardiogram signals </p><p> . Besides, the NN was designed for a special community. That could not be generalized to others, which is the main weakness as [51].Differently, Wan et al. [#b52][53] proposed to use a BPNN to predict the comparison result of wavelet coefficients from two ECG heartbeats. As the comparison classifier, the trained NN model co </p>
<p> toward improving ecg biometric identification using cascaded convolutional neural networks </p><p> ecg authentication for mobile devices </p><p> metric recognition via electrocardiogram (ECG), which records the electrical depolarizationrepolarization patterns of the heart, has been proposed and studied [[#b14]]. Different from other biometrics, ECG sign </p>
<p> toward improving ecg biometric identification using cascaded convolutional neural networks </p><p> development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs </p><p> ers in different convolutional layers are executed. Recent works on skin cancer diagnosis [31] and detection of diabetic retinopathy in retinal fundus photographs [#b31][32] by CNN have gained huge success. Generally, there are some structural features in ECG signals [3,ref type"bibr" target" </p>
<p> toward improving ecg biometric identification using cascaded convolutional neural networks </p><p> development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs </p><p> ers in different convolutional layers are executed. Recent works on skin cancer diagnosis [31] and detection of diabetic retinopathy in retinal fundus photographs [#b31][32] by CNN have gained huge success. Generally, there are some structural features in ECG signals [3,ref type"bibr" target" </p>
<p> toward improving ecg biometric identification using cascaded convolutional neural networks </p><p> ecg personal identification in subspaces using radial basis neural networks </p><p> bibr" target"b49"[] following the review of [10]. NN based on radial basis function (RBF) have also been studied [#b51][52].Due to the mechanism of neural network, directly learning nonlinear relations from the training samples, we argue it is more effective than traditiona </p>
<p> toward improving ecg biometric identification using cascaded convolutional neural networks </p><p> a comparison of heartbeat detectors for the seismocardiogram </p><p> nt for others to implement and compare with the proposed method, five public datasets FANTASIA [57],CEBSDB [58], NSRDB [#b58][59], STDB [60], and AFDB [61] are used for conducting experiments. These datasets are co </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> learning to diversify search results via subtopic attention </p><p> nderlying an ambiguous or a broad query. In recent years, many search result diversification approaches have been proposed [[#b11][][24]re. type"bibr" target"b25"[26], PAMM [23], and NTN [24]. The explicit approaches [[#b11]] stress the relevance between the docume. type"bibr" target"b17"18] and PM2 [] and supervised approaches such as DSSA [#b11][12].Studies have shown that supervised approaches [#b11][12,23,ref type"bibr" target. bibr" target"b5"6,9] and supervised approaches such as DSSA [#b11][12].Studies have shown that supervised approaches [#b11][] are able to outperform the heuristic a. fication is an effective way to solve the problem of query ambiguity, many models have been proposed to solve this problem [[#b11][][24]re. milarities between documents, explicit approaches regard the query as several subtopics, and explicitly leverage subtopics to determine the diversity of results [[#b11]]. Most explicit approaches focus on the subtopic coverage of results, by calcula. ts not covering the subtopics. PM2 [6] is another unsupervised explicit method calculating the distribution by counting the relevant document of the subtopic. DSSA [#b11][12] introduces the machine learning method into explicit approaches. It calculates the distribution using the RNN and attention mechanism ref type"bibr" target. available documents based on the query  and the selected document ranking  or the candidate document  to fool the discriminator. So it also needs a score function. In our method we adapt the DSSA [#b11][12] score function for the generator. We introduce the score function in the form of Eq. ( 2)  ( . former section. We use the generator as the model to generate the final document ranking result.In the training process, we first train RLTR [26] and DSSA [#b11][12] respectively using MLE loss in both ways. It is because our framework needs a warm start to avoid the deviation in the training process. Then we train them by. "bibr" target"b21"[22], RLTR [26], PAMM [6], RLTRNTN, PAMMNTN [24], and DSSA [#b11][12] as supervised baseline methods. Top 20 results of Lemur are used to train the supervised methods. Top 50( ) results of Lemur are used for diversity rerankin. 1 to 10.DSSA. DSSA is the supervised explicit method. We use LSTM [8] as the RNN cell for comparison. In our experiments, we conduct the listpairwise loss [#b11][12] to train DSSA method. The feature vector 1 httpplaybigdata.ruc.edu.cndouhdiv 2 Lemur service httpboston.lti.cs.cmu.eduServicesclueweb09 batch is </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> irgan: a minimax game for unifying generative and discriminative information retrieval models </p><p> samples, which may cause the model hard to tune. How to generate training samples effectively is still a challenge for training diversification models.To tackle this problem, inspired by IRGAN [#b18][19], we introduce Generative Adversarial Network (GAN) [10] into search result diversification. Generator generates negative t. target"b12"[13] introduces the GAN to the text sequence generation area combined with Monte Carlo search. It is also used in the traditional information retrieval area, Wang proposed IRGAN [#b18][19] which consists of two information retrieval models in it. Comparing to other information retrieval models, IRGAN's generator can provide negative training sam. ability calculated by  ( , )     ( , )  exp(  ( , )) 1  exp(  ( , )) .(7)Please note that different from IRGAN [#b18][19], DVGANdoc has an additional component  to represent the former selected documents.As  contain order information, it is required to be ranked.d. 3.2.2 Optimizing Generator. As GAN is put into practice in the contiguous area firstly, it is difficult to calculate the generator gradient due to its discrete nature. Inspired by IRGAN [#b18][19], we generate negative document set   by selecting the documents from the candidate document set with the highest scores. Formally, the gradient of the gener </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> exploiting query reformulations for web search result diversification </p><p> ersification approaches have been proposed [[#b17][][24][25][26]. ref, and NTN [24]. The explicit approaches [[#b17]] stress the relevance between the documents and the subtopics of the query, which infers that the selected document shou. btopic relevance measures. Similar to implicit approaches, explicit diversification approaches can also be categorized into heuristic approaches such as xQuAD [[#b17]] and PM2 [] and supervised approaches such as D. ype"bibr" target"b23"24,26] are able to outperform the heuristic approaches [[#b17]] by learning an optimized ranking function. However, the large number of candidate documents with only few subtopicrelevant documents in it may cause the probl. s have been proposed to solve this problem [[#b17][][24][25][26]. btopics, and explicitly leverage subtopics to determine the diversity of results [[#b17]]. Most explicit approaches focus on the subtopic coverage of results, by calculating subtopic distribution based on ranked documents. The score function for exp. ontains the documentsubtopic information of previous documents. The  rel function reflects the 's relevance to the query  and the  sub function reflects the 's relevance to the subtopics. xQuAD [#b17][18] is one of the representative methods of unsupervised explicit approaches. It defines the subtopic distribution by calculating the probability of the selected. c.orgns1.0"Baseline ModelsWe compare DVGAN with several existing diversification methods. We use Lemur as our nondiversified baseline method. We use xQuAD, TxQuAD, HxQuAD [#b17][18], PM2 [6], TPM2 [5], and HPM2 [9] as our unsupervis </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> search result diversity evaluation based on intent hierarchies </p><p>  </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> expected reciprocal rank for graded relevance </p><p> ole document ranking in discriminator calculated by Eq. ( 4) using PlackettLuce model and the  is the diversification metrics such as NDCG and ERRIA [#b1][2]. The form of  is inspired by PAMM [23] method which aims to maximize the margin between positive and negative rankings ins. ssumed to be uniform. Evaluation MetricsAmong all the evaluation metrics [  ], we use ERRIA [#b1][2], NDCG [4], and NRBP [3] as our diversity evaluation metrics. They measure the document. hat (1). ideal sampling is helpful for discriminator to distinguish the positive and negative rankings but makes it hard for generator to imitate the real distribution of data because it is too ideal [#b1](2). random sampling makes it easy to imitate for generator but can be confusing for discriminator to distinguish the positive and negative rankings and may provide </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> generative adversarial nets </p><p> is still a challenge for training diversification models.To tackle this problem, inspired by IRGAN [19], we introduce Generative Adversarial Network (GAN) [#b9][10] into search result diversification. Generator generates negative training samples instead of using handcrafted rules for discriminator to train and discriminat. . In this paper, we will have a preliminary study on this. Generative Adversarial NetworkGenerative Adversarial Network(GAN) [#b9][10] is initially used in the area of computer vision to generate pictures that are similar to realistic. There are two models in the Generative Adversarial Network </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> exploiting query reformulations for web search result diversification </p><p> ersification approaches have been proposed [[#b17][][24][25][26]. ref, and NTN [24]. The explicit approaches [[#b17]] stress the relevance between the documents and the subtopics of the query, which infers that the selected document shou. btopic relevance measures. Similar to implicit approaches, explicit diversification approaches can also be categorized into heuristic approaches such as xQuAD [[#b17]] and PM2 [] and supervised approaches such as D. ype"bibr" target"b23"24,26] are able to outperform the heuristic approaches [[#b17]] by learning an optimized ranking function. However, the large number of candidate documents with only few subtopicrelevant documents in it may cause the probl. s have been proposed to solve this problem [[#b17][][24][25][26]. btopics, and explicitly leverage subtopics to determine the diversity of results [[#b17]]. Most explicit approaches focus on the subtopic coverage of results, by calculating subtopic distribution based on ranked documents. The score function for exp. ontains the documentsubtopic information of previous documents. The  rel function reflects the 's relevance to the query  and the  sub function reflects the 's relevance to the subtopics. xQuAD [#b17][18] is one of the representative methods of unsupervised explicit approaches. It defines the subtopic distribution by calculating the probability of the selected. c.orgns1.0"Baseline ModelsWe compare DVGAN with several existing diversification methods. We use Lemur as our nondiversified baseline method. We use xQuAD, TxQuAD, HxQuAD [#b17][18], PM2 [6], TPM2 [5], and HPM2 [9] as our unsupervis </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> psgan: a minimax game for personalized search with limited and noisy click data </p><p> etrieval models in it. Comparing to other information retrieval models, IRGAN's generator can provide negative training samples with higher quality. In the personalized search area, Lu proposed PSGAN [#b14][15] inspired by IRGAN. Our framework is inspired by the former two models. However, we combine the idea of minimax game with the method in diversification search. </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> search result diversification based on hierarchical intents </p><p> uristic approaches such as xQuAD [] and PM2 [[#b8]] and supervised approaches such as DSSA [12].Studies have shown that supervised approaches ref type"bibr" target"b. ic judgment) in this dataset. There are 3 to 8 subtopics for each query. The relevance rating is given at subtopic level. We use google query suggestions as subtopics, which are released by Hu et al. [#b8][9] on their website 1 . we only use the first level subtopics and will adapt the hierarchical structure in future work. The weights of these subtopics are assumed. iversified baseline method. We use xQuAD, TxQuAD, HxQuAD [18], PM2 [6], TPM2 [5], and HPM2 [#b8][9] as our unsupervised baseline methods. We use ListMLE [22], RLTR [26], PAMM ref type </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> evaluating search result diversity using intent hierarchies </p><p>  </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> the use of mmr, diversity-based reranking for reordering documents and producing summaries </p><p> is to generate a ranked list of documents that cover different user intents underlying an ambiguous or a broad query. In recent years, many search result diversification approaches have been proposed [#b0][1,6,12,17,18,re. mation of the query and the selected documents. According to the score function, approaches of diversification can be divided into explicit approaches and implicit approaches. The implicit approaches [#b0][] emphasize the novelty of the documents,. nts, which infers that the selected document should be different from the previously selected documents. The diversification score function of implicit approaches can be handcrafted rules such as MMR [#b0][1] or a supervised measure such as RLTR [26], PAMM [23], and NTN ref type"bibr" target. "bibr" target"b11"[] are able to outperform the heuristic approaches [#b0][] by learning an optimized ranking function. However, the large number of candidate. ATED WORK 2.1 Search Result DiversificationAs search result diversification is an effective way to solve the problem of query ambiguity, many models have been proposed to solve this problem [#b0][1,6,12,17,18,re. implicit approaches emphasize the document's relevance to the query and novelty to the selected documents. In the early years' research on diversification, implicit methods are most unsupervised. MMR [#b0][1] can be regarded as the foundation of implicit methods. Its diversification score function is as follows. ( , )  (1  ) r </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> the use of mmr, diversity-based reranking for reordering documents and producing summaries </p><p> is to generate a ranked list of documents that cover different user intents underlying an ambiguous or a broad query. In recent years, many search result diversification approaches have been proposed [#b0][1,6,12,17,18,re. mation of the query and the selected documents. According to the score function, approaches of diversification can be divided into explicit approaches and implicit approaches. The implicit approaches [#b0][] emphasize the novelty of the documents,. nts, which infers that the selected document should be different from the previously selected documents. The diversification score function of implicit approaches can be handcrafted rules such as MMR [#b0][1] or a supervised measure such as RLTR [26], PAMM [23], and NTN ref type"bibr" target. "bibr" target"b11"[] are able to outperform the heuristic approaches [#b0][] by learning an optimized ranking function. However, the large number of candidate. ATED WORK 2.1 Search Result DiversificationAs search result diversification is an effective way to solve the problem of query ambiguity, many models have been proposed to solve this problem [#b0][1,6,12,17,18,re. implicit approaches emphasize the document's relevance to the query and novelty to the selected documents. In the early years' research on diversification, implicit methods are most unsupervised. MMR [#b0][1] can be regarded as the foundation of implicit methods. Its diversification score function is as follows. ( , )  (1  ) r </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> predicting diverse subsets using structural svms </p><p> ef type"bibr" target"b11"12,17,18,[23][24][#b24][25][26].Most previous methods of diversification can be described as the following procedure. At each iteration, the d. b23"[24]. The explicit approaches [[#b24]] stress the relevance between the documents and the subtopics of the query, which infers that the selected document should cover the subtopics which the previou. ef type"bibr" target"b11"12,17,18,[23][24][#b24][25][26]. Depending on whether the subtopics of query are explicitly modeled and the form of score function, existing diversif </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> psgan: a minimax game for personalized search with limited and noisy click data </p><p> etrieval models in it. Comparing to other information retrieval models, IRGAN's generator can provide negative training samples with higher quality. In the personalized search area, Lu proposed PSGAN [#b14][15] inspired by IRGAN. Our framework is inspired by the former two models. However, we combine the idea of minimax game with the method in diversification search. </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> modeling document novelty with neural tensor network for search result diversification </p><p> 6,12,17,18,[23][#b23][24][25][26].Most previous methods of diversification can be described as the foll. , approaches of diversification can be divided into explicit approaches and implicit approaches. The implicit approaches [[#b23]] emphasize the novelty of the documents, which infers that the selected document should be different from the previously. fted rules such as MMR [1] or a supervised measure such as RLTR [26], PAMM [23], and NTN [#b23][24]. The explicit approaches [6,12,17,ref type"bib. approaches such as DSSA [12].Studies have shown that supervised approaches [[#b23]] are able to outperform the heuristic approaches [1,6,. 6,12,17,18,[23][#b23][24][25][26]. Depending on whether the subtopics of query are explicitly modeled and the. sed PAMM [23] in which loss function is designed to directly maximize the score margin of positive and negative rankings. Furthermore, Neural Tensor Network (NTN) [#b23][24] was introduced into search result diversification to measure document similarity automatically. In our framework, we use the score function of the RLTR in di. pervised baseline methods. We use ListMLE [22], RLTR [26], PAMM [6], RLTRNTN, PAMMNTN [#b23][24], and DSSA [12] as supervised baseline methods. Top 20 results of Lemur are used to train the supervised methods. Top 50( </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> modeling document novelty with neural tensor network for search result diversification </p><p> 6,12,17,18,[23][#b23][24][25][26].Most previous methods of diversification can be described as the foll. , approaches of diversification can be divided into explicit approaches and implicit approaches. The implicit approaches [[#b23]] emphasize the novelty of the documents, which infers that the selected document should be different from the previously. fted rules such as MMR [1] or a supervised measure such as RLTR [26], PAMM [23], and NTN [#b23][24]. The explicit approaches [6,12,17,ref type"bib. approaches such as DSSA [12].Studies have shown that supervised approaches [[#b23]] are able to outperform the heuristic approaches [1,6,. 6,12,17,18,[23][#b23][24][25][26]. Depending on whether the subtopics of query are explicitly modeled and the. sed PAMM [23] in which loss function is designed to directly maximize the score margin of positive and negative rankings. Furthermore, Neural Tensor Network (NTN) [#b23][24] was introduced into search result diversification to measure document similarity automatically. In our framework, we use the score function of the RLTR in di. pervised baseline methods. We use ListMLE [22], RLTR [26], PAMM [6], RLTRNTN, PAMMNTN [#b23][24], and DSSA [12] as supervised baseline methods. Top 20 results of Lemur are used to train the supervised methods. Top 50( </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> distributed representations of sentences and documents </p><p> roduce some feature vectors we used.    Embedding vector for document , which is the distributed representation of document. It can be constructed in different ways, In this paper, we use doc2vec [#b13][14] to get document embeddings. , and  ,  Relevance feature vectors between the document  and query  and subtopic . We adapt some traditional IR </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> learning for search result diversification </p><p> f type"bibr" target"b16"17,18,[23][24][25][#b25][26].Most previous methods of diversification can be described as the following procedure. At each iteration, the document with the highest score graded by. ivided into explicit approaches and implicit approaches. The implicit approaches [[#b25]] emphasize the novelty of the documents, which infers that the selected document should be different from the previously selected documents. The diversification. iously selected documents. The diversification score function of implicit approaches can be handcrafted rules such as MMR [1] or a supervised measure such as RLTR [#b25][26], PAMM [23], and NTN [24]. The explicit approaches [6,. target"b11"[12].Studies have shown that supervised approaches [[#b25]] are able to outperform the heuristic approaches []. upervised diversification model is how to sample enough highquality training data that contain an appropriate number of relevant documents from the candidate document set. Some methods such as RLTR [#b25][26] only use the top documents in the ideal rankings while other methods such as PAMM [23] sample the training rankings by ju. mple the training rankings by judging it through diversification evaluation metrics. However, none of the existing approaches solve this problem completely. The quality of training data used by RLTR [#b25][26] is high but its quantity is too small to train the model which may lead to underfit. The quantity of the training dataset used by PAMM ref type"bibr" target. f type"bibr" target"b16"17,18,[23][24][25][#b25][26]. Depending on whether the subtopics of query are explicitly modeled and the form of score function, existing diversification approaches can be categorized int. implicit methods replace the  rel and  div with more complex function and design loss function to use the machine learning method to improve the performance. The relational learningtorank (RLTR) [#b25][26] replaces the  div score by using the relationship matrix between document  and selected documents . And the loss function is inspired by the learning to ra. rankings from the negative ones. Thus, the discriminator needs a score function to calculate score for document  given the query  and selected document ranking . In our method, we adapt the RLTR [#b25][26] score function for discriminator. We introduce the score function   (  , ) in the form of Eq. ( 1)f. te the diversified search result as we show in the former section. We use the generator as the model to generate the final document ranking result.In the training process, we first train RLTR [#b25][26] and DSSA [12] respectively using MLE loss in both ways. It is because our framework needs a warm start to avoid the devia. , TPM2 [5], and HPM2 [9] as our unsupervised baseline methods. We use ListMLE [22], RLTR [#b25][26], PAMM [6], RLTRNTN, PAMMNTN [24], and DSSA [12]re. ed implicit methods. For the diversity feature, we use the same four features in Table 2 with two more features linkbased diversity and URLbased diversity in [#b25][26], for PAMM, we use nDCG20 as the optimization metrics and tune the number of positive rankings   and negative rankings   per query. We tune the function </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> effective approaches to attention-based neural machine translation </p><p> t of the subtopic. DSSA [12] introduces the machine learning method into explicit approaches. It calculates the distribution using the RNN and attention mechanism [#b15][16]. In our framework, we mainly use the score function of the DSSA in our generator.Discussion. As described above, existing implicit and explicit approac. e part of calculating the relevance between documents and queries or subtopics is easy to understand. As DSSA, we will introduce the distribution of subtopic ( ). DSSA uses both RNN and attention [#b15][16] mechanism to calculate it. Noticed that the selected document ranking  contains order information, it is natural to use RNN to encode the previous document i </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> analyzing and modeling rank data </p><p> e same as the diversification score function   ( , ). In DVGANrank method, we calculate the score function of  (particular ), i.e., D  ( , ) and G  ( , ), using PlackettLuce model [#b6][7]. Specifically, we haveDVGANdoc D  ( , )    ( , ) G  ( , )    ( , ), DVGANrank             </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> search result diversity evaluation based on intent hierarchies </p><p>  </p>
<p> DVGAN: A Minimax Game for Search Result Diversification </p><p> explicit web search result diversification </p><p> In recent years, many search result diversification approaches have been proposed [[#b16][][24][25]r. MM [23], and NTN [24]. The explicit approaches [[#b16]] stress the relevance between the documents and the subtopics of the query, whic. pic distribution measures and documentsubtopic relevance measures. Similar to implicit approaches, explicit diversification approaches can also be categorized into heuristic approaches such as xQuAD [#b16][] and PM2 []. e problem of query ambiguity, many models have been proposed to solve this problem [[#b16][][24][25]r. pproaches regard the query as several subtopics, and explicitly leverage subtopics to determine the diversity of results [[#b16]]. Most explicit approaches focus on the subtopic coverage of results, by calculating subtopic distribution based on rank </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> graph representation learning via graphical mutual information maximization </p><p> [pos is CCS CONCEPTS] nal graph into multiple homogeneous ones and adopts the infomax objective used in DGI for modeling split graphs. So DMGI still puts more emphasis on learning the correlation of homogeneous nodes. GMI [#b26][26] proposes a new approach  The yellow dotted lines (Eq.( 1) and Eq.( 3re </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> bine: bipartite network embedding </p><p> [pos is CCS CONCEPTS] k pretty well in the settings of homogeneous and heterogeneous graphs, most of them are not tailored for modeling bipartite graphs. As a result, they are suboptimal to learn bipartite graph embedding [#b7][]. To remedy such a problem, several studies have been specifically proposed for modeling bipartite graphs. They can be roug. [pos is CCS CONCEPTS] a problem, several studies have been specifically proposed for modeling bipartite graphs. They can be roughly divided into two branches random walkbased and reconstructionbased methods. The former [#b7][] relies on designing the heuristics of random walks to generate different node sequ. [pos is CCS CONCEPTS] thods achieve promising results to some extent, but they mainly focus on learning local graph structures with the assumption that nodes within the sliding window or neighborhoods are closely relevant [#b7][]. We argue that they lack the capability of better modeling the global properties. [pos is CCS CONCEPTS] graphs, and the structural characteristics of bipartite graph are hard to be preserved by them. IGE [44], PinSage [40], BiNE [#b7][7] and FOBE [32] are specially designed for bipartite graphs. However, as mentioned in the introduction, they mainly focus on. [pos is CCS CONCEPTS] e it is used to test whether our model can be deployed to largescale bipartite graphs. Data Preprocessing.As used in BiNE [#b7][7], we select 60 edges for training and remaining edges for test in both of DBLP and ML10M. We use the same division in IGMC [42]. [pos is CCS CONCEPTS] le of MI maximization, and it uses the same infomax objective in DGI [36].  Bipartite graph embedding PinSage [40] and BiNE [#b7][7].PinSage integrates random walk into GNN architectures for highscalable performances. BiNE jointly optimizes explicit and implicit relations in a unified </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> knowledge graph embedding preserving soft logical regularity </p><p> [pos is CCS CONCEPTS] id"formula_17"  (,)  (  , )     (,   )    .(14)The negative sampling used in Eq.( 14) is similar to [#b12][]   (,) is composed of real interactions with either the head or tail replaced by a random node from the same node. [pos is CCS CONCEPTS] e the parameter sensitivity of our model on DBLP with respect to two hyperparameters the corruption rate  in Eq.( 9) and the harmonic factor  in Eq. [#b12](12). As shown in Figure 4, when 1e5 and   0.3, our model achieves the best result. Therefore, choosing relative smal </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> estimation of entropy and mutual information </p><p> [pos is CCS CONCEPTS] mbeddings provides a desirable paradigm for the unsupervised learning [35]. However, estimating MI is generally intractable in highdimensional continuous settings [#b25][25]. MINE [1] derives a lower bound of MI and works by training a discriminator to distinguish samples coming from the joint d </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> representation learning on graphs: methods and applications </p><p> [pos is CCS CONCEPTS] a longstanding challenge. Recently, a significant amount of progresses have been made toward the graph embedding paradigm [[#b14]]. Although they work pretty well in the settings of homogeneous and heterogeneous graphs, most of them are not tailored for modeling bipartite graphs. As a resu </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> exploratory adversarial attacks on graph neural networks </p><p> [pos is CCS CONCEPTS] ref40,42] train graph neural networks (GNNs) [[#b22]] to learn node representations via aggregating features of neighborhood nodes recursively. The orange shaded area repres </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> a comprehensive survey of graph embedding: problems, techniques, and applications </p><p> [pos is CCS CONCEPTS] type.Learning meaningful node representations for bipartite graphs is a longstanding challenge. Recently, a significant amount of progresses have been made toward the graph embedding paradigm [#b2][]. Although they work pretty well in the settings of homogeneous and heterogeneous g </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> auto-encoding variational bayes </p><p> [pos is CCS CONCEPTS] re typically randomwalk based. LINE learns a joint probability distribution of connected nodes, and LINE (2nd) is exploited here due to its expressive performances. Based on variational autoencoder [#b18][18], VGAE adopts the graph convolutional network (GCN) [20] as the basic encoder to learn graphstructured data.  Model F110 </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> item-based collaborative filtering recommendation algorithms </p><p> [pos is CCS CONCEPTS] "bibr" target"b34"34,37,40,42] are closely related with collaborative filtering [#b28][28]. They attempt to reconstruct the adjacency matrix by learning different encoders. In particular, some works [34,ref type </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> deepwalk: online learning of social representations </p><p> [pos is CCS CONCEPTS] ED WORK 2.1 Bipartite Graph EmbeddingHomogeneous and heterogeneous graph embeddings are usually used for modeling bipartite graphs. The pioneering homogeneous graph methods include DeepWalk [#b27][27], LINE [33], Node2vec [11] and VGAE [19]. Some r. [pos is CCS CONCEPTS] www.teic.orgns1.0"ComparedBaselines. We compare our model with the following strong baselines which can be divided into Homogeneous graph embedding DeepWalk [#b27][27], LINE [33], Node2vec [11] and VGAE [19]. DeepWa </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> graph convolutional neural networks for web-scale recommender systems </p><p> [pos is CCS CONCEPTS] . The reconstructionbased works [[#b40]] are closely related with collaborative filtering [28]. They attempt to reconstruct. [pos is CCS CONCEPTS] [28]. They attempt to reconstruct the adjacency matrix by learning different encoders. In particular, some works [[#b40]] train graph neural networks (GNNs) [9,20,ref. [pos is CCS CONCEPTS] ]. But they are not tailored for bipartite graphs, and the structural characteristics of bipartite graph are hard to be preserved by them. IGE [44], PinSage [#b40][40], BiNE [7] and FOBE [32] are specially designed for bipartite graphs. However, as ment. [pos is CCS CONCEPTS] inciple of GNN to learn the initial node representations. The proposed encoder is well compatible with our infomax objective. Compared with other GNN encoders [[#b40]] for bipartite graphs, it achieves promising performances empirically. Different from homogeneous graphs, each node in bipartite graph is not the same type as i. [pos is CCS CONCEPTS] " target"b41"[41] also follows the principle of MI maximization, and it uses the same infomax objective in DGI [36].  Bipartite graph embedding PinSage [#b40][40] and BiNE [7].PinSage integrates random walk into GNN architectures for highscalable performances. BiNE jointly opt </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> dropout: a simple way to prevent neural networks from overfitting </p><p> [pos is CCS CONCEPTS] el(4)   ,    and    in Eq.( 3) and Eq.( 4) are also weight matrices. Dropout [#b30][30] is applied to each layer of our encoder to regularize model parameters. LocalGlobal Infomax </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> mutual information neural estimation </p><p> [pos is CCS CONCEPTS] unsupervised learning [35]. However, estimating MI is generally intractable in highdimensional continuous settings [25]. MINE [#b1][1] derives a lower bound of MI and works by training a discriminator to distinguish samples coming from the joint distribution of two random variables or the produ </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> learning deep representations by mutual information estimation and maximization </p><p> [pos is CCS CONCEPTS] 1"[1] derives a lower bound of MI and works by training a discriminator to distinguish samples coming from the joint distribution of two random variables or the product of their marginals. DIM [#b16][16] introduces the structural information into input patches and adopts different infomax objectives.DGI [36] is the f </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> node2vec: scalable feature learning for networks </p><p> [pos is CCS CONCEPTS] ually used for modeling bipartite graphs. The pioneering homogeneous graph methods include DeepWalk [27], LINE [33], Node2vec [#b11][11] and VGAE [19]. Some representative heterogeneous graph methods are Metapath2vec [6] a. [pos is CCS CONCEPTS] nto the global representation via the generated prototype representations, and these two prototypes are not entangled together. Through Eq.( 10) and Eq. [#b11](11), each node has access to the homogeneous prototype and to the heterogeneous prototype simultaneously, which enables our model to break the limit of local grap. [pos is CCS CONCEPTS] following strong baselines which can be divided into Homogeneous graph embedding DeepWalk [27], LINE [33], Node2vec [#b11][11] and VGAE [19]. DeepWalk and Node2vec are typically randomwalk based. LINE learns a joint probability distribution of con </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> a survey on network embedding </p><p> [pos is CCS CONCEPTS] presentations for bipartite graphs is a longstanding challenge. Recently, a significant amount of progresses have been made toward the graph embedding paradigm [[#b5]]. Although they work pretty well in the settings of homogeneous and heterogeneous graphs, most of them are not tailored fo. [pos is CCS CONCEPTS] label8)The local attentive representation also combines different local environments together via the same composition function used in Eq. [#b5](5). It not only highlights the central role of (, ), but also adaptively assigns different importance factors to neighboring nodes by the subgraphlevel attentio </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> a survey on network embedding </p><p> [pos is CCS CONCEPTS] presentations for bipartite graphs is a longstanding challenge. Recently, a significant amount of progresses have been made toward the graph embedding paradigm [[#b5]]. Although they work pretty well in the settings of homogeneous and heterogeneous graphs, most of them are not tailored fo. [pos is CCS CONCEPTS] label8)The local attentive representation also combines different local environments together via the same composition function used in Eq. [#b5](5). It not only highlights the central role of (, ), but also adaptively assigns different importance factors to neighboring nodes by the subgraphlevel attentio </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> arbitrary-order proximity preserved network embedding </p><p> [pos is CCS CONCEPTS] 44"44] relies on designing the heuristics of random walks to generate different node sequences. Afterwards, they learn node representations via predicting context nodes within a sliding window [#b45][45]. The reconstructionbased works [15,32,34,ref </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> neural message passing for quantum chemistry </p><p> [pos is CCS CONCEPTS] ks [] train graph neural networks (GNNs) [#b9][] to learn node representations via aggreg. [pos is CCS CONCEPTS] )where  denotes the LeakyReLU activation function,    is a weight matrix and N (  ) denotes onehop neighbors of   . In contrast with common graph convolutional operators [#b9][], we only aggregate neighborhood features, and the own feature  1  is not inv </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> a survey on network embedding </p><p> [pos is CCS CONCEPTS] presentations for bipartite graphs is a longstanding challenge. Recently, a significant amount of progresses have been made toward the graph embedding paradigm [[#b5]]. Although they work pretty well in the settings of homogeneous and heterogeneous graphs, most of them are not tailored fo. [pos is CCS CONCEPTS] label8)The local attentive representation also combines different local environments together via the same composition function used in Eq. [#b5](5). It not only highlights the central role of (, ), but also adaptively assigns different importance factors to neighboring nodes by the subgraphlevel attentio </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> dropout: a simple way to prevent neural networks from overfitting </p><p> [pos is CCS CONCEPTS] el(4)   ,    and    in Eq.( 3) and Eq.( 4) are also weight matrices. Dropout [#b30][30] is applied to each layer of our encoder to regularize model parameters. LocalGlobal Infomax </p>
<p> Bipartite Graph Embedding via Mutual Information Maximization </p><p> metapath2vec: scalable representation learning for heterogeneous networks </p><p> [pos is CCS CONCEPTS] bibr" target"b33"[33], Node2vec [11] and VGAE [19]. Some representative heterogeneous graph methods are Metapath2vec [#b6][6] and DMGI [41]. But they are not tailored for bipartite graphs, and the structural characteristics of bipartite graph are ha </p>
<p> towards debiasing sentence representations </p><p> deep contextualized word representations </p><p> ) and multiclass (Manzini et al., 2019) bias attributes such as gender, race, and religion.More recently, sentencelevel representations such as ELMo [#b31](Peters et al., 2018), BERT (Devlin et al., 2019), and GPT (Radford et al., 2019) have be. n estimating bias subspaces of sentence representations. Our experiments are performed on two widely popular sentence encoders BERT (Devlin et al., 2019) and ELMo [#b31](Peters et al., 2018), showing that our approach reduces the bias while preserving performance on downstream sequence tasks. We end with a discussion about possibl. t describe the details of applying SENTDEBIAS on two widelyused sentence encoders BERT2  (Devlin et al., 2019) and ELMo [#b31](Peters et al., 2018). Note that the pretrained BERT encoder must be finetuned on taskspecific data. This implies that the final BERT encoder used during debias </p>
<p> towards debiasing sentence representations </p><p> measuring bias in contextualized word representations </p><p> r underlying word embeddings and retraining which is costly. Bordia and Bowman (2019) only study wordlevel language models and also requires retraining. Finally, [#b16]Kurita et al. (2019) only measure bias on BERT by extending the wordlevel Word Embedding Association Test (WEAT) (Caliskan et al., 2 </p>
<p> towards debiasing sentence representations </p><p> identifying and reducing gender bias in word-level language models </p><p> andGarg et al. (2019) are not able to perform posthoc debiasing and require changing the data or underlying word embeddings and retraining which is costly. [#b7]Bordia and Bowman (2019) only study wordlevel language models and also requires retraining. Finally, Kurita et al. (2019) onl </p>
<p> towards debiasing sentence representations </p><p> efficient estimation of word representations in vector space </p><p> bedding layers which learn continuous representations of input information such as words, sentences, and documents from large amounts of data (Devlin et al., 2019[#b27]Mikolov et al., 2013). Although wordlevel embeddings (Pennington et al., 2014[#b27]Mikolov et al.,. bibr" target"b10"(Devlin et al., 2019[#b27]Mikolov et al., 2013). Although wordlevel embeddings (Pennington et al., 2014[#b27]Mikolov et al., 2013) are highly informative features useful for a variety of tasks in Natural Language Processing (NLP), recent work has shown that wordlevel emb. y of the stateoftheart sentencebased embedding models. In contrast with conventional wordlevel embeddings such as GloVe (Pennington et al., 2014) and word2vec [#b27](Mikolov et al., 2013) which can be retrained on a single machine within a few hours, the best sentence encoders such as BERT (Devli </p>
<p> towards debiasing sentence representations </p><p> recursive deep models for semantic compositionality over a sentiment treebank </p><p> 17a), a dataset of formally written Wikipedia articles (we only use the first 10 of WikiText2 which we found to be sufficient to capture formally written text), 2) Stanford Sentiment Treebank [#b35](Socher et al., 2013), a collection of 10000 polarized written movie reviews, 3) Reddit data collected from discussion forums related to politics, electronics, and. specific task before the final BERT representations are debiased. We apply SENTDEBIAS on BERT finetuned on two single sentence datasets, Stanford Sentiment Treebank (SST2) sentiment classification [#b35](Socher et al., 2013) and Corpus of Linguistic Acceptability (CoLA) grammatical acceptability judgment (Warstadt et al., 2018). ggingface. cobertbertbaseuncased.tar.gz.The variant BERT post SST is BERT after being finetuned on the Stanford Sentiment Treebank(SST2) task, a binary singlesentence classification task [#b35](Socher et al., 2013). During finetuning, we first normalize the sentence embedding and then feed it into a linear layer for classification. The variant BERT post </p>
<p> towards debiasing sentence representations </p><p> black is to criminal as caucasian is to police: detecting and removing multiclass bias in word embeddings </p><p> ers working on fairness and ethics in NLP have devised methods towards debiasing these word representations for both binary (Bolukbasi et al., 2016) and multiclass [#b23](Manzini et al., 2019) bias attributes such as gender, race, and religion.More recently, sentencelevel representations such as ELMo ref type"bibr" target. oc debiasing techniques which add a posttraining debiasing step to these sentence representations before they are used in downstream tasks (Bolukbasi et al., 2016[#b23]Manzini et al., 2019). Secondly, sentences display large variety in how they are composed from individual words. This variety is driven by many factors such as top. pplied for fixedlength, pretrained sentence representations. To measure bias over multiple classes, we use the Mean Average Cosine similarity (MAC) metric which extends SEAT to a multiclass setting [#b23](Manzini et al., 2019). For the binary gender setting, we use words from the Caliskan Tests (Caliskan et al., 2017) which measu. Greenwald et al., 2009). To evaluate biases in the multiclass religion setting, we modify the Caliskan Tests used in May et al. (2019) with lexicons used by [#b23]Manzini et al. (2019). Debiasing SetupWe first describe the details of applying SENTDEB. 34"(Rajpurkar et al., 2016) into a binary classification task. These results are   2017) row N . The last row measures bias in a multiclass religion setting using MAC [#b23](Manzini et al., 2019) before and after debiasing. MAC score ranges from 0 to 2 and closer to 1 represents lower bias. Results are reported as x 1 ? x 2 where x 1 </p>
<p> towards debiasing sentence representations </p><p> law and word order: nlp in legal tech </p><p> Machine learning tools for learning from language are increasingly deployed in realworld scenarios such as healthcare (Velupillai et al., 2018), legal systems [#b9](Dale, 2019), and computational social science (Bamman et al., 2016). Key to the success of these models are powerful embedding </p>
<p> towards debiasing sentence representations </p><p> examining gender and race bias in two hundred sentiment analysis systems </p><p> ch between gendered pronouns and the sentence context. For SST, it has been shown that sentiment analysis datasets have labels that correlate with gender information and therefore contain gender bias [#b15](Kiritchenko and Mohammad, 2018). As a result, we do expect possible decreases in accuracy after debiasing. Finally, we test the effect of SENTDEBIAS on QNLI by t </p>
<p> towards debiasing sentence representations </p><p> recursive deep models for semantic compositionality over a sentiment treebank </p><p> 17a), a dataset of formally written Wikipedia articles (we only use the first 10 of WikiText2 which we found to be sufficient to capture formally written text), 2) Stanford Sentiment Treebank [#b35](Socher et al., 2013), a collection of 10000 polarized written movie reviews, 3) Reddit data collected from discussion forums related to politics, electronics, and. specific task before the final BERT representations are debiased. We apply SENTDEBIAS on BERT finetuned on two single sentence datasets, Stanford Sentiment Treebank (SST2) sentiment classification [#b35](Socher et al., 2013) and Corpus of Linguistic Acceptability (CoLA) grammatical acceptability judgment (Warstadt et al., 2018). ggingface. cobertbertbaseuncased.tar.gz.The variant BERT post SST is BERT after being finetuned on the Stanford Sentiment Treebank(SST2) task, a binary singlesentence classification task [#b35](Socher et al., 2013). During finetuning, we first normalize the sentence embedding and then feed it into a linear layer for classification. The variant BERT post </p>
<p> towards debiasing sentence representations </p><p> gender as a variable in naturallanguage processing: ethical considerations </p><p>  </p>
<p> towards debiasing sentence representations </p><p> gender bias in contextualized word embeddings </p><p> b24"(May et al., 2019Basta et al., 2019), none of them have been able to successfully remove bias from pretrained sentence representations. In particular, [#b47]Zhao et al. (2019), Park et al. (2018), andGarg et al. (2019) are not able to perform pos </p>
<p> towards debiasing sentence representations </p><p> man is to computer programmer as woman is to homemaker? debiasing word embeddings </p><p> in training corpora (Lauscher and Glava?, 2019Caliskan et al., 2017Swinger et al., 2019[#b6]Bolukbasi et al., 2016). Machine learning systems that incorporate these word embeddings can further amplify biases (Sun et al., 2019bref t. , particularly those from disadvantaged social groups. Fortunately, researchers working on fairness and ethics in NLP have devised methods towards debiasing these word representations for both binary [#b6](Bolukbasi et al., 2016) and multiclass (Manzini et al., 2019) bias attributes such as gender, race, and religion.More r. of bias is uncovered from data. We therefore focus on posthoc debiasing techniques which add a posttraining debiasing step to these sentence representations before they are used in downstream tasks [#b6](Bolukbasi et al., 2016Manzini et al., 2019). Secondly, sentences display large variety in how they are composed from individu. ttings, and even differences between spoken and written text. As a result, it is difficult to scale traditional wordlevel debiasing approaches (which involve biasattribute words such as man, woman) [#b6](Bolukbasi et al., 2016) to sentences.Related Work Although there has been some recent work in measuring the presence of bias in sentence representations r. contextualizing biasattribute words using a diverse set of sentence templates from various text corpora into biasattribute sentences. We propose SENTDEBIAS, an extension of the HARDDEBIAS method [#b6](Bolukbasi et al., 2016), to debias sentences for both binary1 and multiclass bias attributes spanning gender and religion.. airs and triplets in appendix).Existing methods that investigate biases tend to operate at the wordlevel which simplifies the problem since the set of tokens is bounded by the vocabulary size [#b6](Bolukbasi et al., 2016). This simple approach has the advantage of identifying the presence of biases using predefined sets of word associations, and estimate the. , V represents the topk orthogonal directions which most represent the bias subspace.4) Debiasing Given the estimated bias subspace V, we apply a partial version of the HARDDEBIAS algorithm [#b6](Bolukbasi et al., 2016) to remove bias from new sentence representations. Taking the example of binary gender bias, the HARDDEBIAS algorithm consists of two steps. 1) FastText derives debiased sentence embeddings using an average of debiased FastText word embeddings (Bojanowski et al., 2016) using wordlevel debiasing methods [#b6](Bolukbasi et al., 2016), 2) Debiasing MethodAve. Abs. Effect Size BERT original ref type"bibr". Ave. Abs. Effect Size BERT original (Devlin et al., 2019) 0.354 FastText (Bojanowski et al., 2016) 0.565 BERT word [#b6](Bolukbasi et al., 2016) 0.861 BERT simple (May et al., 2019) 0.298 SENTDEBIAS BERT (ours) 0.256Table ref type"tab. r" target"b5"(Bojanowski et al., 2016) (and BERT word) derives debiased sentence embeddings with an average of debiased FastText (and BERT) word embeddings using wordlevel debiasing methods [#b6](Bolukbasi et al., 2016). BERT simple adapts May et al. (2019) by using simple templates to debias BERT representations. SENTD. scores closer to 0 represent lower bias.BERT word obtains a debiased sentence representation from average debiased BERT word representations, again debiased using wordlevel debiasing methods [#b6](Bolukbasi et al., 2016), and 3) BERT simple adapts May et al. (2019) by using simple templates to debias BERT sentence represe </p>
<p> towards debiasing sentence representations </p><p> using clinical natural language processing for health outcomes research: overview and actionable suggestions for future advances </p><p> body IntroductionMachine learning tools for learning from language are increasingly deployed in realworld scenarios such as healthcare [#b40](Velupillai et al., 2018), legal systems (Dale, 2019), and computational social science (Bamman e </p>
<p> towards debiasing sentence representations </p><p> mitigating gender bias in natural language processing: literature review </p><p>  </p>
<p> towards debiasing sentence representations </p><p> beto, bentz, becas: the surprising cross-lingual effectiveness of bert </p><p> rd et al., 2019) have become the preferred choice for text sequence encoding. When compared to wordlevel representations, these models have achieved better performance on multiple tasks in NLP [#b44](Wu and Dredze, 2019), multimodal learning (Zellers et al., 2019Sun et al., 2019a), and grounded langu </p>
<p> towards debiasing sentence representations </p><p> publicly available clinical bert embeddings </p><p> guage learning (Urbanek et al., 2019). As their usage proliferates across various realworld applications (Huang et al., 2019[#b1]Alsentzer et al., 2019), it becomes necessary to recognize the role they play in shaping social biases and stereotypes.Debiasing sentence representations is </p>
<p> towards debiasing sentence representations </p><p> pointer sentinel mixture models </p><p>  </p>
<p> towards debiasing sentence representations </p><p> gender as a variable in naturallanguage processing: ethical considerations </p><p>  </p>
<p> towards debiasing sentence representations </p><p> pointer sentinel mixture models </p><p>  </p>
<p> towards debiasing sentence representations </p><p> mitigating gender bias in natural language processing: literature review </p><p>  </p>
<p> towards debiasing sentence representations </p><p> gender bias in contextualized word embeddings </p><p> b24"(May et al., 2019Basta et al., 2019), none of them have been able to successfully remove bias from pretrained sentence representations. In particular, [#b47]Zhao et al. (2019), Park et al. (2018), andGarg et al. (2019) are not able to perform pos </p>
<p> towards debiasing sentence representations </p><p> examining gender bias in languages with grammatical gender </p><p> downstream tasks. All experiments are conducted on English terms and downstream tasks. We acknowledge that biases can manifest differently across different languages, in particular gendered languages [#b50](Zhou et al., 2019), and emphasize the need for future extensions in these directions. Experimental details are in the appendix and code is released at httpsgit </p>
<p> towards debiasing sentence representations </p><p> recursive deep models for semantic compositionality over a sentiment treebank </p><p> 17a), a dataset of formally written Wikipedia articles (we only use the first 10 of WikiText2 which we found to be sufficient to capture formally written text), 2) Stanford Sentiment Treebank [#b35](Socher et al., 2013), a collection of 10000 polarized written movie reviews, 3) Reddit data collected from discussion forums related to politics, electronics, and. specific task before the final BERT representations are debiased. We apply SENTDEBIAS on BERT finetuned on two single sentence datasets, Stanford Sentiment Treebank (SST2) sentiment classification [#b35](Socher et al., 2013) and Corpus of Linguistic Acceptability (CoLA) grammatical acceptability judgment (Warstadt et al., 2018). ggingface. cobertbertbaseuncased.tar.gz.The variant BERT post SST is BERT after being finetuned on the Stanford Sentiment Treebank(SST2) task, a binary singlesentence classification task [#b35](Socher et al., 2013). During finetuning, we first normalize the sentence embedding and then feed it into a linear layer for classification. The variant BERT post </p>
<p> towards debiasing sentence representations </p><p> evaluating the underlying gender bias in contextualized word embeddings </p><p> ) to sentences.Related Work Although there has been some recent work in measuring the presence of bias in sentence representations (May et al., 2019[#b4]Basta et al., 2019), none of them have been able to successfully remove bias from pretrained sentence representations. In particular, ref type"bibr" target"b47" </p>
<p> towards debiasing sentence representations </p><p> what are the biases in my word embedding? </p><p> el embeddings reflect and propagate social biases present in training corpora (Lauscher and Glava?, 2019Caliskan et al., 2017[#b38]Swinger et al., 2019Bolukbasi et al., 2016). Machine learning systems that incorporate these word embeddings can further ampli </p>
<p> towards debiasing sentence representations </p><p> learning gender-neutral word embeddings </p><p> xplicit bias control mechanisms on large amounts of naturally occurring text. Given that it becomes infeasible (in standard settings) to completely retrain these large sentence encoders for debiasing [#b49](Zhao et al., 2018Zhang et al., 2018), future work should focus on developing better posthoc debiasing techniques. In our ex </p>
<p> towards debiasing sentence representations </p><p> black is to criminal as caucasian is to police: detecting and removing multiclass bias in word embeddings </p><p> ers working on fairness and ethics in NLP have devised methods towards debiasing these word representations for both binary (Bolukbasi et al., 2016) and multiclass [#b23](Manzini et al., 2019) bias attributes such as gender, race, and religion.More recently, sentencelevel representations such as ELMo ref type"bibr" target. oc debiasing techniques which add a posttraining debiasing step to these sentence representations before they are used in downstream tasks (Bolukbasi et al., 2016[#b23]Manzini et al., 2019). Secondly, sentences display large variety in how they are composed from individual words. This variety is driven by many factors such as top. pplied for fixedlength, pretrained sentence representations. To measure bias over multiple classes, we use the Mean Average Cosine similarity (MAC) metric which extends SEAT to a multiclass setting [#b23](Manzini et al., 2019). For the binary gender setting, we use words from the Caliskan Tests (Caliskan et al., 2017) which measu. Greenwald et al., 2009). To evaluate biases in the multiclass religion setting, we modify the Caliskan Tests used in May et al. (2019) with lexicons used by [#b23]Manzini et al. (2019). Debiasing SetupWe first describe the details of applying SENTDEB. 34"(Rajpurkar et al., 2016) into a binary classification task. These results are   2017) row N . The last row measures bias in a multiclass religion setting using MAC [#b23](Manzini et al., 2019) before and after debiasing. MAC score ranges from 0 to 2 and closer to 1 represents lower bias. Results are reported as x 1 ? x 2 where x 1 </p>
<p> towards debiasing sentence representations </p><p> examining gender and race bias in two hundred sentiment analysis systems </p><p> ch between gendered pronouns and the sentence context. For SST, it has been shown that sentiment analysis datasets have labels that correlate with gender information and therefore contain gender bias [#b15](Kiritchenko and Mohammad, 2018). As a result, we do expect possible decreases in accuracy after debiasing. Finally, we test the effect of SENTDEBIAS on QNLI by t </p>
<p> towards debiasing sentence representations </p><p> visualizing data using t-sne </p><p> ift after the debiasing process is performed. We average the sentence representations of a concept (e.g. man, woman, science, art) across its contexts (sentence templates) and plot the tSNE (van der [#b22]Maaten and Hinton, 2008) Figure 2 Influence of the number of template domains on the effectiveness of bias removal on BERT finetuned on </p>
<p> towards debiasing sentence representations </p><p> pointer sentinel mixture models </p><p>  </p>
<p> towards debiasing sentence representations </p><p> men also like shopping: reducing gender bias amplification using corpus-level constraints </p><p> Bolukbasi et al., 2016). Machine learning systems that incorporate these word embeddings can further amplify biases (Sun et al., 2019b[#b48]Zhao et al., 2017Barocas and Selbst, 2016) and unfairly discriminate against users, particularly those from disadvantaged soci </p>
<p> towards debiasing sentence representations </p><p> strong and simple baselines for multimodal utterance embeddings </p><p> refLiu et al., 2019Wang et al., 2019) and multimodal human language (Liang et al., 2018[#b19](Liang et al., , 2019)). Table 2 summarizes these datasets. We also give some examples of the diverse templates that occur </p>
<p> towards debiasing sentence representations </p><p> law and word order: nlp in legal tech </p><p> Machine learning tools for learning from language are increasingly deployed in realworld scenarios such as healthcare (Velupillai et al., 2018), legal systems [#b9](Dale, 2019), and computational social science (Bamman et al., 2016). Key to the success of these models are powerful embedding </p>
<p> Unsupervised Learning of Visual Features by Contrasting Cluster Assignments </p><p> unsupervised feature learning via non-parametric instance discrimination </p><p> implementations approximate the loss by reducing the number of comparisons to random subsets of images during training [[#b55]]. An alternative to approximate the loss is to approximate the taskthat is to relax the instance discrimination problem. For example, clusteringbased methods. and the codes are learned online, allowing our method to scale to potentially unlimited amounts of data. In addition, SwAV works with small and large batch sizes and does not need a large memory bank [#b55][56] or a momentum encoder [24].Besides our online clusteringbased method, we also propose an improvement to the image. dInstance and contrastive learning. Instancelevel classification considers each image in a dataset as its own class [[#b55]]. Dosovitskiy et al. [16] assign a class explicitly to each image and learn a linear classifier with as many classes as ima. " target"b15"[16] assign a class explicitly to each image and learn a linear classifier with as many classes as images in the dataset. As this approach becomes quickly intractable, Wu et al. [#b55][56] mitigate this issue by replacing the classifier with a memory bank that stores previouslycomputed representations. They rely on noise contrastive estimation. pIn this section, we describe an alternative where we enforce consistency between codes from different augmentations of the same image. This solution is inspired by contrastive instance learning [#b55][56] as we do not consider the codes as a target, but only enforce consistent mapping between views of the same image. Our method can be interpreted as a way of co. two features capture the same information, it should be possible to predict the code from the other feature. A similar comparison appears in contrastive learning where features are compared directly [#b55][56]. In Fig. 1, we illustrate the relation between contrastive learning and our method. div xmlns"httpwww.. rmulat , where p(k) t  exp 1  z t c k k exp 1  z t c k . (2)where  is a temperature parameter [#b55][56]. Taking this loss over all the images and pairs of data augmentations leads to the following loss function for the swapped prediction problemformula xml. rmance compared to training from scratch. B.6 Image classification with KNN classifiers on ImageNetFollowing previous work protocols [#b55][], we evaluate the quality of our unsupervised features with Knearest neighbor (KNN) classifiers on ImageNet. We get fe. speed up training, we choose to use the features computed during the previous epoch instead of dedicating pass forwards to the assignments. This is similar to the memory bank introduced by Wu et al. [#b55][56], without momentum.Assignment phase in DeepClusterv2. DeepClusterv2 uses spherical kmeans to get pseudolabels. In particular, pseudolabels q are obt </p>
<p> Unsupervised Learning of Visual Features by Contrasting Cluster Assignments </p><p> cliquecnn: deep unsupervised exemplar learning </p><p> d along with the ConvNet parameters by backpropragation.Clustering for deep representation learning. Our work is also related to clusteringbased methods [2,[#b3]4,7,8,19,29,ref t </p>
<p> Unsupervised Learning of Visual Features by Contrasting Cluster Assignments </p><p> exploring the limits of weakly supervised pretraining </p><p> results from He et al. [24] but note that their setting is different. They use a curated set of Instagram images, filtered by hashtags similar to ImageNet labels [#b38][39]. We compare SwAV with a randomly initialized network and with a network pretrained on the same data using SimCLR. We observe that SwAV maintains a similar gai. f (right), we explore the limits of pretraining as we increase the model capacity. We consider the variants of the ResNeXt architecture [59] as in Mahajan et al. [#b38][39]. We compare SwAV with supervised models trained from scratch on ImageNet. For all models, SwAV outperforms training from scratch by a significant margin showi. dels, SwAV outperforms training from scratch by a significant margin showing that it can take advantage of the increased model capacity. For reference, we also include the results from Mahajan et al. [#b38][39] obtained with a weaklysupervised model pretrained by predicting hashtags filtered to be similar to ImageNet classes. Interestingly, SwAV performance is stron </p>
<p> Unsupervised Learning of Visual Features by Contrasting Cluster Assignments </p><p> unsupervised representation learning by predicting image rotations </p><p>  </p>
<p> Unsupervised Learning of Visual Features by Contrasting Cluster Assignments </p><p> the inaturalist species classification and detection dataset </p><p> tectures, our method shrinks the gap with supervised training to 0.6. on the Places205 [65], VOC07 [17], and iNaturalist2018 [#b51][52] datasets. Our method outperforms supervised features on all three datasets. Note that SwAV is the first selfsupervised method to surpass ImageNet supervised </p>
<p> Unsupervised Learning of Visual Features by Contrasting Cluster Assignments </p><p> fixing the train-test resolution discrepancy </p><p> uring training. We also observe that mapping small parts of a scene to more global views significantly boosts the performance. Directly working with downsized images introduces a bias in the features [#b50][51], which can be avoided by using a mix of different sizes. Our strategy is simple, yet effective, and can be applied to many selfsupervised methods with consis </p>
<p> Unsupervised Learning of Visual Features by Contrasting Cluster Assignments </p><p> microsoft coco: common objects in context </p><p> o surpass ImageNet supervised features on these datasets. Second, we report network finetuning on object detection on VOC0712 using Faster RCNN [48] and on COCO [#b35][36] with DETR [6]. DETR is a recent object detection framework that reaches competitive performance with Faster RCNN while be. ed over 5 independant runs.Object Detection on COCO. We test the generalization of our ResNet50 features trained on ImageNet with SwAV by transferring them to object detection on COCO dataset [#b35][36] with DETR framework [6]. DETR is a recent object detection framework that relies on a transformer encoderdecoder architect. by replacing the ImageNet supervised network with our model. (2) Object detection with finetuned features on VOC0712 trainval using Faster RCNN [48] and on COCO [#b35][36] using DETR [6]. In this  B.4 More detection metrics for object de. p1 accuracy on all datasets except VOC07 where we report mAP. (2) Object detection with finetuned features on VOC0712 trainval using Faster RCNN[48] and on COCO[#b35][36] using DETR[6]. We report the most standard detection metrics for these datasets AP 50 on VOC0712 and AP on COCO.figDes </p>
<p> Unsupervised Learning of Visual Features by Contrasting Cluster Assignments </p><p> fixing the train-test resolution discrepancy </p><p> uring training. We also observe that mapping small parts of a scene to more global views significantly boosts the performance. Directly working with downsized images introduces a bias in the features [#b50][51], which can be avoided by using a mix of different sizes. Our strategy is simple, yet effective, and can be applied to many selfsupervised methods with consis </p>
<p> Unsupervised Learning of Visual Features by Contrasting Cluster Assignments </p><p> the pascal visual object classes (voc) challenge </p><p> ce with supervised models even further. Indeed, for large architectures, our method shrinks the gap with supervised training to 0.6. on the Places205 [65], VOC07 [#b16][17], and iNaturalist2018 [52] datasets. Our method outperforms supervised features on all three datasets. Note that SwAV is t </p>
<p> Unsupervised Learning of Visual Features by Contrasting Cluster Assignments </p><p> unsupervised visual representation learning by context prediction </p><p> taset size.Handcrafted pretext tasks. Many selfsupervised methods manipulate the input data to extract a supervised signal in the form of a pretext task [1,[#b13]14,30,32,34,40, </p>
<p> Unsupervised Learning of Visual Features by Contrasting Cluster Assignments </p><p> learning to see by moving </p><p> ur method to scale gracefully to any dataset size.Handcrafted pretext tasks. Many selfsupervised methods manipulate the input data to extract a supervised signal in the form of a pretext task [#b0][1,14,30,32,34, </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> name disambiguation in aminer: clustering, maintenance, and human in the loop </p><p> which belong to an identical name or names with highly similar spellings to different people entities. Plenty of researches have been conducted to solve the name ambiguity problem. Supervised methods [#b0][1],The research is supported by the National Key Research and Development Plan (2017YFC1601504), the Natural Science Foundation of China (61836013), the CNT. semantic information of text information and discrete features (e.g., authors, venue). High quality representations play a critical role to quantify distinctions and similarities between publications [#b0][1]. The majority of existing solutions utilize biographical features such as title, abstract, organization, author and venue, as well as relationship features such. ews, which means most of publications belong to a few dominant authors. Hierarchical Agglomerative Clustering (HAC) method works well for skewed data and is widely in many name disambiguation methods [#b0][1], [5], [9], [20], [21. these baseline methods are as follows Component This method simply partitions each PHNet into connected components to generate the clustering results for each name to be disambiguated. Zhang et al. [#b0][1] This method uses a global metric learning and local linkage learning based on a graph autoencoder method to learn the publications embeddings, then it propose. demonstrated in their own paper. These comparison methods use different kinds of cluster strategy. For example, [5] need to specify the number of distinct author, [#b0][1] need labeled data to estimate the number. For a fair comparison, we assume the number of clusters is set to real value and choose HAC as the clustering method o. [5] ignore the text information, and Xu et al. just use the word cooccurrence information of text and loss a certain amount of semantic information. Zhang et al. [#b0][1] also use a graph convolutional network based encoderdecoder model but on homogeneous graph that can not extract multilayer relationship that contains various. elongs to a distinct person. [8] propose a Markov random fields based framework to extract multiple types of characteristics and relations in publication database. [#b0][1] use a global metric learning and local linkage graph autoencoder algorithm to learn the representation of publications, but it requires lots of human labeled d </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> modularity and community structure in networks </p><p> cluster, then the two closest clusters (with biggest similarity) are merged in each step until the number of clusters reaches the specified K.When K is unknown, we adopt an optimal modularity [#b21][22] partitioning mechanism to determine the partition of publications. The modularity M of a partition of nodes on graph G r is defined as followsformula xm </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> heterogeneous graph neural network </p><p> n neural networks to obtain node representations. [35] introduces a relational graph convolutional network to link prediction task and entity classification task. [#b35][36] propose a heterogeneous graph neural network model which considers both types and heterogeneous attributes of nodes. There are a large amount of works on name </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> adana: active name disambiguation </p><p> lly ambiguous names. Many methods [3], [5], [6], [12], [#b12][13] are implemented on a static collection extracted from DLs, and need to run the author disambiguation process on the whole collection when new records are adde. blem.[3] construct a publication network only by coauthor relation and devise a novel similarity metric. Then they use affinity propagation clustering algorithm to group result into clusters. [#b12][13] introduce a pairwise factor graph model which can be extended by incorporation various features. The vast majority of name disambiguation solutions are conduc </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> large-scale named entity disambiguation based on wikipedia data </p><p> shed publications that may contain potentially ambiguous names. Many methods [3], [5], [6], [#b11][12], [13] are implemented on a static collection extracted from DLs, and need to run the author disambiguation process on the </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> effective string processing and matching for author disambiguation </p><p> and construct publication networks, then use graphbased [3] [5] or heuristic methods [6], [#b6][7] to learn the similarity between publications. Some methods [4], [8] ref type"bibr" ta </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> modeling relational data with graph convolutional networks </p><p> aph structural information. GCN [34] and GraphSage [25] use graph convolution neural networks to obtain node representations. [#b34][35] introduces a relational graph convolutional network to link prediction task and entity classification task. [36] propose </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> incremental author name disambiguation for scientific citation data </p><p> The vast majority of name disambiguation solutions are conducted on static datasets. To disambiguate the new introduced publications in DLs, some works design incremental name disambiguation methods, [#b26][27] propose a probabilistic model that use a rich set of metadata and classifies new publications to existing author entities. [28] </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> pathsim: meta path-based top-k similarity search in heterogeneous information networks </p><p> n they are applied on heterogeneous networks, such random walks ignore the types of relations, are biased by highly visible relation types and concentrated nodes, and generate incorporated node paths [#b31][32]. Metapath2Vec [17] proposes an embedding method on heterogeneous network based on metapath. Some other methods have offe </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> a multi-level author name disambiguation algorithm </p><p> works well for skewed data and is widely in many name disambiguation methods [1], [5], [9], [#b19][20], [21]. However, the HAC has the following drawbacks its time complexity is high compared with some other clustering meth </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> incremental author name disambiguation by exploiting domain-specific heuristics </p><p> ental name disambiguation methods, [27] propose a probabilistic model that use a rich set of metadata and classifies new publications to existing author entities. [#b27][28] combines several domainspecific heuristics in order to automatically create and update publication clusters and determine the author of each publication. </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> a unified probabilistic framework for name disambiguation in digital library </p><p> heuristic methods [6], [7] to learn the similarity between publications. Some methods [4], [#b7][8] [10] use the the word cooccurrence information in publications' content to design relations between publications, which ma. "b10"[11] assume that the cluster number of ambiguous author K is known in advance, but in real world we actually have no clue about the number. Some [4], [#b7][8] propose different strategies to estimate the number. However, there are usually predefined parameters in these strategies. In addition, with the increasing dat. rches select various features from digital libraries and use them to quantify the similarity of publications, then cluster them into disjoint clusters, each of the which belongs to a distinct person. [#b7][8] propose a Markov random fields based framework to extract multiple types of characteristics and relations in publication database. ref type"bibr" target"b0" </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> hin2vec: explore meta-paths in heterogeneous information networks for representation learning </p><p> rk with binary edges. It uses metapathbased random walks to construct the heterogeneous neighborhood of a node and then leverages a heterogeneous skipgram model to perform node embeddings. Hin2Vec [#b23][24] Hin2Vec is also an unweighted heterogeneous network embedding method that can capture rich semantic of relationships and the details of network structure to </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> adana: active name disambiguation </p><p> lly ambiguous names. Many methods [3], [5], [6], [12], [#b12][13] are implemented on a static collection extracted from DLs, and need to run the author disambiguation process on the whole collection when new records are adde. blem.[3] construct a publication network only by coauthor relation and devise a novel similarity metric. Then they use affinity propagation clustering algorithm to group result into clusters. [#b12][13] introduce a pairwise factor graph model which can be extended by incorporation various features. The vast majority of name disambiguation solutions are conduc </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> distributed representations of words and phrases and their compositionality </p><p> bel(5)where u i represents the embedding vector of p i encoded from its initial features u (0) i by HGCN in eq.2. Then, we adopt the popular negative sampling method proposed in [#b17][18] to sample negative nodes to increase the optimization efficiency. Then, the probability can be approximately defined aslog p </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> on the combination of domain-specific heuristics for author name disambiguation: the nearest cluster method </p><p> en publications by these characteristics and construct publication networks, then use graphbased [3] [5] or heuristic methods [#b5][6], [7] to learn the similarity between publications. Some methods [4], ref type"bibr" ta. how to efficiently process the new published publications that may contain potentially ambiguous names. Many methods [3], [5], [#b5][6], [12], [13] are implemented on a static collection extracted from DLs, and need to run </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> on graph-based name disambiguation </p><p> citation to generate these representations. Most works design relationships such as coauthorship between publications by these characteristics and construct publication networks, then use graphbased [#b2][3] [5] or heuristic methods [6], [7] to learn the simi. ring strategies for name disambiguation are demanded.The third challenge is how to efficiently process the new published publications that may contain potentially ambiguous names. Many methods [#b2][3], [5], [6], [12], [13 </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> pathsim: meta path-based top-k similarity search in heterogeneous information networks </p><p> n they are applied on heterogeneous networks, such random walks ignore the types of relations, are biased by highly visible relation types and concentrated nodes, and generate incorporated node paths [#b31][32]. Metapath2Vec [17] proposes an embedding method on heterogeneous network based on metapath. Some other methods have offe </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> deepwalk: online learning of social representations </p><p> ight Guided Random Walks Our task is to train the HGCN model so that it can encode each publication node in PHNet to a high quality representation. Inspired by the network embedding methods DeepWalk [#b15][16] and Metapath2Vec [17] which use a random walk strategy and the skipgram model to learning node representation in network. work embedding model. For a fair comparison, We use them to learn publication representations on the networks with same construction as PHNet, and use HAC to generate the clustering results DeepWalk [#b15][16] DeepWalk is a network embedding method based on random walks to learn latent node representations and it is only applicable for homogeneous unweighted networ. create and update publication clusters and determine the author of each publication.Network Embedding. Recently, there has been a growing interest in the network embedding technology. DeepWalk [#b15][16] and Node2Vec [29] use random walk strategy on network and skipgram [30], ref type </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> distributed representations of words and phrases and their compositionality </p><p> bel(5)where u i represents the embedding vector of p i encoded from its initial features u (0) i by HGCN in eq.2. Then, we adopt the popular negative sampling method proposed in [#b17][18] to sample negative nodes to increase the optimization efficiency. Then, the probability can be approximately defined aslog p </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> deepwalk: online learning of social representations </p><p> ight Guided Random Walks Our task is to train the HGCN model so that it can encode each publication node in PHNet to a high quality representation. Inspired by the network embedding methods DeepWalk [#b15][16] and Metapath2Vec [17] which use a random walk strategy and the skipgram model to learning node representation in network. work embedding model. For a fair comparison, We use them to learn publication representations on the networks with same construction as PHNet, and use HAC to generate the clustering results DeepWalk [#b15][16] DeepWalk is a network embedding method based on random walks to learn latent node representations and it is only applicable for homogeneous unweighted networ. create and update publication clusters and determine the author of each publication.Network Embedding. Recently, there has been a growing interest in the network embedding technology. DeepWalk [#b15][16] and Node2Vec [29] use random walk strategy on network and skipgram [30], ref type </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> deep neural networks for learning graph representations </p><p> ork (GNN) based methods that applies deep neural networks on graphstructured data are significant developed in recent years, GNNs can also be used as a node encoder to learn network embeddings. DNGR [#b32][33] proposes a deep neural networks based method to learn a lowdimensional vector representation for each vertex by capturing the graph structural information. G </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> author name disambiguation using graph node embedding method </p><p> many name disambiguation methods [1], [5], [9], [20], [#b20][21]. However, the HAC has the following drawbacks its time complexity is high compared with some other clustering methods and it needs to take the number of clus. rk, [4] construct five relationship networks among publications and use a network embedding algorithm to learn representations of publications via their neighbors, [#b20][21] introduce a simple random walk strategy and a graph embedding method on a homogeneous network and use HAC to partition the publications. However, these method </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> distributed representations of sentences and documents </p><p> h neural network models can be understood as special cases of a simple differentiable messagepassing model.Given a PHNet G  (V, E, R), we start with initialize node attributes, we use Doc2vec [#b14][15] to encode the text information (i.e, title or abstract) of each publication p i  V into a fixed length feature vector u (0) i . For each publication node, it </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> author name disambiguation using graph node embedding method </p><p> many name disambiguation methods [1], [5], [9], [20], [#b20][21]. However, the HAC has the following drawbacks its time complexity is high compared with some other clustering methods and it needs to take the number of clus. rk, [4] construct five relationship networks among publications and use a network embedding algorithm to learn representations of publications via their neighbors, [#b20][21] introduce a simple random walk strategy and a graph embedding method on a homogeneous network and use HAC to partition the publications. However, these method </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> pathsim: meta path-based top-k similarity search in heterogeneous information networks </p><p> n they are applied on heterogeneous networks, such random walks ignore the types of relations, are biased by highly visible relation types and concentrated nodes, and generate incorporated node paths [#b31][32]. Metapath2Vec [17] proposes an embedding method on heterogeneous network based on metapath. Some other methods have offe </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> large-scale named entity disambiguation based on wikipedia data </p><p> shed publications that may contain potentially ambiguous names. Many methods [3], [5], [6], [#b11][12], [13] are implemented on a static collection extracted from DLs, and need to run the author disambiguation process on the </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> a novel approach for author name disambiguation using ranking confidence </p><p> al Agglomerative Clustering (HAC) method works well for skewed data and is widely in many name disambiguation methods [1], [5], [#b8][9], [20], [21]. However, the HAC has the following drawbacks its time complexity is high. "b0"[1] use a global metric learning and local linkage graph autoencoder algorithm to learn the representation of publications, but it requires lots of human labeled data to train the model. [#b8][9] propose a hierarchical agglomerative clustering based approach using the coauthor and title attributes. Other works attempt to utilize graph topology and linkag </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> distributed representations of sentences and documents </p><p> h neural network models can be understood as special cases of a simple differentiable messagepassing model.Given a PHNet G  (V, E, R), we start with initialize node attributes, we use Doc2vec [#b14][15] to encode the text information (i.e, title or abstract) of each publication p i  V into a fixed length feature vector u (0) i . For each publication node, it </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> author name disambiguation using graph node embedding method </p><p> many name disambiguation methods [1], [5], [9], [20], [#b20][21]. However, the HAC has the following drawbacks its time complexity is high compared with some other clustering methods and it needs to take the number of clus. rk, [4] construct five relationship networks among publications and use a network embedding algorithm to learn representations of publications via their neighbors, [#b20][21] introduce a simple random walk strategy and a graph embedding method on a homogeneous network and use HAC to partition the publications. However, these method </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> adana: active name disambiguation </p><p> lly ambiguous names. Many methods [3], [5], [6], [12], [#b12][13] are implemented on a static collection extracted from DLs, and need to run the author disambiguation process on the whole collection when new records are adde. blem.[3] construct a publication network only by coauthor relation and devise a novel similarity metric. Then they use affinity propagation clustering algorithm to group result into clusters. [#b12][13] introduce a pairwise factor graph model which can be extended by incorporation various features. The vast majority of name disambiguation solutions are conduc </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> hin2vec: explore meta-paths in heterogeneous information networks for representation learning </p><p> rk with binary edges. It uses metapathbased random walks to construct the heterogeneous neighborhood of a node and then leverages a heterogeneous skipgram model to perform node embeddings. Hin2Vec [#b23][24] Hin2Vec is also an unweighted heterogeneous network embedding method that can capture rich semantic of relationships and the details of network structure to </p>
<p> unsupervised author disambiguation using heterogeneous graph convolutional network embedding </p><p> two supervised learning approaches for name disambiguation in author citations </p><p> s how to efficiently determine the assignment of publications when we do not exactly know the number of distinct person for an ambiguous name. Some researches [5], [#b10][11] assume that the cluster number of ambiguous author K is known in advance, but in real world we actually have no clue about the number. Some ref type"bibr" t </p>
<p> Adaptive Universal Generalized PageRank Graph Neural Network </p><p> contextual stochastic block models </p><p> lic and heterophilic node label patterns and determine the tradeoff between node and topological feature exploration, we first describe the recently proposed contextual stochastic block model (cSBM) [#b10](Deshpande et al., 2018). The cSBM allows for smoothly controlling the "informativeness ratio" between node features and graph topology, where the graph can vary f. SYNTHETIC AND REALWORLD DATASETSSynthetic data. In order to test the ability of label learning of GNNs on graphs with arbitrary levels of homophily and heterophily, we propose to use cSBMs [#b10](Deshpande et al., 2018) to generate synthetic graphs. We consider the case with two equalsize classes. In cSBMs, the node features are Gaussian random vectors, w. vely. Moreover, positive  s correspond to homophilic graphs while negative  s correspond to heterophilic graphs. The informationtheoretic limits of reconstruction for the cSBM are characterized in [#b10]Deshpande et al. (2018). The results show that, asymptotically, one needs  2   2  gt 1 to ensure a vanishing ratio of the misclassified nodes and the total. ) 2 . Ideally, GNNs that are able to optimally learn on both homophilic and heterophilic graph should have similar performances for  and . Due to space limitation we refer the interested reader to [#b10](Deshpande et al., 2018) for a review of all formal theoretical results and only outline the cSBM properties needed for our analysis. Additional information is als. the node features and the graph structure respectively.One reason for using the cSBM to generate synthetic data is that the informationtheoretic limit of the model is already characterized in [#b10]Deshpande et al. (2018). This result is summarized below.Theorem A.7 (Informal main result in [#b10]Deshpande et al. (2018)).. tiontheoretic limit of the model is already characterized in [#b10]Deshpande et al. (2018). This result is summarized below.Theorem A.7 (Informal main result in [#b10]Deshpande et al. (2018)). Assume that n, f  , n f   and d  . Then there exists an estimator v such that lim inf n v,v  n </p>
<p> Adaptive Universal Generalized PageRank Graph Neural Network </p><p> dimitris berberidis, athanasios nikolakopoulos, and georgios b giannakis. adaptive diffusions for scalable learning over graphs </p><p> tational power to provide stateoftheart performance when addressing the above described application domains. Many GNNs use message passing (Gilmer et al., 2017[#b3]Battaglia et al., 2018) to manipulate node features and graph topology. They are constructed by stacking (graph) neural network layers which essentially propagate a </p>
<p> Adaptive Universal Generalized PageRank Graph Neural Network </p><p> weisfeiler-lehman graph kernels </p><p> of graphstructured data and its importance in solving numerous realworld problems such as semisupervised node classification and graph classification (Zhu, 2005[#b31]Shervashidze et al., 2011L amp Zhou, 2011). Usually, the data at hand contains two sources of information Node features a </p>
<p> Adaptive Universal Generalized PageRank Graph Neural Network </p><p> dimitris berberidis, athanasios nikolakopoulos, and georgios b giannakis. adaptive diffusions for scalable learning over graphs </p><p> tational power to provide stateoftheart performance when addressing the above described application domains. Many GNNs use message passing (Gilmer et al., 2017[#b3]Battaglia et al., 2018) to manipulate node features and graph topology. They are constructed by stacking (graph) neural network layers which essentially propagate a </p>
<p> Adaptive Universal Generalized PageRank Graph Neural Network </p><p> block models and personalized pagerank </p><p>  </p>
<p> Adaptive Universal Generalized PageRank Graph Neural Network </p><p> watch your step: learning node embeddings via graph attention </p><p> type"bibr" target"b19"(Klicpera et al., 2018) and they are not easy to interpret as our GPRGNN method. Some prior work also emphasizes adaptively learning the importance of different steps [#b1](AbuElHaija et al., 2018Berberidis et al., 2018). Nevertheless, none of the above works is applicable for semisupervised learning with GNN </p>
<p> Adaptive Universal Generalized PageRank Graph Neural Network </p><p> provably fast inference of latent features from networks: with applications to learning social circles and multilabel classification </p><p> ext of node classification asserts that nodes from the same class tend to form edges. Homophily is also a common assumption in graph clustering (Von Luxburg, 2007[#b33]Tsourakakis, 2015Dau amp Milenkovic, 2017) and in many GNNs design (Klicpera et al., 2018)r </p>
<p> Adaptive Universal Generalized PageRank Graph Neural Network </p><p> graph attention networks </p><p> actice, including graph convolutional layers (GCN) (Bruna et al., 2014Kipf amp Welling, 2017), graph attention layers (GAT) [#b34](Velickovic et al., 2018) and many others (Hamilton et al., 2017Wijesinghe amp Wang, 2019ref type. ndom splits and different initializations.Methods used for comparisons. We compare GPRGNN with 6 baseline models MLP, GCN (Kipf amp Welling, 2017), GAT [#b34](Velickovic et al., 2018), JKNet (Xu et al., 2018), GCNCheby (Defferrard et al., 2016), </p>
<p> Adaptive Universal Generalized PageRank Graph Neural Network </p><p> scaling graph neural networks with approximate pagerank </p><p> le f  . We can observe from Table 3 that indeed GPRGNN has a running time similar to that of APPNP. It is nevertheless worth pointing out that the authors of [#b5]Bojchevski et al. (2020) successfully scaled APPNP to operate on large graphs. Whether the same techniques may be used to scale GPRGNNs is an interesting open ques </p>
<p> Adaptive Universal Generalized PageRank Graph Neural Network </p><p> latent network features and overlapping community discovery via boolean intersection representations </p><p> same class tend to form edges. Homophily is also a common assumption in graph clustering (Von Luxburg, 2007Tsourakakis, 2015[#b8]Dau amp Milenkovic, 2017) and in many GNNs design (Klicpera et al., 2018). Methods developed for homophilic graphs are nonuni </p>
<p> Adaptive Universal Generalized PageRank Graph Neural Network </p><p> weisfeiler-lehman graph kernels </p><p> of graphstructured data and its importance in solving numerous realworld problems such as semisupervised node classification and graph classification (Zhu, 2005[#b31]Shervashidze et al., 2011L amp Zhou, 2011). Usually, the data at hand contains two sources of information Node features a </p>
<p> knowledge enhanced personalized search </p><p> entity query feature expansion using knowledge base links </p><p> tateoftheart entity linking techniques only have 50 accuracy on web queries [41]. Using such noisy query entities in ranking often requires manual annotations [#b11][12] or soft linkingdiversification [42]. Personalization provides a natural way to help resolve the ambiguity in query entit. . Some take entities contained in the query or document as a kind of relevance ranking features, such as term weight in queries according to entity descriptions [[#b11]]. There are also some researches using entities as connections between the documents and queries for better matching. Liu et al.  </p>
<p> knowledge enhanced personalized search </p><p> word-entity duet representations for document ranking </p><p> ref type"bibr" target"b13"[[#b41]].Both personalized search and entityoriented search leverage information beyond the current query to better unde. s information needs. Their advantages also naturally reinforce each other. One key challenge of entityoriented search is the difficulty of query entity linking. Queries are often short and ambiguous [#b41][42], making query entity linking a challenging task A recent study shows that stateoftheart entity linking techniques only have 50 accuracy on web queries r. eries [41]. Using such noisy query entities in ranking often requires manual annotations [12] or soft linkingdiversification [#b41][42]. Personalization provides a natural way to help resolve the ambiguity in query entity linking For example, in Fig. 1. e personalized entity annotations enable KEPS to construct entity enhanced user profiles, using a memory network that represents user's search preferences in the wordentity duet representation space [#b41][42]. KEPS then conducts personalized ranking to adapt document ranking to satisfy user's information need, using the personalized search intent and the knowledge. r" target"b40"[41] consider the bags of entity representations in search model, and the interaction between bags of word representations and bags of entity representations is also studied in [#b41][42]. Neuralbased search model EDRM [21] study the interaction between word vectors and entity vectors. Our model KEPS is also </p>
<p> knowledge enhanced personalized search </p><p> esdrank: connecting query and documents through external semi-structured data </p><p> xt representation and ranking accuracy [[#b39]].Both personalized search and entityoriented search leverage information. 1"12]. There are also some researches using entities as connections between the documents and queries for better matching. Liu et al. [19] and Xiong et al. [#b39][40] takes the entity as a latent space and learn querydocument matching relevance through the latent space. Ensan et al. [14]ref </p>
<p> knowledge enhanced personalized search </p><p> how do users describe their information need: query recommendation based on snippet click model </p><p> [pos is RELATED WORK] leased dataset from Yandex 1 , the text contents of queries and documents have been encrypted, making it impossible to link them to entities. The Sogou dataset [#b19][20] contains only one month of user search log, limiting the effectiveness of personalized search which often uses a longer period to construct user profiles. Rec </p>
<p> knowledge enhanced personalized search </p><p> selecting good expansion terms for pseudo-relevance feedback </p><p> [pos is RELATED WORK] entity knowledge into adhoc web search. Some take entities contained in the query or document as a kind of relevance ranking features, such as term weight in queries according to entity descriptions [#b7][]. There are also some researches using entities as connections between the documents and queries for better matching. Liu </p>
<p> knowledge enhanced personalized search </p><p> a large-scale evaluation and analysis of personalized search strategies </p><p> [pos is RELATED WORK] 5, 38, 39] manually extracted the rich click features and topic features according to user's historical searches and clicks, which are effective in personalized search. Specifically, some works [#b12][] studied personalized click features, and Dou et al. [#b12][13] proposed PClick using c. [pos is RELATED WORK] ch are effective in personalized search. Specifically, some works [#b12][] studied personalized click features, and Dou et al. [#b12][13] proposed PClick using click features to improve personalized ranking effect. Other works [3,ref type"bibr" target"b8". [pos is RELATED WORK] ? ? ) ? ?(?, ? ? ? ) ? ?(? ? , ? ? ? ) ? ?(? ? , ? ? ? )].Query relevance is the relevance between documents and the original query, including vector similarity, click features ? ? as in [#b12][13], and interactive wordentity duet matching features ? ? ? (?, ?)  ?(?, ?) ? MLP(? ? ) ? ? ? ,where ?, ? is the. [pos is RELATED WORK]  to conduct entity linking for queries and removing the presonalized linking probabilities for comparision.Personalization baselines include the model using traditional features PClick [#b12][13] using click features and SLTB [4] using click features and topic features, which is the stateofart personalization model. [pos is RELATED WORK] ference for repeated queries, but also infer user's preference for nonrepeated queries. And it does not depend heavily on refinding information.Click Entropy Experiments. Larger click entropy [#b12][13] indicates that the query tends to be informational and ambiguous query while less click entropy indicates the query is likely to be a navigational query. So m </p>
<p> knowledge enhanced personalized search </p><p> adapting deep ranknet for personalized search </p><p> [pos is RELATED WORK] as been applied in personalized search [[#b29]]. It significantly improved personalization by learning the effective representations of user profiles and other personalized features from user's history. Ge e </p>
<p> knowledge enhanced personalized search </p><p> convolutional neural networks for soft-matching n-grams in ad-hoc search </p><p> [pos is RELATED WORK] ed search methods and entitybased adhoc ranking models. Since the candidate documents are retrieved by BM25 score, we take BM25 ranking as original ranking.Adhoc baselines include ConvKNRM [#b10][11], a neural model using interactive features and EDRM (EDRMCKNRM [21]), the stateofart adhoc model using interactive ent </p>
<p> knowledge enhanced personalized search </p><p> translating embeddings for modeling multi-relational data </p><p> [pos is RELATED WORK] g  lt  l a t e x i t gt lt l a t e x i t s h a 1 _ b a s e 6 4  "  f S t P L W U c W R p W u q 9 R j I q 3 U F N 8 c s  " gt A      4) and [#b4](5).A A C x n i c j V H L S s N A F D 2 N r 1 p f V Z d u g k V w V R I R d F l 0 0 2 V F  4 B a S j K d 1 t D J g 2 S i l C L 4 A. [pos is RELATED WORK] mula_66"? ? ? ?,?  ? ?  ? ? ? ?,? , ? ? ? ?,?  ? ?  ? ? ? ?,? , ? ?,?  ? ?  (? T ?,?  ? ?  ?),where ? ?,?  ? ? ? ? ? ? indicates the entity relation vector since we use TransE [#b4][5] to pretrain entity embedding. Adding the matrix ? is because the relationship between entities may also reflect the relevance to the query. For example, when q. [pos is RELATED WORK] n the word embedding using GloVe [27], taking the query texts and document titles in the search log as training corpus. We train the entity embedding using TransE [#b4][5], taking the entities and relations extracted from our data as input. Both the word embedding and entity embedding are fixed in convKNRM, EDRM and PEDRM we prop </p>
<p> knowledge enhanced personalized search </p><p> to personalize or not to personalize: modeling queries with variation in user intent </p><p>  </p>
<p> knowledge enhanced personalized search </p><p> dexter: an open source framework for entity linking </p><p> [pos is RELATED WORK] ces between query vectors as the task boundaries in [1].We use the entity titles in Wikipedia. The candidate entities for queries are collected using DEXTER [#b9][10] (note that entity linking for queries are done in Sec. 3.2). Since there is no entity annotation in documents in our data, we use TAGME ref type"bibr" target </p>
<p> knowledge enhanced personalized search </p><p> a picture of search </p><p> [pos is RELATED WORK] n uses a longer period to construct user profiles. Recently, the context search dataset constructed by Wasi et al. [1], which is based on the public AOL search log [#b25][26], makes it possible to study personalized search in the public domain. We conduct experiments on this dataset [1].En </p>
<p> knowledge enhanced personalized search </p><p> towards query log based personalization using topic models </p><p> [pos is RELATED WORK] ick features, and Dou et al. [13] proposed PClick using click features to improve personalized ranking effect. Other works [3,[#b8]9,17,23,29,34,r </p>
<p> knowledge enhanced personalized search </p><p> modeling the impact of short-and long-term behavior on search personalization </p><p> [pos is RELATED WORK] "bibr" target"b37"38] extracted the topics features from user's search history to predicted document relevance. Click features and topic features are combined and studied in some researches [#b3][]. B. [pos is RELATED WORK] r" target"b3"[]. Bennett et al. [#b3][4] proposed SLTB to combine the two types of features using learning to rank.Recently, deep learning has been applied in personalized search ref type"bibr. [pos is RELATED WORK] to promote the training of deep personalized models which further enhances the personalized effect.Another challenge in personalized search is that many search logs are not publicly available [#b3][].. [pos is RELATED WORK] ed linking probabilities for comparision.Personalization baselines include the model using traditional features PClick [13] using click features and SLTB [#b3][4] using click features and topic features, which is the stateofart personalization model using traditional features and the model based on deep learning HRNN </p>
<p> knowledge enhanced personalized search </p><p> learning to rank using gradient descent </p><p> [pos is RELATED WORK] ed the topics features from user's search history to predicted document relevance. Click features and topic features are combined and studied in some researches [[#b5]]. Bennett et al. ref type"bibr" target". [pos is RELATED WORK] queries with larger click entropy, we can see KEPS can better conduct personalization on the queries tend to be ambiguous. This meets our experimental expectations.   only shortterm history in Tab. [#b5]6 We can see memory networks also have certain advantages in dealing with shortterm history. These results show memory networks are effective especially in preserv </p>
<p> knowledge enhanced personalized search </p><p> evaluating search result diversity using intent hierarchies </p><p> [pos is RELATED WORK] ession search and longstanding preferences. RELATED WORKPersonalized Web Search. In addition to search result diversification [#b35][], personalized search is another way to address the problem of vauge queries to search engines. Personalized search has </p>
<p> knowledge enhanced personalized search </p><p> a picture of search </p><p> [pos is RELATED WORK] n uses a longer period to construct user profiles. Recently, the context search dataset constructed by Wasi et al. [1], which is based on the public AOL search log [#b25][26], makes it possible to study personalized search in the public domain. We conduct experiments on this dataset [1].En </p>
<p> knowledge enhanced personalized search </p><p> latent entity space: a novel retrieval approach for entity-bearing queries </p><p> explicit semantics, e.g. entities and relations from knowledge graphs, in search systems and effectively improves the text representation and ranking accuracy [14,[#b18]19,21,28,40,42,. e"bibr" target"b7"[]. There are also some researches using entities as connections between the documents and queries for better matching. Liu et al. [#b18][19] and Xiong et al. [40] takes the entity as a latent space and learn querydocument matching relevance through the latent s </p>
<p> knowledge enhanced personalized search </p><p> psgan: a minimax game for personalized search with limited and noisy click data </p><p> [pos is RELATED WORK] he two types of features using learning to rank.Recently, deep learning has been applied in personalized search [[#b21]]. It significantly improved personalization by learning the effective representa. [pos is RELATED WORK] ations of user profiles and other personalized features from user's history. Ge et al. HRNN [16] proposed to use a hierarchical RNN to model user's profile. PSGAN [#b21][22] proposed a generative adversarial network framework to promote the training of deep personalized models which further enhances the personalized effect.. [pos is RELATED WORK] personalized effect.Another challenge in personalized search is that many search logs are not publicly available [[#b21]]. In the released dataset from Yandex 1. [pos is RELATED WORK] effective because history in the current session tends to reflect user's session search intent, while the previous history may reflect user's global interests [[#b21]].Suppose a query ? has ? entity mentions (text string in query that may refer to certain entity), we denote the candidate entity list for the query by E. [pos is RELATED WORK] ing traditional features and the model based on deep learning HRNN (HRNNQA [16]) using hierarchical RNN and PSGAN (we choose the documentselection based model [#b21][22]) using adversarial training. To make a fair comparison, we also add entity information to the baseline HRNN to construct an entitybased personalization basel. [pos is RELATED WORK] layers and kernel functions in PEDRM are consistent with EDRM [21].Evaluation Metrics. Following the previous work [[#b21]], we use MAP, MRR, PK (precision in the top k positions) and AR (average ranking position of relevant documents) to evaluate our model. We follow official TREC. [pos is RELATED WORK] om Fig. 5b, we still have that the effect of PEDRM is similar on both types of queries. However we find that different from stated in [[#b21]], the personalization models perform better on queries with entropy less than 1. This may be because we count the improvement on MAP over the original ranking b. [pos is RELATED WORK] is may be because we count the improvement on MAP over the original ranking based on BM25, which is less efficient than the ranking from search engine used in [[#b21]]. Compared with SLTB, we can still see HRNN, HRNNEntity and PSGAN have more improvement on queries with click entropy no less than 1, which is consistent with. [pos is RELATED WORK] , we can still see HRNN, HRNNEntity and PSGAN have more improvement on queries with click entropy no less than 1, which is consistent with the previous works [[#b21]]. KEPS has significant improvement over the baselines on both two types of queries, and the gain on queries with less click entropy may partly come from the int </p>
<p> knowledge enhanced personalized search </p><p> convolutional neural networks for soft-matching n-grams in ad-hoc search </p><p> [pos is RELATED WORK] ed search methods and entitybased adhoc ranking models. Since the candidate documents are retrieved by BM25 score, we take BM25 ranking as original ranking.Adhoc baselines include ConvKNRM [#b10][11], a neural model using interactive features and EDRM (EDRMCKNRM [21]), the stateofart adhoc model using interactive ent </p>
<p> knowledge enhanced personalized search </p><p> temporal latent topic user profiles for search personalisation </p><p> [pos is RELATED WORK] f9,17,23,29,34,[#b34]35,38] extracted the topics features from user's search history to predicted document relevance. Click features and topic feat </p>
<p> knowledge enhanced personalized search </p><p> personalizing web search using long term browsing history </p><p> [pos is RELATED WORK] d PClick using click features to improve personalized ranking effect. Other works [[#b22]] </p>
<p> spanet: spatial pyramid attention network for enhanced image recognition </p><p> squeeze-and-excitation networks </p><p> e"bibr" target"b4"[4] in CNNs. By informing a CNN network where to look and what to pay attention to, attention networks achieve a better performance with fewer layers. As an example, SENet [#b5][5] introduces SqueezeandExcitation (SE) blocks to study the channel dependencies in a CNN architecture. Although aforementioned CNN architectures achieve better. proposed spatial pyramid structure is that it does not introduce any additional parameter. All layers in the spatial pyramid structure are not learnable, which is nearly costfree. Compared to SENet [#b5][5], our structure only modifies the first fullyconnected layer to tackle the large input size. The small computation overhead contributes to its enhanced performa. evaluate the performance of SPANet using CIFAR100 and a downsampled ImageNet dataset. Without bells and whistles, SPANet outperforms related stateofart work [[#b5]]. Experimental results show that structural information in the attention mechanism. InceptionV4 contains 36 carefully designed paths. All these paths are integrated together using filter concatenation as input to the next block. More recently, attention based networks such as SENet [#b5][5] and CBAM [6] provide an independent attention path to learn the weight of each channel and achieve stateoftheart performance.At. oftmax and Sigmoid, attention mechanism is able to selectively emphasize salient features as well as suppress insignificant features. Thus, visual features could be better captured and exploited. In [#b5][5], a SqueezeandExtraction block was proposed to learn the channelwise attention for each convolutional layer, which provides an endtoend training paradigm fo. Most of the existing selfattention based networks follow a path design pattern they learn an attention map from a feature map and then apply the learned attention map to the original feature map [#b5][] . However, being confined to aforementioned schema compromises the exploration of attention path connections. For SPANet. sion, SPANet refers to SPANetC unless otherwise specified. Spatial Pyramid AttentionMany existing attention based networks [#b5][] aggregate input feature maps into a 1D vector using global average poo. rget"b23"[23], and more.We note that existing work on global average pooling used the last feature map which is small in size (7 ? 7 for example). However, attention based CNNs (e.g., [#b5][5], [6], [7], etc.) apply global average pooling on each feature map. As presented in ref type"bibr" t. s. However, it cannot be used to learn channel dependency and its nonlinear expression affects the effectiveness of the attention mechanism. To address this problem, we leverage the excitation block [#b5][5] to encode v and generate a 1D attention map ?. The excitation block employs two fullyconnected layers. Then a sigmoid layer is employed to normalize the output. la_5"?  sig (W2? (W1v)) , (6)where ? is a rectified linear unit (ReLU) function and sig denotes the sigmoid function. Like in SENet [#b5][5], we set r to 16. Pointwise ConvolutionThe attention block in our proposed spatial </p>
<p> spanet: spatial pyramid attention network for enhanced image recognition </p><p> aggregated residual transformations for deep neural networks </p><p> base CNN architectures, including VGG [25], MobileNetV2 [11], DenseNet [10], and ResNext [#b26][26], to study the generalizability of SPANet. Experiment Settings and DatasetsWe imple. networks. We employ four base networks, i.e., lightweight model MobileNetV2 [11], heavyweight model DenseNet [10], ResNeXt [#b26][26], and VGG16 [25].Recognition accuracy.  the base networks in all cases. This is different from our intuition. As af </p>
<p> spanet: spatial pyramid attention network for enhanced image recognition </p><p> yolo9000: better, faster, stronger </p><p> target"b21"[21] to replace the conventional fullyconnected layers in CNNs. Since then, it has prevailed in computer vision for recognition [2], detection [#b22][22], segmentation [23], and more.We note that existing work on global average pooling used the last feature map which </p>
<p> spanet: spatial pyramid attention network for enhanced image recognition </p><p> densely connected convolutional networks </p><p> ing CIFAR100 and a downsampled ImageNet dataset. Without bells and whistles, SPANet outperforms related stateofart work [[#b10]]. Experimental results show that structural information in the attention mechanism, which we focus on, is a crucial fact. d in Highway Networks and keeps the information passed though shortcuts. The better performance achieved by ResNet has made shortcut connections attractive. As a more dense reformulation, the work in [#b10][10] connects every convolutional layer in a deep convolutional network. Without introducing more parameters, it effectively alleviates the vanishing gradient prob. esNet. We also apply SPANet and SENet to several other base CNN architectures, including VGG [25], MobileNetV2 [11], DenseNet [#b10][10], and ResNext [26], to study the generalizability of SPANet. head n"4. e performance of our SPANet with SENet and the base networks. We employ four base networks, i.e., lightweight model MobileNetV2 [11], heavyweight model DenseNet [#b10][10], ResNeXt [26], and VGG16 [25].Recognition accuracy.  the base networks in all </p>
<p> spanet: spatial pyramid attention network for enhanced image recognition </p><p> yolo9000: better, faster, stronger </p><p> target"b21"[21] to replace the conventional fullyconnected layers in CNNs. Since then, it has prevailed in computer vision for recognition [2], detection [#b22][22], segmentation [23], and more.We note that existing work on global average pooling used the last feature map which </p>
<p> spanet: spatial pyramid attention network for enhanced image recognition </p><p> fast r-cnn </p><p> f pooling schema, our spatial pyramid struc97817281133192031.00 ?2020 IEEE ture could be considered similar to SPPNet [8] and Region of Interesting Pooling [#b9][9]. In contrast, our spatial pyramid structure encodes a feature map with more structural information while SPPNet and Region of Interesting Pooling aim to obtain </p>
<p> spanet: spatial pyramid attention network for enhanced image recognition </p><p> yolo9000: better, faster, stronger </p><p> target"b21"[21] to replace the conventional fullyconnected layers in CNNs. Since then, it has prevailed in computer vision for recognition [2], detection [#b22][22], segmentation [23], and more.We note that existing work on global average pooling used the last feature map which </p>
<p> spanet: spatial pyramid attention network for enhanced image recognition </p><p> training very deep networks </p><p> tpwww.teic.orgns1.0"RELATED WORKMultiPath Connection. Multipath connection in deep learning was first used in Highway Networks [[#b14]]. By allowing an unimpeded information flowed across several layers, a Highway Network is capable of reusing the information from previous layers, which facilit </p>
<p> spanet: spatial pyramid attention network for enhanced image recognition </p><p> residual attention network for image classification </p><p> . We argue that the limitation originating from the global average pooling makes the shallow layers (which output bigsize feature maps) unable to fully leverage the advantages of attention mechanism [#b7][7]. Following this argument, we present Spatial Pyramid Attention (SPA), which introduces a spatial pyramid structure to encode the intermediate features instead o. global average pooling used the last feature map which is small in size (7 ? 7 for example). However, attention based CNNs (e.g., [5], [6], [#b7][7], etc.) apply global average pooling on each feature map. As presented in [20], GAP behaves similarly to a structural regula </p>
<p> spanet: spatial pyramid attention network for enhanced image recognition </p><p> cbam: convolutional block attention module </p><p>  </p>
<p> spanet: spatial pyramid attention network for enhanced image recognition </p><p> inception-v4, inception-resnet and the impact of residual connections on learning </p><p> y alleviates the vanishing gradient problem and improves feature reuse.In addition to shortcut connections, there are works studying the internal multipath connections in convolutional blocks [#b15][15]. The InceptionV4 Network [#b15][15] is one of this kind. Besides a shortcut connection, each inception block in InceptionV4 con. use.In addition to shortcut connections, there are works studying the internal multipath connections in convolutional blocks [#b15][15]. The InceptionV4 Network [#b15][15] is one of this kind. Besides a shortcut connection, each inception block in InceptionV4 contains 36 carefully designed paths. All these paths are integrated </p>
<p> Contrastive Learning with Hard Negative Samples </p><p> hard negative mixing for contrastive learning </p><p> product reviews (CR) (Hu amp Liu, 2004), and sentiment of movie reviews (MR) (Pang amp Lee, 2005).Comparison with [#b23]Kalantidis et al. (2020) [#b23]Kalantidis et al. (2020) also consider ways to sample negatives, and propose a mixing strategy for h. Liu, 2004), and sentiment of movie reviews (MR) (Pang amp Lee, 2005).Comparison with [#b23]Kalantidis et al. (2020) [#b23]Kalantidis et al. (2020) also consider ways to sample negatives, and propose a mixing strategy for hard negatives, called MoCHi. The main points of difference are </p>
<p> Contrastive Learning with Hard Negative Samples </p><p> deep residual learning for image recognition </p><p> extra lines of code compared to the standard objective. D.1 VISUAL REPRESENTATIONSWe implement SimCLR in PyTorch. We use a ResNet50 [#b18](He et al., 2016) as the backbone with embedding dimension 2048 (the representation used for linear readout), and projection head into the lower 128dimensional sp </p>
<p> Contrastive Learning with Hard Negative Samples </p><p> deep chakraborty, and erik learned-miller. unsupervised hard example mining from videos for improved object detection </p><p>  </p>
<p> Contrastive Learning with Hard Negative Samples </p><p> learning a similarity metric discriminatively, with application to face verification </p><p>     INTRODUCTIONOwing to their empirical success, contrastive learning methods [#b7](Chopra et al., 2005Hadsell et al., 2006) have become one of the most popular selfsupervised approaches for learning represen </p>
<p> Contrastive Learning with Hard Negative Samples </p><p> a sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts </p><p> cation (TREC) (Voorhees amp Harman, 2002), opinion polarity (MPQA) (Wiebe et al., 2005), subjectivity classification (SUBJ) [#b35](Pang amp Lee, 2004), product reviews (CR) (Hu amp Liu, 2004), and sentiment of movie reviews (MR) ref type"bibr" target </p>
<p> Contrastive Learning with Hard Negative Samples </p><p> time-contrastive networks: self-supervised learning from video </p><p> video data (Logeswaran amp Lee, 2018Oord et al., 2018Purushwalkam amp Gupta, 2020[#b40]Sermanet et al., 2018).Surprisingly, the choice of negative pairs has drawn much less attention in contrastive learning. Often, given an "anchor" point x, a </p>
<p> Contrastive Learning with Hard Negative Samples </p><p> deep metric learning with hierarchical triplet loss </p><p>  </p>
<p> Contrastive Learning with Hard Negative Samples </p><p> an efficient framework for learning sentence representations </p><p> in learning control policies from raw pixel data (Srinivas et al., 2020). Positive sampling techniques have also been proposed for sentence, audio, and video data [#b28](Logeswaran amp Lee, 2018Oord et al., 2018Purushwalkam amp Gupta, 2020ref type"b. es (beforeafter) as positive samples. Embeddings are trained using the unlabeled BookCorpus dataset (Kiros et al., 2015), and evaluated following the protocol of [#b28]Logeswaran amp Lee (2018) on six downstream tasks. The results are reported in  Fig. 4 (left,middle) shows that for </p>
<p> Contrastive Learning with Hard Negative Samples </p><p> a training algorithm for optimal margin classifiers </p><p> r details). In doing so, we illustrate that it is easy to adapt our hard sampling method to other contrastive frameworks.Fig. 3 shows the results of finetuning an SVM [#b3](Boser et al., 1992Cortes amp Vapnik, 1995) on the fixed, learned embedding for a range of different values of . Hard sampl </p>
<p> Contrastive Learning with Hard Negative Samples </p><p> learning classifiers from only positive and unlabeled data </p><p> mpling strategies often apply transformations that preserve semantic content, e.g., jittering, random cropping, separating color channels, etc. (Chen et al., 2020a[#b12]cTian et al., 2019). Such transformations have also been effective in learning control policies from raw pixel data ref type. sen to be the marginal distribution p, or, in practice, an empirical approximation of it (Tian et al., 2019Chen et al., 2020a[#b12]cHe et al., 2020Chen et al., 2020cOord et al., 2018re. ar how to sample efficiently from it. To work towards a practical method, note that we can rewrite this distribution by adopting a PUlearning viewpoint (Elkan amp Noto, 2008[#b12]Du Plessis et al., 2014Chuang et al., 2020). That is, by conditioning on the event h(x)  h(x  ) we can split q  (x  ) as </p>
<p> Contrastive Learning with Hard Negative Samples </p><p> deep graph infomax </p><p>  </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> multilingual knowledge graph embeddings for cross-lingual knowledge alignment </p><p> oss different embeddings is hindered by the lack of reliable alignment information that bridges different KGs. Recent works on multilingual KG embeddings provide support for automated entity matching [#b5](Chen et al., 2017(Chen et al., , 2018bSun et al., 2018Sun. "Ji et al., 2020) for more information.Multilingual KG Embeddings. Recent studies have extended embedding models to bridge multiple KGs, typically for KGs of multiple languages. MTransE [#b5](Chen et al., 2017) jointly learns a transformation across two separate translational embedding spaces along with the KG structures. BootEA ref type"bibr" target. ounts. Embedding LearningThe embedding learning process jointly trains the knowledge model and the alignment model following [#b5]Chen et al. (2017), while selflearning is added to improve the alignment learning. The details are described below. Knowledge model. A knowledge model seeks to enc. ed in Eq (1),J G i G j Ais the alignment loss between G i and G j .  is a positive hyperparameter that weights the two model components.Following [#b5]Chen et al. (2017), instead of directly optimizing J in Eq. ( 5), our implementation optimizes each J G K and each Jformula xmlid" </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> rankboost: an efficient boosting algorithm for combining preferences </p><p> odel, particularly by learning model weights from the sample distribution.Representative methods include AdaBoost (Freund and Schapire, 1997) and RankBoost [#b9](Freund et al., 2004), which target at classification and ranking respectively. AdaBoost starts with a pool of weak classifiers and iteratively selects the best one. bination, which aims at reinforcing correct beliefs and compensating for alignment error. An embedding model that makes more accurate predictions should receive a higher weight. Inspired by RankBoost [#b9](Freund et al., 2004), we reduce the ranking combination problem to a classifier ensemble problem. KEnS b therefore learns model weights in a similar manner as AdaB. ry, Japan) (The Tale of Genji, genre, Monogatari) (The Tale of Genji, genre, Love Story) Queries Q  q1  (The Tale of Genji, country, ?t) q2  (The Tale of Genji, genre, ?t) Similar to RankBoost [#b9](Freund et al., 2004) Ranking loss. The overall objective of KEnS b is to minimize the sum of ranks of all correct answers in the combined ranking list q e(q) r(e. tive is minimizing the number of misordered critical entity pairs in the combined ranking list.Let the set of all the critical entity pairs from all the validation queries of an entity as P . [#b9]Freund et al. (2004) have proved that, when using RankBoost, this ranking loss is bounded as followsp  p  P, p is misordered </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> bio-joie: joint representation learning of biological knowledge bases </p><p> future research. One is to exploit the potential of KEnS on completing lowresource KGs, and the other is to extend the ensemble transfer mechanism to population sparse domain knowledge in biological [#b12](Hao et al., 2020) and medical knowledge bases (Zhang et al., 2020). Pariticularly, we also seek to ensure the global logical </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> cross-lingual entity alignment via joint attributepreserving embedding </p><p> including entity descriptions (Chen et al., 2018bZhang et al., 2019), attributes (Trsedya et al., 2019[#b25]Sun et al., 2017Yang et al., 2019), neighborhood information (Wang et al., 2018ref typ </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> a benchmarking study of embedding-based entity alignment for knowledge graphs </p><p> "b27"Sun et al., , 2020a) ) and degree centrality measures (Pei et al., 2019). A systematic summary of relevant approaches is given in a recent survey by [#b29]Sun et al. (2020b). Although these approaches focus on the KG alignment that is different from the problem we tackle here, such techniques can be leveraged to supp </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> embedding uncertain knowledge graphs </p><p> r" target"b42"(Zhang et al., 2020). Pariticularly, we also seek to ensure the global logical consistency of predicted facts in the ensemble process by incorporating probabilistic constraints [#b6](Chen et al., 2019).Figure1 A depiction of the ensemble i </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> yago: a multilingual knowledge base from wikipedia, wordnet, and geonames </p><p> er of influential knowledge bases, including DBpedia (Lehmann et al., 2015), Wikidata (Vrandei and Krtzsch, 2014) and YAGO [#b22](Rebele et al., 2016). In contrast, KGs often consist of numerous entities that cannot be easily aligned, and entity alignment is available only in small amounts. </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> open question answering with weakly supervised embedding models </p><p> tionable knowledge that is crucial to various knowledgedriven applications (KoncelKedziorski et al., 2019Chen et al., 2018a[#b1]Bordes et al., 2014). Recently, extensive efforts have been invested in KG embedding models, which encode entities as lowdimensional vectors and capture relations </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> yago: a multilingual knowledge base from wikipedia, wordnet, and geonames </p><p> er of influential knowledge bases, including DBpedia (Lehmann et al., 2015), Wikidata (Vrandei and Krtzsch, 2014) and YAGO [#b22](Rebele et al., 2016). In contrast, KGs often consist of numerous entities that cannot be easily aligned, and entity alignment is available only in small amounts. </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> bio-joie: joint representation learning of biological knowledge bases </p><p> future research. One is to exploit the potential of KEnS on completing lowresource KGs, and the other is to extend the ensemble transfer mechanism to population sparse domain knowledge in biological [#b12](Hao et al., 2020) and medical knowledge bases (Zhang et al., 2020). Pariticularly, we also seek to ensure the global logical </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> probabilistic alignment of relations, instances, and schema </p><p> elines are set to their best configurations. We also include a baseline named RotatEPARIS, which trains RotatE on 5 KGs and uses the representative nonembedding symbolic entity alignment tool PARIS [#b24](Suchanek et al., 2011) for entity matching. PARIS delivered entity matching predictions for 5862 entities in the English, French, and Spanish KG, but almost no </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> rotate: knowledge graph embedding by relational rotation in complex space </p><p> te their simplicity, translational models achieve satisfactory performance on KG completion and are robust against the sparsity of data (Hao et al., 2019). RotatE [#b30](Sun et al., 2019b) employs a complex embedding space and models the relation r as the rotation instead of translation of the complex vector h toward t, which lead. upting either head or tail of a true triple (h, r, t).We here consider two representative triple scoring techniques TransE (Bordes et al., 2013) and RotatE [#b30](Sun et al., 2019b). TransE models relations as translations between head entities and tail entities in a Euclidean space, while RotatE models relations as rotatio. dels and three ensemble inference techniques introduced in in Section 3. For baseline methods, besides the singleembedding TransE (Bordes et al., 2013) and RotatE [#b30](Sun et al., 2019b), we also include DistMult (Yang et al., 2015), TransD (Ji et al., 2015)re </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> neural article pair modeling for wikipedia sub-article matching </p><p> s of realworld entities and relations, constituting actionable knowledge that is crucial to various knowledgedriven applications (KoncelKedziorski et al., 2019[#b3]Chen et al., 2018aBordes et al., 2014). Recently, extensive efforts have been invested in KG embedding models, which encode ent </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> co-training embeddings of knowledge graphs and entity descriptions for cross-lingual entity alignment </p><p> liable alignment information that bridges different KGs. Recent works on multilingual KG embeddings provide support for automated entity matching (Chen et al., 2017[#b4](Chen et al., , 2018bSun et al., 2018Sun et al., , 2020a)). However, the performance of. odes KGs via multichannel Graph Neural Network to reconcile the structural differences. Some others also leverage side information to enhance the alignment performance, including entity descriptions [#b4](Chen et al., 2018bZhang et al., 2019), attributes (Trsedya et al., 2019ref type"bibr" target"b25" </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> cross-lingual knowledge graph alignment via graph convolutional networks </p><p> ref), attributes (Trsedya et al., 2019Sun et al., 2017Yang et al., 2019), neighborhood information [#b36](Wang et al., 2018Yang et al., 2015Li et al., 2019Sun e </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> universal representation learning of knowledge bases by jointly embedding instances and ontological concepts </p><p> sD (Ji et al., 2015). Despite their simplicity, translational models achieve satisfactory performance on KG completion and are robust against the sparsity of data [#b11](Hao et al., 2019). RotatE (Sun et al., 2019b) employs a complex embedding space and models the relation r as the rotation ins </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> open question answering with weakly supervised embedding models </p><p> tionable knowledge that is crucial to various knowledgedriven applications (KoncelKedziorski et al., 2019Chen et al., 2018a[#b1]Bordes et al., 2014). Recently, extensive efforts have been invested in KG embedding models, which encode entities as lowdimensional vectors and capture relations </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> a benchmarking study of embedding-based entity alignment for knowledge graphs </p><p> "b27"Sun et al., , 2020a) ) and degree centrality measures (Pei et al., 2019). A systematic summary of relevant approaches is given in a recent survey by [#b29]Sun et al. (2020b). Although these approaches focus on the KG alignment that is different from the problem we tackle here, such techniques can be leveraged to supp </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> holographic embeddings of knowledge graphs </p><p> of Bilinear models such as RESCAL (Nickel et al., 2011) and DistMult (Yang et al., 2015), as well as neural models like HolE [#b19](Nickel et al., 2016) and ConvE (Dettmers et al., 2018). Due to the large body of work in this line of research, we only provid. br" target"b30"(Sun et al., 2019b), we also include DistMult (Yang et al., 2015), TransD (Ji et al., 2015), and HolE [#b19](Nickel et al., 2016). After extensive hyperparameter tuning, the baselines are set to their best configurations. We also include a baseline named RotatEPARIS, wh </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> open question answering with weakly supervised embedding models </p><p> tionable knowledge that is crucial to various knowledgedriven applications (KoncelKedziorski et al., 2019Chen et al., 2018a[#b1]Bordes et al., 2014). Recently, extensive efforts have been invested in KG embedding models, which encode entities as lowdimensional vectors and capture relations </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> semi-supervised entity alignment via joint knowledge embedding model and cross-graph model </p><p> Yang et al., 2019), neighborhood information (Wang et al., 2018Yang et al., 2015[#b18]Li et al., 2019Sun et al., 2019aSun et al., , 2020a) ) and degree centrality measures r </p>
<p> Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer </p><p> aligning cross-lingual entities with multi-aspect information </p><p> "b4"(Chen et al., 2018bZhang et al., 2019), attributes (Trsedya et al., 2019Sun et al., 2017[#b39]Yang et al., 2019), neighborhood information (Wang et al., 2018Yang et al., 2015ref ty </p>
